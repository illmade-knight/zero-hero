{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c34ac4d2-9589-4bf5-af94-dbac44a00301",
   "metadata": {},
   "source": [
    "## Query, Key, Value\n",
    "\n",
    "The attention head and its associated paper - Attention is all you need - is in a lot of ways remarkably simple considering its impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce39034a-676e-4764-8fda-91de4855557f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0164a9eb-62b7-4dbc-807b-17654f4fbb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../lib/bookreader.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4a7eddb7-8487-4bf6-9995-fcb3c0941aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lowercase only\n"
     ]
    }
   ],
   "source": [
    "alice = BookReader()\n",
    "alice.read(\"../resources/alice.txt\")\n",
    "\n",
    "vocab_size = len(alice.itos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6905c70b-2e1b-4c74-a0be-44a45833b1a3",
   "metadata": {},
   "source": [
    "### Explore the mechanism\n",
    "\n",
    "take a text sample of resonable context length\n",
    "\n",
    "we want to think of how the tokens affect each other\n",
    "\n",
    "if we throw our mind way back to bigrams - we create a lookup table of all characters against each other\n",
    "\n",
    "for a context of length 36 we can think of doing the same thing again \n",
    "\n",
    "we're using dot-product attention \n",
    "\n",
    "(compared in the paper to additive attention: this is a simple linear layer - the wavenet used successive dilutions each of which was a kind of additive attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0952b017-2f21-4424-b8fb-7762a32f657e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ting for the hedgehogs  and in a ver'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs, ys = alice.sample_batch(5, 36)\n",
    "alice.decode(xs[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9a419e77-9978-457c-8125-2d605e41bb49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 36, 3])\n",
      "torch.Size([5, 36, 48]) torch.Size([5, 48, 36])\n",
      "torch.Size([5, 36, 36])\n",
      "tensor([ 21.5033, -21.4774,  12.3940,   6.1019,   4.9846,  22.4517,   5.4259,\n",
      "        -23.3626,   4.9846,  21.5033,  -1.7026,  15.2632,   4.9846,  -1.7026,\n",
      "         15.2632,   0.9464,   6.1019,  15.2632,  -1.7026,   5.4259,   6.1019,\n",
      "          8.3596,   4.9846,   4.9846,  23.0301,  12.3940,   0.9464,   4.9846,\n",
      "        -21.4774,  12.3940,   4.9846,  23.0301,   4.9846,  -2.3029,  15.2632,\n",
      "        -23.3626])\n",
      "tensor([ 21.5033, -21.4774,  12.3940,   6.1019,   4.9846,  22.4517,   5.4259,\n",
      "        -23.3626,   4.9846,  21.5033,  -1.7026,  15.2632,   4.9846,  -1.7026,\n",
      "         15.2632,   0.9464,   6.1019,  15.2632,  -1.7026,   5.4259,   6.1019,\n",
      "          8.3596,   4.9846,   4.9846,  23.0301,  12.3940,   0.9464,   4.9846,\n",
      "        -21.4774,  12.3940,   4.9846,  23.0301,   4.9846,  -2.3029,  15.2632,\n",
      "        -23.3626])\n"
     ]
    }
   ],
   "source": [
    "embedding_dimension = 3\n",
    "# attention dimension\n",
    "dk = 48\n",
    "\n",
    "em = torch.randn(vocab_size, embedding_dimension)\n",
    "\n",
    "emx = em[xs]\n",
    "\n",
    "print(emx.shape)\n",
    "\n",
    "B, T, C = emx.shape\n",
    "\n",
    "q = torch.randn(C, dk)\n",
    "k = torch.randn(C, dk)\n",
    "\n",
    "qt = emx @ q\n",
    "kt = emx @ k\n",
    "\n",
    "print(qt.shape, kt.mT.shape)\n",
    "\n",
    "# x.mT is equivalent to x.transpose(-2, -1).\n",
    "lu = qt @ kt.mT\n",
    "# andreq uses the equivalent\n",
    "lub = qt @ torch.transpose(kt, -2, -1)\n",
    "\n",
    "print(lu.shape)\n",
    "\n",
    "print(lu[0][0])\n",
    "print(lub[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d73a642-0e3f-4a0b-912f-e98a939adc0d",
   "metadata": {},
   "source": [
    "### What have we here?\n",
    "a kind of lookup for how each token depends on all the others\n",
    "\n",
    "if we softmax it we'll get a probabilithy like view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "01470760-a045-4a04-9b2d-12e0d3a6d92e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 36, 36]), tensor(1.))"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = F.softmax(lu, dim=1)\n",
    "probs.shape, probs.sum(dim=1)[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d48686-9e02-4b12-baac-3ee873b1b92c",
   "metadata": {},
   "source": [
    "### Does the k, q, v language make sense?\n",
    "\n",
    "we've got 2 learnable tensors here, the 'query' is in reality the input tensor\n",
    "\n",
    "the attention heads query vector is really a tensor that converts the input tensor into the attention head's space\n",
    "\n",
    "we also take the dot-product of the input with 'key' vector - again this converts the input into the attention head's space\n",
    "\n",
    "for me it's really the dot-product of these that is the query-key lookup: this is where we look at the probs[0][0] to see how we think \n",
    "the first Token interacts with the other Tokens in the context.\n",
    "\n",
    "#### Value\n",
    "\n",
    "the 'value' tensor operates in a similar way - we find the dot product of the input with it first\n",
    "\n",
    "#### Scale the q, k table\n",
    "the paper introduces a scaling for the query-key lookup \n",
    " *'We suspect that for large values of dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\n",
    " extremely small gradients 4. To counteract this effect, we scale the dot products by 1/sqrt(dk)'*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e0dca939-3daf-4a45-b763-57fc524a407b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 36, 48])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 36, 48]),\n",
       " tensor([ 6.6203e-07,  3.1450e-07, -1.6754e-07, -1.2448e-06, -5.7189e-07,\n",
       "          8.9880e-07,  1.7292e-07, -1.4849e-06,  5.6165e-07,  7.0879e-07,\n",
       "         -5.9794e-07, -1.0631e-06,  1.7446e-07, -6.2405e-07, -4.0282e-07,\n",
       "         -9.4367e-08,  7.9970e-07, -1.8231e-06,  6.6989e-07, -2.4515e-06,\n",
       "          3.0819e-07,  6.8690e-07,  1.2855e-06, -1.8265e-06,  3.2168e-07,\n",
       "          1.3574e-06,  5.9770e-07,  4.6274e-08, -5.2082e-07,  6.5413e-07,\n",
       "         -3.7107e-07,  1.5625e-06, -1.7119e-07, -8.3103e-07, -1.2251e-06,\n",
       "          2.1602e-06, -7.1606e-07, -1.2778e-06, -1.2490e-06, -2.9580e-07,\n",
       "         -2.7646e-07, -1.8160e-07, -7.1711e-07, -8.5725e-07,  2.4421e-07,\n",
       "         -9.9975e-07,  1.7295e-06,  4.0540e-07]))"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = torch.randn(C, dk)\n",
    "\n",
    "vt = emx @ v\n",
    "print(vt.shape)\n",
    "\n",
    "out = probs * dk**-0.5 @ vt\n",
    "out.shape, out[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02104a0-7a38-47f2-9a6a-2100ddb3e89c",
   "metadata": {},
   "source": [
    "## Masked attention\n",
    "\n",
    "in a decoder network we don't want preceeding tokens to be able to gain information from the following tokens (it's compared to being able to see the answers to questions in advance)\n",
    "\n",
    "to do this we use a simple triangluar tensor multiplication - a lower left (tril) with the upper right elements set to -inf so when we use softmax we keep a normalized output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "6795f23e-7279-4b50-8ef6-61d4399f1698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2135, -0.4079, -0.2872, -0.8498,  1.6850, -1.2782],\n",
      "        [ 0.3345,  1.6951,  0.1399,  1.2216,  0.0459, -1.8412],\n",
      "        [ 0.0864, -0.6933, -1.9226,  0.1483,  0.3684,  1.7560],\n",
      "        [ 0.7434,  0.3826,  1.2543,  1.6146, -0.7157,  0.6360],\n",
      "        [-1.8041, -0.0030, -1.1502,  2.4319,  0.8799,  1.5553],\n",
      "        [-0.6071, -0.8940, -0.4337,  1.4431, -0.1497,  2.5575]])\n",
      "tensor([[ 0.2135,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.3345,  1.6951,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0864, -0.6933, -1.9226,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.7434,  0.3826,  1.2543,  1.6146,  0.0000,  0.0000],\n",
      "        [-1.8041, -0.0030, -1.1502,  2.4319,  0.8799,  0.0000],\n",
      "        [-0.6071, -0.8940, -0.4337,  1.4431, -0.1497,  2.5575]])\n",
      "tensor([[ 0.2135,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "        [ 0.3345,  1.6951,    -inf,    -inf,    -inf,    -inf],\n",
      "        [ 0.0864, -0.6933, -1.9226,    -inf,    -inf,    -inf],\n",
      "        [ 0.7434,  0.3826,  1.2543,  1.6146,    -inf,    -inf],\n",
      "        [-1.8041, -0.0030, -1.1502,  2.4319,  0.8799,    -inf],\n",
      "        [-0.6071, -0.8940, -0.4337,  1.4431, -0.1497,  2.5575]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2041, 0.7959, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.6279, 0.2879, 0.0842, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1738, 0.1212, 0.2897, 0.4153, 0.0000, 0.0000],\n",
       "        [0.0108, 0.0653, 0.0207, 0.7453, 0.1579, 0.0000],\n",
       "        [0.0278, 0.0209, 0.0331, 0.2160, 0.0439, 0.6583]])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qk = torch.randn(6, 6)\n",
    "print(qk)\n",
    "tri = torch.tril(qk)\n",
    "print(tri)\n",
    "mask = tri == 0\n",
    "out = tri.masked_fill(mask, float('-inf'))\n",
    "print(out)\n",
    "F.softmax(out, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc016611-c403-46ab-9745-c7c6ac9588e5",
   "metadata": {},
   "source": [
    "### Why multiple heads?\n",
    "\n",
    "It's not really clear from the lecture why we have multiple head rather than just increasing the dimensions dk of a single head\n",
    "\n",
    "again we look to the paper - the softmax is the difference - with multiple samller heads each with its own softmax the model learns\n",
    "different attention types for each head which would otherwise be averaged out in a single larger head"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
