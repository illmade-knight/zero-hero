{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57ba5f38-a0cd-487d-888b-13428ca38aaa",
   "metadata": {},
   "source": [
    "## Wavnet(ish)\n",
    "\n",
    "we successively combine layer outputs, beginning with raw character input, to form larger contexts \n",
    "\n",
    "consider a list:\n",
    "\n",
    "[1,2,3,4,5,6,7,8]\n",
    "\n",
    "our first layer combines\n",
    "\n",
    "(1,2), (3,4), (5,6), (7,8)\n",
    "\n",
    "our second\n",
    "\n",
    "((1,2), (3,4)), ((5,6), (7,8))\n",
    "\n",
    "and our output layer represents the whole combined list input to a single output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b380bfb-1cb5-4c95-89eb-ecf3fc684e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e8ffe8f-462a-457b-b480-d9078375d519",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../lib/bookreader.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258b4e8c-9b5f-40ba-ae21-c05dbc0195c0",
   "metadata": {},
   "source": [
    "### Open the book\n",
    "\n",
    "and create an example 3D embedding for the characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9dd1f7b4-c231-42df-9949-787c33bdd736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lowercase only\n"
     ]
    }
   ],
   "source": [
    "alice = BookReader()\n",
    "alice.read(\"../resources/alice.txt\")\n",
    "\n",
    "vocab_size = len(alice.itos)\n",
    "embedding_size = 3\n",
    "context_length = 8\n",
    "ex_em = torch.randn(vocab_size, embedding_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aec3349-63d7-4a89-a7ac-16785c55808b",
   "metadata": {},
   "source": [
    "### Take some text and embed it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cdb9a0a1-8188-41ef-ae88-1d0152db3a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 6, 11,  4, 19, 23,  8, 21,  1],\n",
      "        [12,  1,  0,  7, 18, 26, 17,  1],\n",
      "        [23, 11,  8,  1, 21,  4,  5,  5],\n",
      "        [12, 23,  1, 11, 18, 15,  8,  0],\n",
      "        [ 0,  0,  4, 15, 12,  6,  8,  1]])\n",
      "first sample: tensor([[-2.3582,  0.4293, -0.2912],\n",
      "        [ 0.6305, -2.3464, -0.2017],\n",
      "        [ 0.1161,  1.7447, -1.7238],\n",
      "        [-0.4493,  0.3968, -0.8936],\n",
      "        [ 0.7610, -1.5595,  2.1578],\n",
      "        [ 0.5193, -2.1500,  0.4494],\n",
      "        [-1.6135,  0.1362,  0.6421],\n",
      "        [-0.7695, -0.3563,  0.4386]])\n"
     ]
    }
   ],
   "source": [
    "encoded = [alice.stoi[c] for c in alice.train[:40]]\n",
    "print(torch.tensor(encoded).view(5, context_length))\n",
    "embedded_text = ex_em[encoded].view(5, context_length, embedding_size)\n",
    "print(\"first sample:\", embedded_text[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8fc42a-dc63-4c69-bad1-e206899a8911",
   "metadata": {},
   "source": [
    "### Split into character pairs\n",
    "\n",
    "to do this we can just view the tensor differently so the 8 characters of our first sample grouped in 2's (each of dimension 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "890ab88c-9c2d-454e-bb55-0f73324ab905",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-2.3582,  0.4293, -0.2912],\n",
       "         [ 0.6305, -2.3464, -0.2017]],\n",
       "\n",
       "        [[ 0.1161,  1.7447, -1.7238],\n",
       "         [-0.4493,  0.3968, -0.8936]],\n",
       "\n",
       "        [[ 0.7610, -1.5595,  2.1578],\n",
       "         [ 0.5193, -2.1500,  0.4494]],\n",
       "\n",
       "        [[-1.6135,  0.1362,  0.6421],\n",
       "         [-0.7695, -0.3563,  0.4386]]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs = embedded_text.view(5, 4, 2, embedding_size)\n",
    "pairs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718c0993-644b-45e6-a878-4456bc4f8f40",
   "metadata": {},
   "source": [
    "### Create a layer\n",
    "\n",
    "We'll return to building our own layers to demonstrate what's happening inside"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e57bd246-4dce-49e4-906e-8af00ed8c5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViewDilaltion(nn.Module):\n",
    "\n",
    "    def __init__(self, n):\n",
    "        super().__init__()\n",
    "        self.n = n\n",
    "        self.type = 'flatten'\n",
    "\n",
    "    def __call__(self, x):\n",
    "        B, T, C = x.shape\n",
    "        x = x.view(B, -1, self.n * C)\n",
    "        \n",
    "        self.out = x\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1aa337-d1c6-40ea-93c1-acc1db538bb6",
   "metadata": {},
   "source": [
    "### Not quite the same\n",
    "\n",
    "we want to represent the batch, the number of combined tokens and a 3rd dimension \n",
    "\n",
    "our 3rd dimensions holds both input characters instead of separating them\n",
    "\n",
    "#### The dialation needs a linear layer\n",
    "\n",
    "just changing the view doesn't do much\n",
    "\n",
    "what we want is this followed by a linear layer, the linear layer makes the paired characters talk *only* to each other\n",
    "\n",
    "if our linear layer had a single neuron then the two input characters (in their embedded form) would now be represented by a single new\n",
    "*fused* representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e157d081-8042-4548-8178-a6ad060cb2be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 4, 6]) tensor([[-2.3582,  0.4293, -0.2912,  0.6305, -2.3464, -0.2017],\n",
      "        [ 0.1161,  1.7447, -1.7238, -0.4493,  0.3968, -0.8936],\n",
      "        [ 0.7610, -1.5595,  2.1578,  0.5193, -2.1500,  0.4494],\n",
      "        [-1.6135,  0.1362,  0.6421, -0.7695, -0.3563,  0.4386]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 4, 1]),\n",
       " tensor([[[-0.9607],\n",
       "          [-2.2880],\n",
       "          [ 2.1361],\n",
       "          [-1.1945]],\n",
       " \n",
       "         [[-0.5021],\n",
       "          [ 0.5503],\n",
       "          [-2.7257],\n",
       "          [ 0.2443]],\n",
       " \n",
       "         [[ 1.3186],\n",
       "          [ 1.7463],\n",
       "          [-3.8608],\n",
       "          [ 0.3361]],\n",
       " \n",
       "         [[ 3.4059],\n",
       "          [ 0.2266],\n",
       "          [ 1.9840],\n",
       "          [ 4.7713]],\n",
       " \n",
       "         [[ 4.4159],\n",
       "          [ 2.0203],\n",
       "          [-2.9771],\n",
       "          [ 1.7463]]]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fl = ViewDilaltion(2)\n",
    "\n",
    "out = fl(embedded_text)\n",
    "\n",
    "print(out.shape, out[0])\n",
    "\n",
    "ll = torch.randn(6, 1)\n",
    "\n",
    "ll_out = out @ ll\n",
    "\n",
    "ll_out.shape, ll_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44e915a-b2f1-44b5-8f54-64eaa487f62d",
   "metadata": {},
   "source": [
    "## Create a model\n",
    "\n",
    "we'll create a sequential model to successively combine our character into an output\n",
    "\n",
    "pull in our old homemade layers and build a sequential model to represent this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2b82182-b1d7-406f-818d-c7a21a55028f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../lib/nn_layers.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7bd60e47-a68f-40f7-a4f2-cbb7db780593",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee5ad11e-f4f7-498a-a3b6-42f894834d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Embedding(vocab_size, embedding_size),\n",
    "    ViewDilaltion(2),\n",
    "    Linear(embedding_size * 2, n_hidden, bias=False), \n",
    "    Tanh(),\n",
    "    ViewDilaltion(2),\n",
    "    Linear(n_hidden * 2, n_hidden, bias=False), \n",
    "    Tanh(),\n",
    "    ViewDilaltion(2),\n",
    "    Linear(n_hidden * 2, n_hidden, bias=False), \n",
    "    Tanh(),\n",
    "    Linear(n_hidden, vocab_size, bias=True),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2a9659-d522-4bd2-ba9e-ffc2bada84fc",
   "metadata": {},
   "source": [
    "## Run the model\n",
    "\n",
    "if we run our first basic sample through the network we get what we want\n",
    "\n",
    "i.e for each sample in the batch we get a single value in our vocab - space\n",
    "\n",
    "so this looks like our [sequence](../sequences/sequence.ipynb) network from before\n",
    "\n",
    "* ...e  -> m\n",
    "* ..em  -> m\n",
    "* .emm  -> a\n",
    "* emma  -> ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3bcbda93-202b-4098-bdaf-11ad5fdb0351",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 1, 30]),\n",
       " tensor([[ 0.5829, -5.1795, -2.3836, -0.2851, -0.8202, -4.9881, -3.6234,  2.9232,\n",
       "           1.5306, -0.9459,  2.2703,  2.0584, -6.6450, -7.0613, -2.0522, -6.1872,\n",
       "           3.5458,  1.3866, 12.6264,  4.4885, -0.7386, -3.1797, -7.6663,  3.9959,\n",
       "          -6.1365,  6.0381, -1.2655, 14.0181,  0.4901, -6.0630]]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_out = model(torch.tensor(encoded).view(5, 8))\n",
    "model_out.shape, model_out[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1608b40c-a750-4c89-b1a1-65c3c4cfdc4d",
   "metadata": {},
   "source": [
    "## So is it the same?\n",
    "\n",
    "Or at least does it work in a similar way.\n",
    "\n",
    "Lets go right back to [sequence](../sequences/sequence.ipynb) and see if we get a similar result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1117d64a-b82b-4443-b23a-ff4c6c831518",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_length = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "93f197b0-5329-4433-bbe8-e7b915bd8810",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randrange, randint\n",
    "\n",
    "import string\n",
    "letters = [l for l in string.ascii_lowercase]\n",
    "\n",
    "itos = {0: \".\"}\n",
    "stoi = {\".\": 0}\n",
    "\n",
    "for i, l in enumerate(letters):\n",
    "    offset = i+1\n",
    "    stoi[l] = offset\n",
    "    itos[offset] = l\n",
    "\n",
    "with open(\"../resources/names.txt\", \"r\") as r:\n",
    "    encoded_names = [[stoi[c] for c in f] for f in r.read().split()]\n",
    "\n",
    "names_length = len(encoded_names)\n",
    "\n",
    "def sample(size=5, context_length=8):\n",
    "    prepend = [0 for _ in range(context_length)]\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for i in range(size):\n",
    "        ni = randrange(names_length-1)\n",
    "        name = prepend + encoded_names[ni] + [0] \n",
    "        offset = randint(0, len(name)-context_length-1)\n",
    "        xs.append(name[offset:offset+context_length])\n",
    "        ys.append(name[offset+context_length])\n",
    "    return xs, ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "189154de-2a41-45cc-9c3b-b71a38049279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([[0, 0, 0, 0, 0, 0, 1, 12], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 18, 1], [0, 0, 1, 1, 12, 1, 14, 1], [0, 0, 0, 0, 0, 0, 0, 0]], [9, 8, 25, 0, 11])\n"
     ]
    }
   ],
   "source": [
    "print(sample())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c96659-ec0e-4e00-a202-9779701cfbbe",
   "metadata": {},
   "source": [
    "## Compare\n",
    "let's do roughly the same amount of learing as [sequence](../sequences/sequence.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bba6eeb8-d087-41d8-b055-ca5507566aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 120\n",
    "batch_size = 400\n",
    "learning_rate = .2\n",
    "samples = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7b79890c-e4bb-452a-8a73-1281f817141b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Embedding(vocab_size, embedding_size),\n",
    "    ViewDilaltion(2),\n",
    "    Linear(embedding_size * 2, n_hidden, bias=False), \n",
    "    Tanh(),\n",
    "    ViewDilaltion(2),\n",
    "    Linear(n_hidden * 2, n_hidden, bias=False), \n",
    "    Tanh(),\n",
    "    ViewDilaltion(2),\n",
    "    Linear(n_hidden * 2, n_hidden, bias=False), \n",
    "    Tanh(),\n",
    "    Linear(n_hidden, vocab_size, bias=True),\n",
    "])\n",
    "\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "393b87b4-bded-436b-89b2-0797c5a37662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.9918)\n",
      "0 0.18400000000000002\n",
      "tensor(2.4656)\n",
      "10 0.16928000000000004\n",
      "tensor(2.4518)\n",
      "20 0.15573760000000003\n",
      "tensor(2.4285)\n",
      "30 0.14327859200000004\n",
      "tensor(2.4033)\n",
      "40 0.13181630464000005\n",
      "tensor(2.3909)\n",
      "50 0.12127100026880006\n",
      "tensor(2.3769)\n",
      "60 0.11156932024729606\n",
      "tensor(2.3678)\n",
      "70 0.10264377462751238\n",
      "tensor(2.3581)\n",
      "80 0.0944322726573114\n",
      "tensor(2.3525)\n",
      "90 0.08687769084472649\n",
      "tensor(2.3445)\n",
      "100 0.07992747557714837\n",
      "tensor(2.3361)\n",
      "110 0.0735332775309765\n",
      "tensor(2.3289)\n"
     ]
    }
   ],
   "source": [
    "for ep in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    for s in range(samples):\n",
    "        x, y = sample(batch_size)\n",
    "        Y = torch.tensor(y)\n",
    "        X = torch.tensor(x)\n",
    "\n",
    "        logits = model(X)\n",
    "\n",
    "        loss = F.cross_entropy(logits.view(-1, vocab_size), Y) # loss function\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            epoch_loss += loss\n",
    "\n",
    "        # again stuff on parameters should probably be in model?\n",
    "        for p in model.parameters():\n",
    "          p.grad = None\n",
    "        loss.backward()\n",
    "\n",
    "        for p in model.parameters():\n",
    "            p.data -= learning_rate * p.grad\n",
    "\n",
    "    #just keep any epoch stuff in a no grad block\n",
    "    with torch.no_grad():\n",
    "        if ep % 10 == 0:\n",
    "            print(epoch_loss/samples)\n",
    "            learning_rate *= .92\n",
    "            print(ep, learning_rate)\n",
    "\n",
    "print(epoch_loss/samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5422ff52-199e-4ac5-8ce8-37124fb1772b",
   "metadata": {},
   "source": [
    "### Not so promising\n",
    "\n",
    "now we haven't initialized our weights or done any optimization but it's worse than our basic sequence model\n",
    "\n",
    "one thing we have now is a deeper network\n",
    "\n",
    "if gradients explode or go to zero on one layer this will effect surronding layers\n",
    "\n",
    "so here's were we probably want to start looking at normalization techniques for our network\n",
    "\n",
    "#### start slowly\n",
    "\n",
    "lets use torch to create our model and apply the basic initialization techniques from before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "130e3812-6be2-4d50-b8ef-5a2e0a85d5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "class Wavenetish(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_size, context_length, hidden_size):\n",
    "        super().__init__()\n",
    "\n",
    "        layers = OrderedDict([\n",
    "            ('embed', nn.Embedding(vocab_size, embedding_size)),\n",
    "            ('flatten_a', ViewDilaltion(2)),\n",
    "            ('feed_forward_a', nn.Linear(embedding_size*2, hidden_size, bias=True)),\n",
    "            ('non_linearity_a', nn.Tanh()),\n",
    "            ('flatten_b', ViewDilaltion(2)),\n",
    "            ('feed_forward_b', nn.Linear(n_hidden*2, hidden_size, bias=True)),\n",
    "            ('non_linearity_b', nn.Tanh()),\n",
    "            ('flatten_c', ViewDilaltion(2)),\n",
    "            ('feed_forward_c', nn.Linear(n_hidden*2, hidden_size, bias=True)),\n",
    "            ('non_linearity_c', nn.Tanh()),\n",
    "            ('logits', nn.Linear(hidden_size, vocab_size, bias=True)),\n",
    "        ])\n",
    "\n",
    "        nn.init.xavier_uniform_(layers['feed_forward_a'].weight, gain=5/3)\n",
    "        nn.init.zeros_(layers['feed_forward_a'].bias)\n",
    "\n",
    "        nn.init.xavier_uniform_(layers['feed_forward_b'].weight, gain=5/3)\n",
    "        nn.init.zeros_(layers['feed_forward_b'].bias)\n",
    "\n",
    "        nn.init.xavier_uniform_(layers['feed_forward_c'].weight, gain=5/3)\n",
    "        nn.init.zeros_(layers['feed_forward_c'].bias)\n",
    "\n",
    "        nn.init.zeros_(layers['logits'].bias)\n",
    "\n",
    "        self.model = nn.Sequential(layers)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        logits = self.model(idx)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            loss = F.cross_entropy(logits.view(-1, vocab_size), targets)\n",
    "\n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e3d03a1d-6b5a-4ce9-a8db-7ab625aa0f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 120\n",
    "batch_size = 800\n",
    "learning_rate = .2\n",
    "samples = 1000\n",
    "\n",
    "model = Wavenetish(vocab_size, embedding_size, context_length, n_hidden)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5664ca36-631a-4af7-aa6a-f81bfcfc1b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.2917)\n",
      "0 0.18400000000000002\n",
      "tensor(2.1066)\n",
      "10 0.16928000000000004\n",
      "tensor(2.0942)\n",
      "20 0.15573760000000003\n",
      "tensor(2.0882)\n",
      "30 0.14327859200000004\n",
      "tensor(2.0847)\n",
      "40 0.13181630464000005\n",
      "tensor(2.0821)\n",
      "50 0.12127100026880006\n",
      "tensor(2.0809)\n",
      "60 0.11156932024729606\n",
      "tensor(2.0795)\n",
      "70 0.10264377462751238\n",
      "tensor(2.0800)\n",
      "80 0.0944322726573114\n",
      "tensor(2.0759)\n",
      "90 0.08687769084472649\n",
      "tensor(2.0756)\n",
      "100 0.07992747557714837\n",
      "tensor(2.0777)\n",
      "110 0.0735332775309765\n",
      "tensor(2.0753)\n"
     ]
    }
   ],
   "source": [
    "for ep in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    for s in range(samples):\n",
    "        x, y = sample(batch_size)\n",
    "        Y = torch.tensor(y)\n",
    "        X = torch.tensor(x)\n",
    "\n",
    "        logits, loss = model.forward(X, Y)\n",
    "        \n",
    "        epoch_loss += loss.detach()\n",
    "        model.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    #just keep any epoch stuff in a no grad block\n",
    "    with torch.no_grad():\n",
    "        if ep % 10 == 0:\n",
    "            print(epoch_loss/samples)\n",
    "            learning_rate *= .92\n",
    "            print(ep, learning_rate)\n",
    "\n",
    "print(epoch_loss/samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977c0373-98d5-483f-bf9a-4fc92cfd609e",
   "metadata": {},
   "source": [
    "### Better already\n",
    "\n",
    "some initialization again seems to do a lot of the necessary work - our large batch does some of the normalization for us\n",
    "\n",
    "still it's worth looking at using a Normalization technique (we'll use LayerNorm straight away because BatchNorm is sloooow)\n",
    "\n",
    "note that we remove bias from the linear layers as LayerNorm has a bias term that should work "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "219fd4bf-e2fb-47c0-b1a0-644613821548",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalWavenetish(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_size, context_length, hidden_size):\n",
    "        super().__init__()\n",
    "\n",
    "        layers = OrderedDict([\n",
    "            ('embed', nn.Embedding(vocab_size, embedding_size)),\n",
    "            ('flatten_a', ViewDilaltion(2)),\n",
    "            ('feed_forward_a', nn.Linear(embedding_size*2, hidden_size, bias=False)),\n",
    "            ('layer_norm_a', nn.LayerNorm(hidden_size)),\n",
    "            ('non_linearity_a', nn.Tanh()),\n",
    "            ('flatten_b', ViewDilaltion(2)),\n",
    "            ('feed_forward_b', nn.Linear(n_hidden*2, hidden_size, bias=False)),\n",
    "            ('layer_norm_b', nn.LayerNorm(hidden_size)),\n",
    "            ('non_linearity_b', nn.Tanh()),\n",
    "            ('flatten_c', ViewDilaltion(2)),\n",
    "            ('feed_forward_c', nn.Linear(n_hidden*2, hidden_size, bias=False)),\n",
    "            ('layer_norm_c', nn.LayerNorm(hidden_size)),\n",
    "            ('non_linearity_c', nn.Tanh()),\n",
    "            ('logits', nn.Linear(hidden_size, vocab_size, bias=True)),\n",
    "        ])\n",
    "\n",
    "        nn.init.xavier_uniform_(layers['feed_forward_a'].weight, gain=5/3)\n",
    "        nn.init.xavier_uniform_(layers['feed_forward_b'].weight, gain=5/3)\n",
    "        nn.init.xavier_uniform_(layers['feed_forward_c'].weight, gain=5/3)\n",
    "\n",
    "        nn.init.zeros_(layers['logits'].bias)\n",
    "\n",
    "        self.model = nn.Sequential(layers)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        logits = self.model(idx)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            loss = F.cross_entropy(logits.view(-1, vocab_size), targets)\n",
    "\n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "53142601-925f-4301-8d8f-723f9b173bfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8200"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 120\n",
    "batch_size = 100\n",
    "learning_rate = .1\n",
    "samples = 4000\n",
    "\n",
    "model = NormalWavenetish(vocab_size, embedding_size, context_length, n_hidden)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "\n",
    "sum(p.nelement() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1b2729ea-0d5e-4747-ac35-3687c7491edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3080)\n",
      "0 0.09200000000000001\n",
      "tensor(2.1408)\n",
      "10 0.08464000000000002\n",
      "tensor(2.1189)\n",
      "20 0.07786880000000002\n",
      "tensor(2.1094)\n",
      "30 0.07163929600000002\n",
      "tensor(2.1030)\n",
      "40 0.06590815232000002\n",
      "tensor(2.0999)\n",
      "50 0.06063550013440003\n",
      "tensor(2.0938)\n",
      "60 0.05578466012364803\n",
      "tensor(2.0885)\n",
      "70 0.05132188731375619\n",
      "tensor(2.0868)\n",
      "80 0.0472161363286557\n",
      "tensor(2.0841)\n",
      "90 0.043438845422363245\n",
      "tensor(2.0801)\n",
      "100 0.039963737788574184\n",
      "tensor(2.0816)\n",
      "110 0.03676663876548825\n",
      "tensor(2.0785)\n"
     ]
    }
   ],
   "source": [
    "for ep in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    for s in range(samples):\n",
    "        x, y = sample(batch_size)\n",
    "        Y = torch.tensor(y)\n",
    "        X = torch.tensor(x)\n",
    "\n",
    "        logits, loss = model.forward(X, Y)\n",
    "        \n",
    "        epoch_loss += loss.detach()\n",
    "        model.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    #just keep any epoch stuff in a no grad block\n",
    "    with torch.no_grad():\n",
    "        if ep % 10 == 0:\n",
    "            print(epoch_loss/samples)\n",
    "            learning_rate *= .92\n",
    "            print(ep, learning_rate)\n",
    "\n",
    "print(epoch_loss/samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a10f6e6-3552-41bb-b6c0-f485a79a521e",
   "metadata": {},
   "source": [
    "### Faster\n",
    "\n",
    "we were able to make ouir batch size much smaller and achieve the same learing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "58098047-2eb0-405e-98b3-bb03ace94138",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(num_names):\n",
    "    for i in range(num_names):\n",
    "        \n",
    "        out = []\n",
    "        ix = [[0 for _ in range(context_length)]]\n",
    "    \n",
    "        for _ in range(14):\n",
    "            logits, _ = model.forward(torch.tensor(ix))\n",
    "            \n",
    "            p = F.softmax(logits.view(-1, vocab_size), dim=1)\n",
    "\n",
    "            prediction = torch.multinomial(p, num_samples=1).item()\n",
    "    \n",
    "            for i in range(context_length-1):\n",
    "                ix[0][i]= ix[0][i+1]\n",
    "    \n",
    "            ix[0][context_length-1] = prediction\n",
    "    \n",
    "            if prediction == 0:\n",
    "                break\n",
    "            out.append(itos[prediction])\n",
    "        print(\"\".join(out))\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "973dc6d8-8be5-4076-8ced-071f141fc565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gbellah\n",
      "colli\n",
      "adam\n",
      "seii\n",
      "anaid\n",
      "keni\n",
      "rowice\n",
      "rulei\n",
      "branton\n",
      "kyvan\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['k', 'y', 'v', 'a', 'n']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814e3a39-73a9-4774-812f-763be9cf2eba",
   "metadata": {},
   "source": [
    "## More thoughts\n",
    "\n",
    "there's a few things here\n",
    "\n",
    "the hidden layer size - what are we trying to do with it?\n",
    "\n",
    "it's operating on pairs:\n",
    "* initially its taking in characters and combining them in different ways to form fused representations\n",
    "* these fused representations are then themselves combined\n",
    "do we have an intuition that these pairings operate in similarly sized spaces?\n",
    "\n",
    "how complex does that representation need to be?\n",
    "\n",
    "(i'd suggest the necessary representation space increases layer by layer - we can write more words than characters - more sentences than words)\n",
    "\n",
    "what if we change those parameter sizes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ce5be22a-d41b-4aeb-a48c-3c6cc0c0d34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariableWavenetish(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_size, context_length, hidden_sizes):\n",
    "        super().__init__()\n",
    "\n",
    "        layers = OrderedDict([\n",
    "            ('embed', nn.Embedding(vocab_size, embedding_size)),\n",
    "            ('flatten_a', ViewDilaltion(2)),\n",
    "            ('feed_forward_a', nn.Linear(embedding_size*2, hidden_sizes[0], bias=False)),\n",
    "            ('layer_norm_a', nn.LayerNorm(hidden_sizes[0])),\n",
    "            ('non_linearity_a', nn.Tanh()),\n",
    "            ('flatten_b', ViewDilaltion(2)),\n",
    "            ('feed_forward_b', nn.Linear(hidden_sizes[0]*2, hidden_sizes[1], bias=False)),\n",
    "            ('layer_norm_b', nn.LayerNorm(hidden_sizes[1])),\n",
    "            ('non_linearity_b', nn.Tanh()),\n",
    "            ('flatten_c', ViewDilaltion(2)),\n",
    "            ('feed_forward_c', nn.Linear(hidden_sizes[1]*2, hidden_sizes[2], bias=False)),\n",
    "            ('layer_norm_c', nn.LayerNorm(hidden_sizes[2])),\n",
    "            ('non_linearity_c', nn.Tanh()),\n",
    "            ('logits', nn.Linear(hidden_sizes[2], vocab_size, bias=True)),\n",
    "        ])\n",
    "\n",
    "        nn.init.xavier_uniform_(layers['feed_forward_a'].weight, gain=5/3)\n",
    "        nn.init.xavier_uniform_(layers['feed_forward_b'].weight, gain=5/3)\n",
    "        nn.init.xavier_uniform_(layers['feed_forward_c'].weight, gain=5/3)\n",
    "\n",
    "        nn.init.zeros_(layers['logits'].bias)\n",
    "\n",
    "        self.model = nn.Sequential(layers)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        logits = self.model(idx)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            loss = F.cross_entropy(logits.view(-1, vocab_size), targets)\n",
    "\n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "88a293fc-7a8b-4220-b997-190545e10ea2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4540"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 120\n",
    "batch_size = 100\n",
    "learning_rate = .1\n",
    "samples = 4000\n",
    "\n",
    "model = VariableWavenetish(vocab_size, embedding_size, context_length, [10, 30, 40])\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "\n",
    "sum(p.nelement() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e56698d-a4f2-482a-8a6d-1f8fe1f7d0f0",
   "metadata": {},
   "source": [
    "### Paramaters\n",
    "\n",
    "that's given us a lot of parameters to play around with - lets increase our dimensions, we'll still have a bunch of parameters to spare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5ae89f7c-5720-4ccc-b81c-0e2bea1213fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4990"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 120\n",
    "batch_size = 100\n",
    "learning_rate = .1\n",
    "samples = 4000\n",
    "hidden_states = [10, 30, 40]\n",
    "embedding_size = 12\n",
    "\n",
    "model = VariableWavenetish(vocab_size, embedding_size, context_length, hidden_states)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "\n",
    "sum(p.nelement() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106edbf4-1cc8-4c71-8ec5-2b48f86cde2e",
   "metadata": {},
   "source": [
    "## Or allow us to increase our context length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30dc6ca-2c04-447a-8106-c470c529ffc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeeperWavenetish(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_size, context_length, hidden_sizes):\n",
    "        super().__init__()\n",
    "\n",
    "        layers = OrderedDict([\n",
    "            ('embed', nn.Embedding(vocab_size, embedding_size)),\n",
    "        ])\n",
    "\n",
    "        hidden_sizes = [embedding_size] + hidden_sizes\n",
    "\n",
    "        sufixes = ['_a', '_b', '_c', '_d', '_e', '_f', '_g', '_h']\n",
    "        for i in range(len(hidden_sizes)-1):\n",
    "            self.add_block(layers, sufixes[i], hidden_sizes[i], hidden_sizes[i+1])\n",
    "        \n",
    "        layers['logits'] = nn.Linear(hidden_sizes[-1], vocab_size, bias=True)\n",
    "        nn.init.zeros_(layers['logits'].bias)\n",
    "\n",
    "        self.model = nn.Sequential(layers)\n",
    "\n",
    "    def add_block(self, layer_dict, suffix=\"_a\", fan_in=10, fan_out=20):\n",
    "        linear_layer = nn.Linear(fan_in*2, fan_out, bias=False)\n",
    "        nn.init.xavier_uniform_(linear_layer.weight, gain=5/3)\n",
    "        layer_dict['flatten'+suffix] = ViewDilaltion(2)\n",
    "        layer_dict['feed_forward'+suffix] = linear_layer\n",
    "        layer_dict['layer_norm'+suffix] = nn.LayerNorm(fan_out)\n",
    "        layer_dict['non_linearity'+suffix] = nn.Tanh()\n",
    "        print(\"added block\", suffix, fan_in, fan_out)\n",
    "        \n",
    "    def forward(self, idx, targets=None):\n",
    "        logits = self.model(idx)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            loss = F.cross_entropy(logits.view(-1, vocab_size), targets)\n",
    "\n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e8970da1-e68d-4299-bd66-e48dea6ffb20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "added block _a 12 30\n",
      "added block _b 30 30\n",
      "added block _c 30 30\n",
      "added block _d 30 40\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(16, 8517)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = 29\n",
    "epochs = 120\n",
    "batch_size = 100\n",
    "learning_rate = .1\n",
    "samples = 4000\n",
    "hidden_states = [30, 30, 30, 40]\n",
    "embedding_size = 12\n",
    "\n",
    "context_length = 2**len(hidden_states)\n",
    "\n",
    "model = DeeperWavenetish(vocab_size, embedding_size, context_length, hidden_states)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "\n",
    "context_length, sum(p.nelement() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1d4369db-2d1d-472b-9739-cfa96d2a7471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.2849)\n",
      "0 0.09200000000000001\n",
      "tensor(2.1224)\n",
      "10 0.08464000000000002\n",
      "tensor(2.1027)\n",
      "20 0.07786880000000002\n",
      "tensor(2.0944)\n",
      "30 0.07163929600000002\n",
      "tensor(2.0828)\n",
      "40 0.06590815232000002\n",
      "tensor(2.0825)\n",
      "50 0.06063550013440003\n",
      "tensor(2.0813)\n",
      "60 0.05578466012364803\n",
      "tensor(2.0805)\n",
      "70 0.05132188731375619\n",
      "tensor(2.0775)\n",
      "80 0.0472161363286557\n",
      "tensor(2.0743)\n",
      "90 0.043438845422363245\n",
      "tensor(2.0697)\n",
      "100 0.039963737788574184\n",
      "tensor(2.0702)\n",
      "110 0.03676663876548825\n",
      "tensor(2.0705)\n"
     ]
    }
   ],
   "source": [
    "for ep in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    for s in range(samples):\n",
    "        x, y = sample(batch_size, context_length)\n",
    "        \n",
    "        Y = torch.tensor(y)\n",
    "        X = torch.tensor(x)\n",
    "\n",
    "        logits, loss = model.forward(X, Y)\n",
    "        \n",
    "        epoch_loss += loss.detach()\n",
    "        model.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    #just keep any epoch stuff in a no grad block\n",
    "    with torch.no_grad():\n",
    "        if ep % 10 == 0:\n",
    "            print(epoch_loss/samples)\n",
    "            learning_rate *= .92\n",
    "            print(ep, learning_rate)\n",
    "\n",
    "print(epoch_loss/samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d4ea2534-bad9-4a98-9216-137d6d97629c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ealynn\n",
      "jurashi\n",
      "join\n",
      "merris\n",
      "sena\n",
      "shaus\n",
      "banden\n",
      "keena\n",
      "caslyn\n",
      "camarah\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['c', 'a', 'm', 'a', 'r', 'a', 'h']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abced26-eb36-41e9-9dcc-403261689cec",
   "metadata": {},
   "source": [
    "### Longer context\n",
    "\n",
    "we've achieved the same learning, given ourselves a longer context \n",
    "\n",
    "things haven't got noticably better but we're still using the names dataset - \n",
    "let's read Alice again and see what our results look like in the [next chapter](wavenet_alice.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
