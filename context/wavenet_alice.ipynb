{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57ba5f38-a0cd-487d-888b-13428ca38aaa",
   "metadata": {},
   "source": [
    "## Wavnet larger corpus\n",
    "\n",
    "we'll try reading alice again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b380bfb-1cb5-4c95-89eb-ecf3fc684e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e8ffe8f-462a-457b-b480-d9078375d519",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../lib/bookreader.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2b82182-b1d7-406f-818d-c7a21a55028f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../lib/nn_layers.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258b4e8c-9b5f-40ba-ae21-c05dbc0195c0",
   "metadata": {},
   "source": [
    "### Repeat a few things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9dd1f7b4-c231-42df-9949-787c33bdd736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lowercase only\n"
     ]
    }
   ],
   "source": [
    "alice = BookReader()\n",
    "alice.read(\"../resources/alice.txt\")\n",
    "\n",
    "vocab_size = len(alice.itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e57bd246-4dce-49e4-906e-8af00ed8c5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViewDilaltion(nn.Module):\n",
    "\n",
    "    def __init__(self, n):\n",
    "        super().__init__()\n",
    "        self.n = n\n",
    "        self.type = 'flatten'\n",
    "\n",
    "    def __call__(self, x):\n",
    "        B, T, C = x.shape\n",
    "        x = x.view(B, -1, self.n * C)\n",
    "        \n",
    "        self.out = x\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b30dc6ca-2c04-447a-8106-c470c529ffc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeeperWavenetish(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_size, context_length, hidden_sizes):\n",
    "        super().__init__()\n",
    "\n",
    "        layers = OrderedDict([\n",
    "            ('embed', nn.Embedding(vocab_size, embedding_size)),\n",
    "        ])\n",
    "\n",
    "        hidden_sizes = [embedding_size] + hidden_sizes\n",
    "\n",
    "        sufixes = ['_a', '_b', '_c', '_d', '_e', '_f', '_g', '_h']\n",
    "        for i in range(len(hidden_sizes)-1):\n",
    "            self.add_block(layers, sufixes[i], hidden_sizes[i], hidden_sizes[i+1])\n",
    "        \n",
    "        layers['logits'] = nn.Linear(hidden_sizes[-1], vocab_size, bias=True)\n",
    "        nn.init.zeros_(layers['logits'].bias)\n",
    "\n",
    "        self.model = nn.Sequential(layers)\n",
    "\n",
    "    def add_block(self, layer_dict, suffix=\"_a\", fan_in=10, fan_out=20):\n",
    "        linear_layer = nn.Linear(fan_in*2, fan_out, bias=False)\n",
    "        nn.init.xavier_uniform_(linear_layer.weight, gain=5/3)\n",
    "        layer_dict['flatten'+suffix] = ViewDilaltion(2)\n",
    "        layer_dict['feed_forward'+suffix] = linear_layer\n",
    "        layer_dict['layer_norm'+suffix] = nn.LayerNorm(fan_out)\n",
    "        layer_dict['non_linearity'+suffix] = nn.Tanh()\n",
    "        print(\"added block\", suffix, fan_in, fan_out)\n",
    "        \n",
    "    def forward(self, idx, targets=None):\n",
    "        logits = self.model(idx)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            loss = F.cross_entropy(logits.view(-1, vocab_size), targets)\n",
    "\n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c26bec4-dd37-4447-b0ee-a3424ea5d8c7",
   "metadata": {},
   "source": [
    "### Run a short test first\n",
    "\n",
    "run a smaller model just to get our bearings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "977576dc-0be2-4d00-b0a3-f9e7e7c6fa6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "added block _a 6 30\n",
      "added block _b 30 30\n",
      "added block _c 30 30\n",
      "added block _d 30 40\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(16, 8030)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 20\n",
    "batch_size = 80\n",
    "learning_rate = .1\n",
    "samples = 1000\n",
    "hidden_states = [30, 30, 30, 40]\n",
    "embedding_size = 6\n",
    "\n",
    "context_length = 2**len(hidden_states)\n",
    "\n",
    "model = DeeperWavenetish(vocab_size, embedding_size, context_length, hidden_states)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "\n",
    "context_length, sum(p.nelement() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a59bdc29-23c6-4cf3-a242-20b78fc6479b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.2813)\n",
      "0 0.09200000000000001\n",
      "tensor(1.6810)\n",
      "10 0.08464000000000002\n",
      "tensor(1.6245)\n"
     ]
    }
   ],
   "source": [
    "for ep in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    for s in range(samples):\n",
    "        x, y = alice.sample_batch(batch_size, context_length)\n",
    "        \n",
    "        logits, loss = model.forward(x, y)\n",
    "        \n",
    "        epoch_loss += loss.detach()\n",
    "        model.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    #just keep any epoch stuff in a no grad block\n",
    "    with torch.no_grad():\n",
    "        if ep % 10 == 0:\n",
    "            print(epoch_loss/samples)\n",
    "            learning_rate *= .92\n",
    "            print(ep, learning_rate)\n",
    "\n",
    "print(epoch_loss/samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c3bf3f18-4376-4368-a872-4143891e2690",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(max_characters, context_length, vocab_size):\n",
    "    out = []\n",
    "    c_out = []\n",
    "    \n",
    "    ix = [[0 for _ in range(context_length)]]\n",
    "    c_ix = [[0 for _ in range(context_length)]]\n",
    "\n",
    "    for _ in range(max_characters):\n",
    "        logits = model.forward(torch.tensor(c_ix))\n",
    "\n",
    "        p = F.softmax(logits[0].view(vocab_size), dim=0)\n",
    "\n",
    "        prediction = torch.multinomial(p, num_samples=1).item()\n",
    "\n",
    "        del c_ix[0][0]\n",
    "\n",
    "        c_ix[0].append(prediction)\n",
    "\n",
    "        out.append(alice.itos[prediction])\n",
    "        c_out.append(alice.itos[prediction])\n",
    "\n",
    "    return c_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c0278699-0437-434b-a76d-ff6afc43dab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "the were windln  i she a sis\n",
      "icunse said eve mis out\n",
      "it a\n",
      "duchess im and a very pave nignt it neahliln  shociyd on inse or to tone could firp so usle rew tran  in the\n",
      "only \n",
      "\n",
      "\n",
      " see she tear hounting of as  the lare in sbout alice   the nealie   side as awlution  ele this aptering of aif  who sh\n"
     ]
    }
   ],
   "source": [
    "print(\"\".join(generate(300)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2111aa19-ad20-4735-bab3-2b00947133b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "added block _a 6 20\n",
      "added block _b 20 30\n",
      "added block _c 30 30\n",
      "added block _d 30 40\n",
      "added block _e 40 40\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(32, 10570)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 40\n",
    "batch_size = 80\n",
    "learning_rate = .1\n",
    "samples = 1000\n",
    "hidden_states = [20, 30, 30, 40, 40]\n",
    "embedding_size = 6\n",
    "\n",
    "context_length = 2**len(hidden_states)\n",
    "\n",
    "model = DeeperWavenetish(vocab_size, embedding_size, context_length, hidden_states)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "\n",
    "context_length, sum(p.nelement() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce1e070-0159-41ad-86b7-5c72a2120589",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4a4860ea-e72f-4f45-ac3a-3ba7c0e11774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3480)\n",
      "0 0.09200000000000001\n",
      "tensor(1.7291)\n",
      "10 0.08464000000000002\n",
      "tensor(1.6468)\n",
      "20 0.07786880000000002\n",
      "tensor(1.6172)\n",
      "30 0.07163929600000002\n",
      "tensor(1.5753)\n"
     ]
    }
   ],
   "source": [
    "for ep in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    for s in range(samples):\n",
    "        x, y = alice.sample_batch(batch_size, context_length)\n",
    "        \n",
    "        logits, loss = model.forward(x, y)\n",
    "        \n",
    "        epoch_loss += loss.detach()\n",
    "        model.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    #just keep any epoch stuff in a no grad block\n",
    "    with torch.no_grad():\n",
    "        if ep % 10 == 0:\n",
    "            print(epoch_loss/samples)\n",
    "            learning_rate *= .92\n",
    "            print(ep, learning_rate)\n",
    "\n",
    "print(epoch_loss/samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "34c641cb-4495-4cea-b134-d82a5a926060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "here  and agas eyeso otpen on she gar ugulter hissherals   i know mout thon allesireaded was havery  \n",
      "\n",
      "\n",
      " i such groken a lick  i near cats  i girse is look! bouse now abthing the time thrinking \n",
      "and like grak   of voice you lurkle if not her had le  one looketer  \n",
      "\n",
      "\n",
      " eut  she gink again bati     an\n"
     ]
    }
   ],
   "source": [
    "print(\"\".join(generate(300)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56058b1a-3a15-44b9-8ca9-f67eb21a78e4",
   "metadata": {},
   "source": [
    "### Promising?\n",
    "\n",
    "but we don't really have a way to evaluate these continuous texts yet \n",
    "\n",
    "compare to the original corpus:\n",
    "* we could see if it's learning real words\n",
    "* word bigram evaluation - we could see if it learns short associations\n",
    "* structure, do we get sentences, paragraphs, chapters like the original\n",
    "\n",
    "etc...\n",
    "\n",
    "lets come back to that after attention so we can compare the approaches\n",
    "\n",
    "for now lets though lets look at different variations of the model to get \n",
    "some more intuition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "83549ec0-069a-4e4f-bc6e-94567b86aa2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "added block _a 9 20\n",
      "added block _b 20 30\n",
      "added block _c 30 30\n",
      "added block _d 30 40\n",
      "added block _e 40 40\n",
      "added block _f 40 30\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(64, 13240)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 100\n",
    "batch_size = 80\n",
    "learning_rate = .1\n",
    "samples = 2000\n",
    "hidden_states = [20, 30, 30, 40, 40, 30]\n",
    "embedding_size = 9\n",
    "\n",
    "context_length = 2**len(hidden_states)\n",
    "\n",
    "model = DeeperWavenetish(vocab_size, embedding_size, context_length, hidden_states)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "\n",
    "context_length, sum(p.nelement() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "14b4f159-1a05-4f08-b2a1-2c14aaa7bb41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alice = BookReader(False)\n",
    "alice.read(\"../resources/alice.txt\")\n",
    "\n",
    "vocab_size = len(alice.itos)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f97456a-b30c-45fa-a232-0d4a2af3a66c",
   "metadata": {},
   "source": [
    "what effect does increasing vocab size have on our model size?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "304231a9-37bd-4801-a466-78b0cd3d04c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "added block _a 9 20\n",
      "added block _b 20 30\n",
      "added block _c 30 30\n",
      "added block _d 30 40\n",
      "added block _e 40 40\n",
      "added block _f 40 30\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(64, 13980)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 100\n",
    "batch_size = 80\n",
    "learning_rate = .1\n",
    "samples = 2000\n",
    "hidden_states = [20, 30, 30, 40, 40, 30]\n",
    "embedding_size = 9\n",
    "\n",
    "context_length = 2**len(hidden_states)\n",
    "\n",
    "model = DeeperWavenetish(vocab_size, embedding_size, context_length, hidden_states)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "\n",
    "context_length, sum(p.nelement() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a715ed66-fd4a-4981-876f-cf851312798c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3888)\n",
      "0 0.09200000000000001\n",
      "tensor(1.6780)\n",
      "10 0.08464000000000002\n",
      "tensor(1.6056)\n",
      "20 0.07786880000000002\n",
      "tensor(1.5573)\n",
      "30 0.07163929600000002\n",
      "tensor(1.5340)\n",
      "40 0.06590815232000002\n",
      "tensor(1.5203)\n",
      "50 0.06063550013440003\n",
      "tensor(1.5055)\n",
      "60 0.05578466012364803\n",
      "tensor(1.5005)\n",
      "70 0.05132188731375619\n",
      "tensor(1.4837)\n",
      "80 0.0472161363286557\n",
      "tensor(1.4841)\n",
      "90 0.043438845422363245\n",
      "tensor(1.4727)\n"
     ]
    }
   ],
   "source": [
    "for ep in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    for s in range(samples):\n",
    "        x, y = alice.sample_batch(batch_size, context_length)\n",
    "        \n",
    "        logits, loss = model.forward(x, y)\n",
    "        \n",
    "        epoch_loss += loss.detach()\n",
    "        model.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    #just keep any epoch stuff in a no grad block\n",
    "    with torch.no_grad():\n",
    "        if ep % 10 == 0:\n",
    "            print(epoch_loss/samples)\n",
    "            learning_rate *= .92\n",
    "            print(ep, learning_rate)\n",
    "\n",
    "print(epoch_loss/samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1c4c170a-b024-4189-9dce-d20bc4673b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Queen   Verhess \n",
      "   live ruzp alit I beginning to stelit said\n",
      "the others not a mers at one could had been theop up out aners to when were thing I know\n",
      "pudon  No  such asleet  but but I\n",
      "haves? \n",
      "\n",
      " This\n",
      "till   cond would before  whis falwoby fis moud juiaplenx on a ce did who igh like should I m  queen cautiedly\n",
      "beiges the chy k inatu RO begin a littlittle \n",
      "hemefith   He which poo indece \n",
      "hatto whiled  and she was see  or aid\n",
      "glose with hill\n",
      "of the did they stoe! Dord\n",
      " all be all\n",
      "a\n",
      "all all all see whitiited  but\n",
      "Alice at up  she was rith \n",
      "\n",
      "\n",
      " Yes  she had\n",
      "of fewended \n",
      "\n",
      "This  and all she pine ler had\n",
      "shuses  that frand! \n",
      "\n",
      "Alice thill  what\n",
      " and a little! \n",
      "Deewhere  net ping baben all obll   said! It was got was letent  and the Gryphiightece wholr them sile\n",
      "  Nettjteze was loudel and feters af nowd\n",
      "into  and I ll make in a little  and oisely little  and out pan him mutting fetchin delis best agitioned  and mething   nes icato they when  I it! Is shall beariowtiin cried a toverself \n",
      "would\n"
     ]
    }
   ],
   "source": [
    "print(\"\".join(generate(1000, 64, 56)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
