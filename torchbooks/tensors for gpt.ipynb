{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93229e23-e812-402e-89e6-38685d57985f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58065b4c-c187-4022-8326-38f69fe82581",
   "metadata": {},
   "source": [
    "## Lets play with torch for a while to get used to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d10453c-7294-4449-9c32-4a0fffa08709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a tensor\n",
      "tensor([[-1.7526, -0.6130, -1.9947,  0.8156, -1.2796],\n",
      "        [ 1.3196, -1.3459, -1.3788,  0.3734, -0.0524],\n",
      "        [ 0.8171, -0.2849,  0.9884, -0.1884, -0.7868],\n",
      "        [-0.8577, -0.2184, -0.3566, -0.0097, -0.9393]])\n",
      "first row tensor([-1.7526, -0.6130, -1.9947,  0.8156, -1.2796])\n",
      "first column\n",
      "tensor([-1.7526,  1.3196,  0.8171, -0.8577])\n",
      "tensor after changing[0][0] tensor([[ 2.4000, -0.6130, -1.9947,  0.8156, -1.2796],\n",
      "        [ 1.3196, -1.3459, -1.3788,  0.3734, -0.0524],\n",
      "        [ 0.8171, -0.2849,  0.9884, -0.1884, -0.7868],\n",
      "        [-0.8577, -0.2184, -0.3566, -0.0097, -0.9393]])\n",
      "first column using slice\n",
      "tensor([ 2.4000,  1.3196,  0.8171, -0.8577])\n",
      "a left (lower) triangluar matric\n",
      "tensor([[1., 0., 0., 0.],\n",
      "        [1., 1., 0., 0.],\n",
      "        [1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1.]])\n",
      "our original matric mutlipled by the left triangular matrix\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.4000, -0.0000, -0.0000,  0.0000],\n",
       "        [ 1.3196, -1.3459, -0.0000,  0.0000],\n",
       "        [ 0.8171, -0.2849,  0.9884, -0.0000],\n",
       "        [-0.8577, -0.2184, -0.3566, -0.0097]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "four_by_five = torch.randn(4, 5)\n",
    "print(\"a tensor\")\n",
    "print(four_by_five)\n",
    "print(\"first row\", four_by_five[0])\n",
    "\n",
    "print(\"first column\")\n",
    "col1 = torch.select(four_by_five, 1, 0)\n",
    "print(col1)\n",
    "\n",
    "col1[0] = 2.4\n",
    "print(\"tensor after changing[0][0]\", four_by_five)\n",
    "print(\"first column using slice\")\n",
    "print(four_by_five[:,0])\n",
    "\n",
    "four_by_four = four_by_five[:,0:4]\n",
    "\n",
    "tril = torch.tril(torch.ones(4,4))\n",
    "print(\"a left (lower) triangluar matric\")\n",
    "print(tril)\n",
    "\n",
    "print(\"our original matric mutlipled by the left triangular matrix\") \n",
    "four_by_four * tril"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817c3d46-7fdd-4f1c-b605-d066cf964218",
   "metadata": {},
   "source": [
    "## B, T, C\n",
    "\n",
    "what's this B, T, C stuff?\n",
    "\n",
    "Lets use it to get used to some more stuff\n",
    "\n",
    "it's maybe not super clear\n",
    "* B - is the Batch Dimension\n",
    "\n",
    "so if we have 3 examples in a Batch and each example is 4 tokens long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5926a161-7953-4adb-8cb3-086699c53e9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6932,  1.1297,  0.1804,  2.0001],\n",
       "        [-1.6543,  0.1379,  1.9224,  0.8588],\n",
       "        [ 0.9539,  1.3666, -0.7944, -0.3785]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BT = torch.randn(3, 4)\n",
    "BT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23de36cf-3f05-4b6c-a76d-910f709c4c6d",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "so the first example in the batch is BT[0], the second BT[1] etc...\n",
    "\n",
    "so the first index place is the B (batch) and the chosen index is the example\n",
    "\n",
    "* T is for time\n",
    "\n",
    "then within the example T is the Time dimension refering to it this way is confusing in our language model\n",
    "\n",
    "what's time got to do with it?\n",
    "\n",
    "it's from thinking of the example as a sequence, one element following another (Like as the waves make towards the pebbl'd shore)\n",
    "\n",
    "so if we have a 3x4 batch:\n",
    "\n",
    "[ \n",
    " [w, h, e, n],\n",
    " [b. o, t, h],\n",
    " [w, h, a, t],\n",
    "]\n",
    "\n",
    "here we can see the time dimension for the first example in the batch \n",
    "\n",
    "| B       | T        |          |\n",
    "| :-----: | :------: | :------: |\n",
    "| [0]     | [0]      | -> w     |\n",
    "| [0]     | [1]      | -> h     |\n",
    "| [0]     | [2]      | -> e     |\n",
    "| [0]     | [3]      | -> n     |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6206fea2-40e9-43db-b82e-c43b566c29a4",
   "metadata": {},
   "source": [
    "## C is for Channel\n",
    "say we've embedded our letters in a two dimensional space, each dimension is a channel\n",
    "\n",
    "or in an image, a pixel could have 3 colour 'channels' RGB\n",
    "\n",
    "we'll start with 2 channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e4507c0-48df-449f-9c9b-232e9e9bfe06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4868,  0.3267],\n",
       "         [ 0.0085,  0.1575],\n",
       "         [ 0.9436,  0.0135],\n",
       "         [ 0.6672,  0.1382]],\n",
       "\n",
       "        [[-1.0947, -0.7651],\n",
       "         [-0.5687, -0.6855],\n",
       "         [-0.7222,  1.0697],\n",
       "         [ 0.5297, -1.1687]],\n",
       "\n",
       "        [[ 0.9568, -0.7414],\n",
       "         [-0.1002,  0.5613],\n",
       "         [ 0.0028,  1.1634],\n",
       "         [-0.5339, -1.4513]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BTC = torch.randn(3, 4, 2)\n",
    "BTC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d43bbdc-11b5-41dd-9f46-2035edd9bb9d",
   "metadata": {},
   "source": [
    "so now our 'w' in when is represented by 2 numbers - a duple, the first element of the sequence of the first example in the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b560866-0560-4654-afdb-a4ee1b60326e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w tensor([-0.4868,  0.3267])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('X', tensor(-0.4868), 'Y', tensor(0.3267))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = BTC[0, 0]\n",
    "print(\"w\", w)\n",
    "w_ChannelX = w[0]\n",
    "w_ChannelY = w[1]\n",
    "\"X\", w_ChannelX, \"Y\", w_ChannelY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e3729f-7251-4d2b-a361-358169f48070",
   "metadata": {},
   "source": [
    "## Read a book\n",
    "lets read some lines from a book and play with them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60286326-e1a1-48ac-8201-f6039bd87abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../lib/bookreader.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3958b9c2-bdb9-489d-a510-2e7b732cc3d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lowercase only\n",
      "number of symbols 29 lengths of our train, dev, test splits 2525 0 0\n",
      "the symbols with their int representations (zero is a newline)\n",
      "0:\n",
      ", 1: , 2:!, 3:?, 4:a, 5:b, 6:c, 7:d, 8:e, 9:f, 10:g, 11:h, 12:i, 13:j, 14:k, 15:l, 16:m, 17:n, 18:o, 19:p, 20:q, 21:r, 22:s, 23:t, 24:u, 25:v, 26:w, 27:x, 28:y, \n",
      "the text with each character encoded to an int in our lookup\n",
      "tensor([ 4, 15, 12,  6,  8,  1, 26,  4, 22,  1,  5,  8, 10, 12, 17, 17, 12, 17,\n",
      "        10,  1, 23, 18,  1, 10,  8, 23,  1, 25,  8, 21, 28,  1, 23, 12, 21,  8,\n",
      "         7,  1, 18,  9,  1, 22, 12, 23, 23, 12, 17, 10,  1,  5, 28,  1, 11,  8,\n",
      "        21,  1, 22, 12, 22, 23,  8, 21,  1, 18, 17,  1, 23, 11,  8,  0,  5,  4,\n",
      "        17, 14,  1,  1,  4, 17,  7,  1, 18,  9,  1, 11,  4, 25, 12, 17, 10,  1,\n",
      "        17, 18, 23, 11, 12, 17, 10,  1, 23, 18,  1,  7, 18,  1,  1, 18, 17,  6,\n",
      "         8,  1, 18, 21,  1, 23, 26, 12,  6,  8,  1, 22, 11,  8,  1, 11,  4,  7,\n",
      "         1, 19,  8,  8, 19,  8,  7,  1, 12, 17, 23, 18,  0, 23, 11,  8,  1,  5,\n",
      "        18, 18, 14,  1, 11,  8, 21,  1, 22, 12, 22, 23,  8, 21,  1, 26,  4, 22,\n",
      "         1, 21,  8,  4,  7, 12, 17, 10,  1,  1,  5, 24, 23,  1, 12, 23,  1, 11,\n",
      "         4,  7,  1, 17, 18,  1, 19, 12,  6, 23, 24, 21,  8, 22,  1, 18, 21,  0,\n",
      "         6, 18, 17, 25,  8, 21, 22,  4, 23, 12, 18, 17, 22,  1, 12, 17,  1, 12,\n",
      "        23,  1,  1,  1,  4, 17,  7,  1, 26, 11,  4, 23,  1, 12, 22,  1, 23, 11,\n",
      "         8,  1, 24, 22,  8,  1, 18,  9,  1,  4,  1,  5, 18, 18, 14,  1,  1,  1,\n",
      "        23, 11, 18, 24, 10, 11, 23,  1,  4, 15, 12,  6,  8,  0,  1, 26, 12, 23,\n",
      "        11, 18, 24, 23,  1, 19, 12,  6, 23, 24, 21,  8, 22,  1, 18, 21,  1,  6,\n",
      "        18, 17, 25,  8, 21, 22,  4, 23, 12, 18, 17, 22,  3,  1,  0,  0, 22, 18,\n",
      "         1, 22, 11,  8,  1, 26,  4, 22,  1,  6, 18, 17, 22, 12,  7,  8, 21, 12,\n",
      "        17, 10,  1, 12, 17,  1, 11,  8, 21,  1, 18, 26, 17,  1, 16, 12, 17,  7,\n",
      "         1,  1,  4, 22,  1, 26,  8, 15, 15,  1,  4, 22,  1, 22, 11,  8,  1,  6,\n",
      "        18, 24, 15,  7,  1,  1,  9, 18, 21,  1, 23, 11,  8,  0, 11, 18, 23,  1,\n",
      "         7,  4, 28,  1, 16,  4,  7,  8,  1, 11,  8, 21,  1,  9,  8,  8, 15,  1,\n",
      "        25,  8, 21, 28])\n",
      "the text decoded from the int representations\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'alice was beginning to get very tired of sitting by her sister on the\\nbank  and of having nothing to do  once or twice she had peeped into\\nthe book her sister was reading  but it had no pictures or\\nconversations in it   and what is the use of a book   thought alice\\n without pictures or conversations? \\n\\nso she was considering in her own mind  as well as she could  for the\\nhot day made her feel very'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "br = BookReader()\n",
    "\n",
    "br.read(\"../resources/alice.txt\")\n",
    "\n",
    "print(\"number of symbols\", br.vocab_size, \"lengths of our train, dev, test splits\", len(br.data[0]), len(br.data[1]), len(br.data[2]))\n",
    "print(\"the symbols with their int representations (zero is a newline)\")\n",
    "symbols = \"\"\n",
    "for i, c in enumerate(br.chars):\n",
    "    symbols += f\"{i}:{c}, \"\n",
    "print(symbols)\n",
    "encodedText = torch.tensor(br.data[0][:400])\n",
    "print(\"the text with each character encoded to an int in our lookup\")\n",
    "print(encodedText)\n",
    "print(\"the text decoded from the int representations\")\n",
    "br.decode(encodedText.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c10f7bb-5792-476f-accc-2225d9c0da7a",
   "metadata": {},
   "source": [
    "## Data loading\n",
    "typically we want to get batches of a given sequence length from one of our train, dev, test data sets\n",
    "\n",
    "for sequence length we want the combined length of the example and the target - we tend to use x and y in our code\n",
    "\n",
    "so for our texts the examples each have context length number of symbols and the labels are what follows (typically a single symbol in our simple models)\n",
    "\n",
    "so to use get_batch to get a batch 5 sequences which each contain 5 symbols and their labels of length 1 we can call\n",
    "\n",
    "get_batch(train, 6, 5)\n",
    "\n",
    "below we show we can choose any value >=6 and use splits to form our x and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a47c9175-1966-4d1a-b1c9-1789cbf7ad9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[22, 12, 22, 23,  8, 21,  1, 26,  4, 22,  1, 21,  8,  4,  7, 12, 17, 10,\n",
       "          1,  1,  5, 24, 23,  1, 12, 23,  1, 11,  4,  7,  1, 17],\n",
       "        [ 4, 15, 12,  6,  8,  1, 26,  4, 22,  1,  5,  8, 10, 12, 17, 17, 12, 17,\n",
       "         10,  1, 23, 18,  1, 10,  8, 23,  1, 25,  8, 21, 28,  1],\n",
       "        [ 8, 21, 22,  4, 23, 12, 18, 17, 22,  3,  1,  0,  0, 22, 18,  1, 22, 11,\n",
       "          8,  1, 26,  4, 22,  1,  6, 18, 17, 22, 12,  7,  8, 21],\n",
       "        [ 4, 15, 12,  6,  8,  1, 26,  4, 22,  1,  5,  8, 10, 12, 17, 17, 12, 17,\n",
       "         10,  1, 23, 18,  1, 10,  8, 23,  1, 25,  8, 21, 28,  1],\n",
       "        [ 8,  1, 26,  4, 22,  1,  5,  8, 10, 12, 17, 17, 12, 17, 10,  1, 23, 18,\n",
       "          1, 10,  8, 23,  1, 25,  8, 21, 28,  1, 23, 12, 21,  8]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data loading\n",
    "def get_batch(data, batch_length=5, batch_size=5):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    ix = torch.randint(len(data) - batch_length, (batch_size,))\n",
    "    b = torch.stack([data[i:i+batch_length] for i in ix])\n",
    "    return b\n",
    "\n",
    "big_size = 32\n",
    "batch_one = get_batch(encodedText, big_size)\n",
    "batch_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af497a10-a59b-48fb-9039-f657863eb06e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b:  sister was reading  but it had n\n",
      "b:  alice was beginning to get very \n",
      "b:  ersations? \n",
      "\n",
      "so she was consider\n",
      "b:  alice was beginning to get very \n",
      "b:  e was beginning to get very tire\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(\"b: \", br.decode(batch_one[i].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7215a20-4577-4f54-937d-001afde50b22",
   "metadata": {},
   "source": [
    "## X and Y\n",
    "\n",
    "so we can decide after what we want to do with the batch we got\n",
    "\n",
    "from above we know we can index into things easily\n",
    "\n",
    "so we can create an X batch and their Ys as we like, say we wanted Xs of length 4 and we wanted to see Ys of length 3 in our cross_entropy\n",
    "\n",
    "(we just do this so it's easy to see our Ys are the correct following letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d42dda00-1b8f-43c6-8d1d-e4e05637f26c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[22, 12, 22, 23],\n",
      "        [ 4, 15, 12,  6],\n",
      "        [ 8, 21, 22,  4],\n",
      "        [ 4, 15, 12,  6],\n",
      "        [ 8,  1, 26,  4]]) tensor([[ 8, 21,  1],\n",
      "        [ 8,  1, 26],\n",
      "        [23, 12, 18],\n",
      "        [ 8,  1, 26],\n",
      "        [22,  1,  5]])\n",
      "xy:  sist ->  er \n",
      "xy:  alic ->  e w\n",
      "xy:  ersa ->  tio\n",
      "xy:  alic ->  e w\n",
      "xy:  e wa ->  s b\n"
     ]
    }
   ],
   "source": [
    "context_length = 4\n",
    "x = batch_one[:, 0:context_length]\n",
    "y = batch_one[:, context_length:context_length+3]\n",
    "print(x, y)\n",
    "for i in range(5):\n",
    "    print(\"xy: \", br.decode(x[i].tolist()), \"-> \", br.decode(y[i].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c1ca0b-41d5-4ae0-a0de-b0a4ed98a163",
   "metadata": {},
   "source": [
    "## or just do it all at once..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3b321cfc-bb47-483f-8193-5e0be93756b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xy:  e or ->   \n",
      "xy:   con ->  s\n",
      "xy:   con ->  s\n",
      "xy:  ing  ->  i\n",
      "xy:  ter  ->  w\n"
     ]
    }
   ],
   "source": [
    "def get_labeled_batch(data, context_length=4, label_length=1, batch_size=5):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    ix = torch.randint(len(data) - context_length, (batch_size,))\n",
    "    b = torch.stack([data[i:i+context_length+label_length] for i in ix])\n",
    "    \n",
    "    x = b[:, 0:context_length]\n",
    "    y = b[:, context_length:context_length+label_length]\n",
    "    return x, y\n",
    "\n",
    "X, Y = get_labeled_batch(encodedText)\n",
    "for i in range(5):\n",
    "    print(\"xy: \", br.decode(X[i].tolist()), \"-> \", br.decode(Y[i].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ab28a6-83ae-43ca-a1bb-998725e3c9e2",
   "metadata": {},
   "source": [
    "## multiple examples from a single batch element\n",
    "\n",
    "to get more out of our context we go back to the get_batch() implementation because we want to make multiple examples from a single example \n",
    "\n",
    "looking at a sequence 'alice ' we can form the following\n",
    "* a.... -> e\n",
    "* al... -> i\n",
    "* ali.. -> c\n",
    "* alic. -> e\n",
    "* alice ->\n",
    "\n",
    "so now we want to form the labels for all 5 all at once ['e', 'i', 'c', 'e', ' ']\n",
    "and we can see if we get context_length+1 our example is just [:-1] and our label is just [1:] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e01663-283f-494e-b463-b9ad54dacc3f",
   "metadata": {},
   "source": [
    "## a shortcut to mushrooms\n",
    "\n",
    "we can form these examples using a triangular matric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ffddccf-19fa-4094-80c8-aacda241426f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0, 0],\n",
       "        [1, 1, 0, 0],\n",
       "        [1, 1, 1, 0],\n",
       "        [1, 1, 1, 1]], dtype=torch.int32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr = torch.tril(torch.ones((4, 4), dtype=torch.int))\n",
    "tr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f5870f-a802-470b-9467-f5231d7b95e5",
   "metadata": {},
   "source": [
    "if alice was encoded [a->1, l->2, i->3, c->4, e->5]\n",
    "\n",
    "then using our tensor tr we get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e6560ef2-e564-4f9a-a344-96f464b99ffd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0, 0],\n",
       "        [1, 2, 0, 0],\n",
       "        [1, 2, 3, 0],\n",
       "        [1, 2, 3, 4]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "al = torch.tensor(range(1, 5))\n",
    "st = al.repeat(1, 4)\n",
    "bt = al * tr\n",
    "bt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9938307-55fa-4bce-a9bd-041254b22edd",
   "metadata": {},
   "source": [
    "## We can batch our batch too \n",
    "\n",
    "using a 3 dimensional triangular tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a816cdfe-66ef-462f-95b8-efd6562456e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 4,  1,  5, 18,  4,  1,  5, 18,  4,  1,  5, 18,  4,  1,  5, 18],\n",
      "         [18, 17, 22, 12, 18, 17, 22, 12, 18, 17, 22, 12, 18, 17, 22, 12],\n",
      "         [17,  1, 11,  8, 17,  1, 11,  8, 17,  1, 11,  8, 17,  1, 11,  8],\n",
      "         [18,  1, 22, 11, 18,  1, 22, 11, 18,  1, 22, 11, 18,  1, 22, 11],\n",
      "         [ 1, 22, 12, 22,  1, 22, 12, 22,  1, 22, 12, 22,  1, 22, 12, 22]]]) torch.Size([1, 5, 16])\n",
      "tensor([[[ 4,  1,  5, 18],\n",
      "         [ 4,  1,  5, 18],\n",
      "         [ 4,  1,  5, 18],\n",
      "         [ 4,  1,  5, 18]],\n",
      "\n",
      "        [[18, 17, 22, 12],\n",
      "         [18, 17, 22, 12],\n",
      "         [18, 17, 22, 12],\n",
      "         [18, 17, 22, 12]],\n",
      "\n",
      "        [[17,  1, 11,  8],\n",
      "         [17,  1, 11,  8],\n",
      "         [17,  1, 11,  8],\n",
      "         [17,  1, 11,  8]],\n",
      "\n",
      "        [[18,  1, 22, 11],\n",
      "         [18,  1, 22, 11],\n",
      "         [18,  1, 22, 11],\n",
      "         [18,  1, 22, 11]],\n",
      "\n",
      "        [[ 1, 22, 12, 22],\n",
      "         [ 1, 22, 12, 22],\n",
      "         [ 1, 22, 12, 22],\n",
      "         [ 1, 22, 12, 22]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 4,  0,  0,  0],\n",
       "         [ 4,  1,  0,  0],\n",
       "         [ 4,  1,  5,  0],\n",
       "         [ 4,  1,  5, 18]],\n",
       "\n",
       "        [[18,  0,  0,  0],\n",
       "         [18, 17,  0,  0],\n",
       "         [18, 17, 22,  0],\n",
       "         [18, 17, 22, 12]],\n",
       "\n",
       "        [[17,  0,  0,  0],\n",
       "         [17,  1,  0,  0],\n",
       "         [17,  1, 11,  0],\n",
       "         [17,  1, 11,  8]],\n",
       "\n",
       "        [[18,  0,  0,  0],\n",
       "         [18,  1,  0,  0],\n",
       "         [18,  1, 22,  0],\n",
       "         [18,  1, 22, 11]],\n",
       "\n",
       "        [[ 1,  0,  0,  0],\n",
       "         [ 1, 22,  0,  0],\n",
       "         [ 1, 22, 12,  0],\n",
       "         [ 1, 22, 12, 22]]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_length = 4\n",
    "batch_one = get_batch(encodedText, context_length + 1)\n",
    "x = batch_one[:, 0:context_length]\n",
    "\n",
    "tr = torch.tril(torch.ones((5, 4, 4), dtype=torch.int))\n",
    "\n",
    "mult = x.repeat(1, 1, 4)\n",
    "print(mult, mult.shape)\n",
    "print(mult.view(5, 4, 4))\n",
    "\n",
    "lower_batch = mult.view(5, 4, 4) * tr\n",
    "lower_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f346d317-5d40-4813-9e93-bf331c2be1a4",
   "metadata": {},
   "source": [
    "Now say we put these through something to calculate the next character all the examples in each batch \n",
    "\n",
    "we can see that the solution for the first batch, batch is as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cd750a2e-5a46-4b5c-8477-3a0545ef719b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 4,  0,  0,  0],\n",
      "        [ 4,  1,  0,  0],\n",
      "        [ 4,  1,  5,  0],\n",
      "        [ 4,  1,  5, 18]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 1,  5, 18, 18])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(lower_batch[0])\n",
    "batch_no = 0\n",
    "y = batch_one[batch_no, batch_no+1: batch_no+context_length+1]\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "699417ce-094d-4be1-9dc5-6093f1215d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 4, 4]) torch.Size([5, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([ 4,  0,  0,  0,  4,  1,  0,  0,  4,  1,  5,  0,  4,  1,  5, 18, 18,  0,\n",
       "          0,  0, 18, 17,  0,  0, 18, 17, 22,  0, 18, 17, 22, 12, 17,  0,  0,  0,\n",
       "         17,  1,  0,  0, 17,  1, 11,  0, 17,  1, 11,  8, 18,  0,  0,  0, 18,  1,\n",
       "          0,  0, 18,  1, 22,  0, 18,  1, 22, 11,  1,  0,  0,  0,  1, 22,  0,  0,\n",
       "          1, 22, 12,  0,  1, 22, 12, 22]),\n",
       " tensor([ 1,  5, 18, 18, 17, 22, 12,  7,  1, 11,  8, 21,  1, 22, 11,  8, 22, 12,\n",
       "         22, 23]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys = y = batch_one[:, batch_no+1: batch_no+context_length+1]\n",
    "\n",
    "print(lower_batch.shape, ys.shape)\n",
    "\n",
    "lower_batch.view(-1), ys.reshape(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d70489-f8fb-489c-86e0-e28b740b69ab",
   "metadata": {},
   "source": [
    "## Next\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
