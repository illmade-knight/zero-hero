{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14b5cd7b-bbea-4769-8512-e1c440795bf2",
   "metadata": {},
   "source": [
    "## continuous data\n",
    "\n",
    "experiment with feeding the model different data\n",
    "\n",
    "view our names as a continuous 'text' of words separated by spaces - we'll sample randomly from the text\n",
    "\n",
    "our evaluation will not be directly comparable but it'll prepare us for reading in books in the next section"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e7065e-4b63-46d3-9c5c-bdc3ce1f2782",
   "metadata": {},
   "source": [
    "### standard setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7eabc6c-08fb-4ae2-b7ab-26f664ad82d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import collections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ad4688-4d82-46a3-9e5e-cbf168387f73",
   "metadata": {},
   "source": [
    "### new sampling\n",
    "continuous sampling from a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4745ac40-30b7-4e76-b0ae-a72e5ea3d498",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../lib/continuous_sampling.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33b59c17-863b-4377-ad06-16d8a2babbb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_length = 5\n",
    "\n",
    "names = WordSampling()\n",
    "names.from_file(\"../resources/names.txt\")\n",
    "\n",
    "vocab_size = len(names.itos)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fe6a9b5-dd3a-4df2-803e-0728a1fba01a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[18,  5, 22,  1,  0],\n",
       "         [ 9, 25,  1, 14, 14],\n",
       "         [ 1,  0,  5, 13,  2],\n",
       "         [ 1, 11,  1, 18,  9],\n",
       "         [ 5,  0, 26,  5,  4]]),\n",
       " tensor([20,  1,  5,  0,  4]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names.sample_batch(Split.TRAIN, 5, context_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32455bf2-178f-4d5f-b0c2-34341d9baab7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Neural net lib\n",
    "\n",
    "same as last time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5f484c3-3373-4cb7-a77b-c0b15de3dfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNames(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_size = 6, hidden_size = 100, context_length = 5, vocab_size = 27):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.context_length = context_length\n",
    "        \n",
    "        layer_dict = collections.OrderedDict([\n",
    "            ('embed', nn.Embedding(vocab_size, embedding_size)),\n",
    "            ('flatten', nn.Flatten(1)),\n",
    "            ('feed_forward', nn.Linear(embedding_size*context_length, hidden_size, bias=True)),\n",
    "            ('non_linearity', nn.Tanh()),\n",
    "            ('logits', nn.Linear(hidden_size, vocab_size, bias=True)),\n",
    "        ])\n",
    "\n",
    "        nn.init.xavier_uniform_(layer_dict['feed_forward'].weight, gain=5/3)\n",
    "        nn.init.zeros_(layer_dict['feed_forward'].bias)\n",
    "        \n",
    "        nn.init.zeros_(layer_dict['logits'].bias)\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            layer_dict\n",
    "        )\n",
    "        self.layer_dict = layer_dict\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        logits = self.model(idx)\n",
    "    \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, num_names):\n",
    "        new_names = []\n",
    "        for i in range(num_names):\n",
    "            out = []\n",
    "            ix = [[0 for _ in range(self.context_length)]]\n",
    "            for nl in range(10):\n",
    "                logits = self.model(torch.tensor(ix))\n",
    "\n",
    "                p = F.softmax(logits, dim=1)\n",
    "        \n",
    "                prediction = torch.multinomial(p, num_samples=1).item()\n",
    "    \n",
    "                for i in range(self.context_length-1):\n",
    "                    ix[0][i]= ix[0][i+1]\n",
    "                    \n",
    "                ix[0][self.context_length-1] = prediction\n",
    "    \n",
    "                if prediction == 0:\n",
    "                    break\n",
    "                out.append(names.itos[prediction])\n",
    "                \n",
    "            new_names.append(\"\".join(out))\n",
    "\n",
    "        return new_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c61274f-233c-4228-b036-79e08c87b7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestRig:\n",
    "\n",
    "    def __init__(self, embedding_size, hidden_size, context_length=5, learning_rate = .2):\n",
    "        self.context_length = context_length\n",
    "        \n",
    "        model = SimpleNames(embedding_size, hidden_size, context_length)\n",
    "\n",
    "        parameters = model.parameters()\n",
    "        print(model.embedding_size, model.hidden_size, model.context_length, sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "\n",
    "        self.model = model\n",
    "        self.optimizer = optim.SGD(self.model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "        self.scheduler = optim.lr_scheduler.MultiplicativeLR(self.optimizer, lr_lambda=lmbda)\n",
    "        \n",
    "        self.track = {'gradients': [], 'loss': [], 'learning_rate': []}\n",
    "\n",
    "    def train(self, epochs, batch_size, samples):\n",
    "        for ep in range(epochs):\n",
    "            epoch_loss = 0\n",
    "            for s in range(samples):\n",
    "                X, Y = names.sample_batch(Split.TRAIN, batch_size, self.context_length)\n",
    "                logits, loss = self.model.forward(X, Y)\n",
    "                \n",
    "                epoch_loss += loss.item()\n",
    "                self.model.zero_grad(set_to_none=True)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "        \n",
    "            self.scheduler.step()\n",
    "        \n",
    "            if ep % 10 == 0:\n",
    "                self.track['loss'].append(epoch_loss)\n",
    "                self.track['learning_rate'].append(self.scheduler.get_last_lr())\n",
    "                print(\"learning rate\", ep, self.scheduler.get_last_lr())\n",
    "                print(\"running loss\", epoch_loss/samples)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def val_split(self, split):\n",
    "        batch_length = self.context_length * 20\n",
    "\n",
    "        batches = len(split) // batch_length\n",
    "\n",
    "        val_loss = 0\n",
    "        \n",
    "        ix = [[0 for _ in range(self.context_length-1)]]\n",
    "        for i in range(batches):\n",
    "            offset = i * batch_length\n",
    "            xs = []\n",
    "            ys = []\n",
    "            for bi in range(batch_length):\n",
    "                xs.append(split[offset+bi:offset+bi+self.context_length])\n",
    "                ys.append(split[offset+bi+self.context_length])\n",
    "        \n",
    "            X = torch.tensor(xs)\n",
    "            Y = torch.tensor(ys)\n",
    "    \n",
    "            logits, loss = self.model.forward(X, Y)\n",
    "        \n",
    "            val_loss += loss\n",
    "        \n",
    "        return val_loss / batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f2442e-f2f8-4606-b7fd-29b33522329d",
   "metadata": {},
   "source": [
    "### Run new data\n",
    "\n",
    "the model is the same but we've changed what the data looks like - it's a test to see what might happen with running a book instead of words through our model\n",
    "\n",
    "Our running loss looks smaller but this is not the same as previously - the model 'learns' we have a lot more chance of 0s at the end of words and so the loss seems artifically smaller\n",
    "\n",
    "We keep our evaluation model the same as before though so we can directly compare the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad2b9aa5-6878-482a-a51e-1d5145e62b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets see it run first\n",
    "epochs = 1\n",
    "batch_size = 5\n",
    "samples = 4\n",
    "\n",
    "lmbda = lambda epoch: 0.98"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e5ccc31-5439-4ea0-8523-a2cf340e5454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 60 5 2688\n",
      "learning rate 0 [0.196]\n",
      "running loss 3.441326081752777\n"
     ]
    }
   ],
   "source": [
    "tr = TestRig(3, 60)\n",
    "tr.train(epochs, batch_size, samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5995aadf-50eb-4fdb-a677-ba96bc0cedea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run for longer\n",
    "epochs = 30\n",
    "batch_size = 20\n",
    "samples = 200\n",
    "\n",
    "lmbda = lambda epoch: 0.98"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c418cf00-b4dd-4323-ab6e-810106c089c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 60 5 2688\n",
      "learning rate 0 [0.196]\n",
      "running loss 2.956515581607819\n",
      "learning rate 10 [0.16014627014995916]\n",
      "running loss 2.599955712556839\n",
      "learning rate 20 [0.13085116246399847]\n",
      "running loss 2.581373880505562\n",
      "tensor(2.4888)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['tunego',\n",
       " 'yysea',\n",
       " 'vydlerin',\n",
       " 'aleal',\n",
       " 'ysath',\n",
       " 'ylata',\n",
       " 'yylen',\n",
       " 'dshak',\n",
       " 'yykthyi',\n",
       " 'yiam']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr = TestRig(3, 60)\n",
    "tr.train(epochs, batch_size, samples)\n",
    "print(tr.val_split(names.data[1]))\n",
    "tr.model.generate(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc39b550-2c9c-4344-ac97-a1398910109f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now keep these values for a few runs\n",
    "epochs = 140\n",
    "batch_size = 80\n",
    "samples = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7b855ac-ebd9-4a33-966d-7c648bf92f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 90 5 3978\n",
      "learning rate 0 [0.196]\n",
      "running loss 2.438344991862774\n",
      "learning rate 10 [0.16014627014995916]\n",
      "running loss 2.339213436305523\n",
      "learning rate 20 [0.13085116246399847]\n",
      "running loss 2.3264740296006203\n",
      "learning rate 30 [0.10691492659895763]\n",
      "running loss 2.298908307373524\n",
      "learning rate 40 [0.08735727917438633]\n",
      "running loss 2.289029165148735\n",
      "learning rate 50 [0.07137725729707488]\n",
      "running loss 2.2648463631272318\n",
      "learning rate 60 [0.058320415967655595]\n",
      "running loss 2.2473923162817955\n",
      "learning rate 70 [0.047652025973541665]\n",
      "running loss 2.234460784971714\n",
      "learning rate 80 [0.03893517461607997]\n",
      "running loss 2.219443362057209\n",
      "learning rate 90 [0.03181287241021722]\n",
      "running loss 2.2139969464540483\n",
      "learning rate 100 [0.025993432955371577]\n",
      "running loss 2.1992474996447564\n",
      "learning rate 110 [0.021238527225488715]\n",
      "running loss 2.1922692571878435\n",
      "learning rate 120 [0.017353423054287647]\n",
      "running loss 2.1756745886802675\n",
      "learning rate 130 [0.014179010084073873]\n",
      "running loss 2.172455868780613\n",
      "['elila', 'krylon', 'lomeh', 'keovyn', 'reden', 'liazna', 'lotsi', 'liorw', 'reile', 'kerrick']\n",
      "tensor(2.1952)\n"
     ]
    }
   ],
   "source": [
    "tr = TestRig(3, 90)\n",
    "tr.train(epochs, batch_size, samples)\n",
    "print(tr.model.generate(10))\n",
    "print(tr.val_split(names.data[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ef7aecf-6bf8-45cb-9af4-a5a6ec5634fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 120 5 7149\n",
      "learning rate 0 [0.196]\n",
      "running loss 2.420335504949093\n",
      "learning rate 10 [0.16014627014995916]\n",
      "running loss 2.322846375703812\n",
      "learning rate 20 [0.13085116246399847]\n",
      "running loss 2.281452134013176\n",
      "learning rate 30 [0.10691492659895763]\n",
      "running loss 2.2689787284731864\n",
      "learning rate 40 [0.08735727917438633]\n",
      "running loss 2.238318187892437\n",
      "learning rate 50 [0.07137725729707488]\n",
      "running loss 2.2216697558760643\n",
      "learning rate 60 [0.058320415967655595]\n",
      "running loss 2.2086603502035143\n",
      "learning rate 70 [0.047652025973541665]\n",
      "running loss 2.188600591659546\n",
      "learning rate 80 [0.03893517461607997]\n",
      "running loss 2.1705390880107878\n",
      "learning rate 90 [0.03181287241021722]\n",
      "running loss 2.148191851198673\n",
      "learning rate 100 [0.025993432955371577]\n",
      "running loss 2.138000304877758\n",
      "learning rate 110 [0.021238527225488715]\n",
      "running loss 2.1209690326452257\n",
      "learning rate 120 [0.017353423054287647]\n",
      "running loss 2.1127639092803\n",
      "learning rate 130 [0.014179010084073873]\n",
      "running loss 2.102838237166405\n",
      "['rodik', 'elbelliee', 'bracylon', 'gerneeetn', 'tizia', 'italin', 'dychya', 'diyla', 'ylie', 'ruvi']\n",
      "tensor(2.1503)\n"
     ]
    }
   ],
   "source": [
    "tr = TestRig(6, 120)\n",
    "tr.train(epochs, batch_size, samples)\n",
    "print(tr.model.generate(10))\n",
    "print(tr.val_split(names.data[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0e2024fd-7b48-491a-8815-c8d2223e6713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 120 5 9030\n",
      "learning rate 0 [0.196]\n",
      "running loss 2.42196000123024\n",
      "learning rate 10 [0.16014627014995916]\n",
      "running loss 2.310877370238304\n",
      "learning rate 20 [0.13085116246399847]\n",
      "running loss 2.269715944111347\n",
      "learning rate 30 [0.10691492659895763]\n",
      "running loss 2.248701598584652\n",
      "learning rate 40 [0.08735727917438633]\n",
      "running loss 2.2197221710681916\n",
      "learning rate 50 [0.07137725729707488]\n",
      "running loss 2.187728522002697\n",
      "learning rate 60 [0.058320415967655595]\n",
      "running loss 2.175956507444382\n",
      "learning rate 70 [0.047652025973541665]\n",
      "running loss 2.1497475920319555\n",
      "learning rate 80 [0.03893517461607997]\n",
      "running loss 2.1243366671204567\n",
      "learning rate 90 [0.03181287241021722]\n",
      "running loss 2.110474820911884\n",
      "learning rate 100 [0.025993432955371577]\n",
      "running loss 2.0954085856080056\n",
      "learning rate 110 [0.021238527225488715]\n",
      "running loss 2.0825293553471567\n",
      "learning rate 120 [0.017353423054287647]\n",
      "running loss 2.067236022174358\n",
      "learning rate 130 [0.014179010084073873]\n",
      "running loss 2.056269667327404\n",
      "['powor', 'byline', 'parshawar', 'pnoze', 'nellian', 'cari', 'corgia', 'hovanna', 'cevins', 'qminci']\n",
      "tensor(2.1338)\n"
     ]
    }
   ],
   "source": [
    "tr = TestRig(9, 120)\n",
    "tr.train(epochs, batch_size, samples)\n",
    "print(tr.model.generate(10))\n",
    "print(tr.val_split(names.data[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "35c97443-d134-4e6a-8eb9-f42b4da49c31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 120 5 10911\n",
      "learning rate 0 [0.196]\n",
      "running loss 2.4420952622294427\n",
      "learning rate 10 [0.16014627014995916]\n",
      "running loss 2.2964034487009046\n",
      "learning rate 20 [0.13085116246399847]\n",
      "running loss 2.264759113907814\n",
      "learning rate 30 [0.10691492659895763]\n",
      "running loss 2.2270787361860274\n",
      "learning rate 40 [0.08735727917438633]\n",
      "running loss 2.202738694310188\n",
      "learning rate 50 [0.07137725729707488]\n",
      "running loss 2.169598310291767\n",
      "learning rate 60 [0.058320415967655595]\n",
      "running loss 2.142024384558201\n",
      "learning rate 70 [0.047652025973541665]\n",
      "running loss 2.1208778102993966\n",
      "learning rate 80 [0.03893517461607997]\n",
      "running loss 2.0935299550294877\n",
      "learning rate 90 [0.03181287241021722]\n",
      "running loss 2.0833990780711176\n",
      "learning rate 100 [0.025993432955371577]\n",
      "running loss 2.0572130019664763\n",
      "learning rate 110 [0.021238527225488715]\n",
      "running loss 2.041448968052864\n",
      "learning rate 120 [0.017353423054287647]\n",
      "running loss 2.021359495103359\n",
      "learning rate 130 [0.014179010084073873]\n",
      "running loss 2.008201594650745\n",
      "['livin', 'cyussian', 'idea', 'janeen', 'juzsiri', 'eyan', 'imry', 'lykelland', 'eamaria', 'ulif']\n",
      "tensor(2.1232)\n"
     ]
    }
   ],
   "source": [
    "tr = TestRig(12, 120)\n",
    "tr.train(epochs, batch_size, samples)\n",
    "print(tr.model.generate(10))\n",
    "\n",
    "print(tr.val_split(names.data[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2ade1e45-00ac-4cf7-b0a4-2a82c1eefb50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 200 5 17951\n",
      "learning rate 0 [0.196]\n",
      "running loss 2.4282220850586893\n",
      "learning rate 10 [0.16014627014995916]\n",
      "running loss 2.2907778553366662\n",
      "learning rate 20 [0.13085116246399847]\n",
      "running loss 2.2596419665217398\n",
      "learning rate 30 [0.10691492659895763]\n",
      "running loss 2.2339411221146586\n",
      "learning rate 40 [0.08735727917438633]\n",
      "running loss 2.199094163775444\n",
      "learning rate 50 [0.07137725729707488]\n",
      "running loss 2.175538654744625\n",
      "learning rate 60 [0.058320415967655595]\n",
      "running loss 2.147812264084816\n",
      "learning rate 70 [0.047652025973541665]\n",
      "running loss 2.122078421354294\n",
      "learning rate 80 [0.03893517461607997]\n",
      "running loss 2.103333488345146\n",
      "learning rate 90 [0.03181287241021722]\n",
      "running loss 2.076894433736801\n",
      "learning rate 100 [0.025993432955371577]\n",
      "running loss 2.0637942019701003\n",
      "learning rate 110 [0.021238527225488715]\n",
      "running loss 2.038330291569233\n",
      "learning rate 120 [0.017353423054287647]\n",
      "running loss 2.0355284547805788\n",
      "learning rate 130 [0.014179010084073873]\n",
      "running loss 2.02250409668684\n",
      "['avya', 'cilah', 'yinah', 'matheline', 'annai', 'ymylouly', 'yusliv', 'ycril', 'yiraline', 'evinaghell']\n",
      "tensor(2.1259)\n"
     ]
    }
   ],
   "source": [
    "tr = TestRig(12, 200)\n",
    "tr.train(epochs, batch_size, samples)\n",
    "print(tr.model.generate(10))\n",
    "\n",
    "print(tr.val_split(names.data[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "78bddcb1-3548-432f-aef9-2e80c5ec5555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we still learn so lets learn longer - our 'epoch' is a lot longer than our previous samples size\n",
    "epochs = 200\n",
    "batch_size = 80\n",
    "samples = 6000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "391b65c8-e2d9-4b57-8ae6-e077f664dde5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 200 5 14870\n",
      "learning rate 0 [0.196]\n",
      "running loss 2.365648023545742\n",
      "learning rate 10 [0.16014627014995916]\n",
      "running loss 2.3082727882862093\n",
      "learning rate 20 [0.13085116246399847]\n",
      "running loss 2.281898501376311\n",
      "learning rate 30 [0.10691492659895763]\n",
      "running loss 2.252478352944056\n",
      "learning rate 40 [0.08735727917438633]\n",
      "running loss 2.222281000216802\n",
      "learning rate 50 [0.07137725729707488]\n",
      "running loss 2.1884094891349473\n",
      "learning rate 60 [0.058320415967655595]\n",
      "running loss 2.159437515536944\n",
      "learning rate 70 [0.047652025973541665]\n",
      "running loss 2.136063412308693\n",
      "learning rate 80 [0.03893517461607997]\n",
      "running loss 2.1068271798292795\n",
      "learning rate 90 [0.03181287241021722]\n",
      "running loss 2.0895823192596437\n",
      "learning rate 100 [0.025993432955371577]\n",
      "running loss 2.0653802466988562\n",
      "learning rate 110 [0.021238527225488715]\n",
      "running loss 2.0406739804943403\n",
      "learning rate 120 [0.017353423054287647]\n",
      "running loss 2.031770083725452\n",
      "learning rate 130 [0.014179010084073873]\n",
      "running loss 2.0130070190032323\n",
      "learning rate 140 [0.011585283568281068]\n",
      "running loss 1.9982423411607741\n",
      "learning rate 150 [0.009466020163723585]\n",
      "running loss 1.9835743444363276\n",
      "learning rate 160 [0.007734427665227742]\n",
      "running loss 1.9651949334144592\n",
      "learning rate 170 [0.006319590522096326]\n",
      "running loss 1.9535079501867294\n",
      "learning rate 180 [0.005163565566269181]\n",
      "running loss 1.9392826253573099\n",
      "learning rate 190 [0.0042190090107794434]\n",
      "running loss 1.9254466700951258\n",
      "['qeedsh', 'vitana', 'quanmy', 'ziyah', 'quodaz', 'nigrah', 'quuzzel', 'quenti', 'zayce', 'quuza']\n",
      "tensor(2.1306)\n"
     ]
    }
   ],
   "source": [
    "tr = TestRig(9, 200)\n",
    "tr.train(epochs, batch_size, samples)\n",
    "print(tr.model.generate(10))\n",
    "\n",
    "print(tr.val_split(names.data[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9122275f-1341-4ba5-937e-7007e62aad0e",
   "metadata": {},
   "source": [
    "### Starting to overfit\n",
    "\n",
    "loss is going down, validation is staying the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0eb6b3f8-55bd-4e26-b935-0edfecda0ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_length = 4\n",
    "\n",
    "names = WordSampling()\n",
    "names.from_file(\"../resources/names.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "870afcf5-491c-4e85-b9b3-fa7175bb3111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 12 4 588\n",
      "learning rate 0 [0.196]\n",
      "running loss 2.4036873132387795\n",
      "learning rate 10 [0.16014627014995916]\n",
      "running loss 2.3767006654938063\n",
      "tensor(2.3627)\n"
     ]
    }
   ],
   "source": [
    "tr = TestRig(3, 12, context_length)\n",
    "tr.train(20, batch_size, samples)\n",
    "tr.model.generate(10)\n",
    "print(tr.val_split(names.data[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "980c533a-c697-4e9d-a8e3-97ee8bad02eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 160 4 6508\n",
      "learning rate 0 [0.196]\n",
      "running loss 2.4210894192854564\n",
      "learning rate 10 [0.16014627014995916]\n",
      "running loss 2.37120360147953\n",
      "learning rate 20 [0.13085116246399847]\n",
      "running loss 2.3515283158024154\n",
      "learning rate 30 [0.10691492659895763]\n",
      "running loss 2.3283791618148486\n",
      "learning rate 40 [0.08735727917438633]\n",
      "running loss 2.3095569537480674\n",
      "learning rate 50 [0.07137725729707488]\n",
      "running loss 2.2842877165873845\n",
      "learning rate 60 [0.058320415967655595]\n",
      "running loss 2.264364729265372\n",
      "learning rate 70 [0.047652025973541665]\n",
      "running loss 2.247017851014932\n",
      "tensor(2.2481)\n"
     ]
    }
   ],
   "source": [
    "tr = TestRig(3, 160, context_length)\n",
    "tr.train(80, batch_size, samples)\n",
    "tr.model.generate(10)\n",
    "print(tr.val_split(names.data[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "32911e2c-15a4-4bdc-a25d-a888ebff4644",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 120\n",
    "batch_size = 80\n",
    "samples = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2fb24cb1-601b-4bb2-8005-ac624661518d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 200 4 13070\n",
      "learning rate 0 [0.196]\n",
      "running loss 2.482522686958313\n",
      "learning rate 10 [0.16014627014995916]\n",
      "running loss 2.3205695670843123\n",
      "learning rate 20 [0.13085116246399847]\n",
      "running loss 2.2850529050827024\n",
      "learning rate 30 [0.10691492659895763]\n",
      "running loss 2.2699478480815887\n",
      "learning rate 40 [0.08735727917438633]\n",
      "running loss 2.2425458602905275\n",
      "learning rate 50 [0.07137725729707488]\n",
      "running loss 2.219335655212402\n",
      "learning rate 60 [0.058320415967655595]\n",
      "running loss 2.1955071012973786\n",
      "learning rate 70 [0.047652025973541665]\n",
      "running loss 2.1881264252662658\n",
      "learning rate 80 [0.03893517461607997]\n",
      "running loss 2.170803068161011\n",
      "learning rate 90 [0.03181287241021722]\n",
      "running loss 2.1507494719028473\n",
      "learning rate 100 [0.025993432955371577]\n",
      "running loss 2.14399935901165\n",
      "learning rate 110 [0.021238527225488715]\n",
      "running loss 2.1214653807878494\n",
      "tensor(2.1566)\n"
     ]
    }
   ],
   "source": [
    "tr = TestRig(9, 200, context_length)\n",
    "tr.train(epochs, batch_size, samples)\n",
    "tr.model.generate(10)\n",
    "print(tr.val_split(names.data[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "61562540-63ad-4054-954b-e193fa3b614d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hoson',\n",
       " 'dasille',\n",
       " 'hamah',\n",
       " 'hrishy',\n",
       " 'hamiah',\n",
       " 'horoy',\n",
       " 'zaja',\n",
       " 'omberly',\n",
       " 'harvin',\n",
       " 'cadaleen']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr.model.generate(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "427c7ed1-52a8-4186-b54d-1792b6cfaeb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 220 4 17071\n",
      "learning rate 0 [0.392]\n",
      "running loss 2.519497916162014\n",
      "learning rate 10 [0.3202925402999183]\n",
      "running loss 2.4586433222293853\n",
      "learning rate 20 [0.26170232492799694]\n",
      "running loss 2.4135772078037263\n",
      "learning rate 30 [0.21382985319791525]\n",
      "running loss 2.373646735459566\n",
      "learning rate 40 [0.17471455834877267]\n",
      "running loss 2.3359570707380772\n",
      "learning rate 50 [0.14275451459414976]\n",
      "running loss 2.2964313144385815\n",
      "learning rate 60 [0.11664083193531119]\n",
      "running loss 2.2677467233836652\n",
      "learning rate 70 [0.09530405194708333]\n",
      "running loss 2.2373385948836804\n",
      "learning rate 80 [0.07787034923215994]\n",
      "running loss 2.2044726055562496\n",
      "learning rate 90 [0.06362574482043444]\n",
      "running loss 2.1723292561769485\n",
      "learning rate 100 [0.051986865910743155]\n",
      "running loss 2.1487865211069583\n",
      "learning rate 110 [0.04247705445097743]\n",
      "running loss 2.1254321322739123\n",
      "learning rate 120 [0.034706846108575294]\n",
      "running loss 2.1004812537431716\n",
      "learning rate 130 [0.028358020168147747]\n",
      "running loss 2.084448489189148\n",
      "learning rate 140 [0.023170567136562136]\n",
      "running loss 2.0682429263591766\n",
      "learning rate 150 [0.01893204032744717]\n",
      "running loss 2.0544050981104376\n",
      "learning rate 160 [0.015468855330455484]\n",
      "running loss 2.037326356858015\n",
      "learning rate 170 [0.012639181044192652]\n",
      "running loss 2.0237350947856902\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['zabrikondo',\n",
       " 'vedyanjoli',\n",
       " 'rifelsi',\n",
       " 'jazor',\n",
       " 'kaedricia',\n",
       " 'wirus',\n",
       " 'kabria',\n",
       " 'zaeden',\n",
       " 'releina',\n",
       " 'bitha']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 180\n",
    "batch_size = 80\n",
    "samples = 4000\n",
    "\n",
    "tr = TestRig(12, 220, context_length, 0.4)\n",
    "tr.train(epochs, batch_size, samples)\n",
    "tr.model.generate(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1a8e1f45-033a-41f7-b910-c80e8baa392c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['penettick',\n",
       " 'raci',\n",
       " 'zibellae',\n",
       " 'metiyanus',\n",
       " 'breyaris',\n",
       " 'meika',\n",
       " 'azaliana',\n",
       " 'sah',\n",
       " 'benra',\n",
       " 'quanna']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr.model.generate(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "30d1ec35-ce67-4d00-b2ea-f2398be2292f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.1208)\n"
     ]
    }
   ],
   "source": [
    "print(tr.val_split(names.data[1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
