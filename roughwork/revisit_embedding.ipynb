{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "965b36f1-1440-4a07-a843-80cba31c34e8",
   "metadata": {},
   "source": [
    "### Go back though embedding and onto sequence\n",
    "\n",
    "with what we've learnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d689a91-95ca-42cd-802e-ab97496243cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec8919ad-3d6b-4421-94ce-553e3a506dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run bookreader.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5b6ef18-283c-41fd-9375-ef8d13ed5e0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = BookReader(\"names.txt\")\n",
    "vocab_size = names.vocab_size\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a713daf8-4a5f-430d-8f00-9c6db6743d22",
   "metadata": {},
   "source": [
    "### Get the batch with both x and y unseparated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91f9fe0b-25d9-4578-a105-4eb99138428b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(data, batch_length=5, batch_size=5):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    ix = torch.randint(len(data) - batch_length, (batch_size,))\n",
    "    b = torch.stack([data[i:i+batch_length] for i in ix])\n",
    "    return b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3b116f-f526-4958-bfed-07f665e89cba",
   "metadata": {},
   "source": [
    "### OK so bring back embeddings\n",
    "\n",
    "now we'll have full batch BTC or however you want to name the dimensions\n",
    "\n",
    "and then we're going to add another dimension because eventually we'll be creating multiple examples from each example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d5c6cd1-f396-48ef-afa7-d816320c68fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4, 4, 2]) tensor([[[ 1.2590, -0.4949],\n",
      "         [ 1.2590, -0.4949],\n",
      "         [ 1.2590, -0.4949],\n",
      "         [ 1.2590, -0.4949]],\n",
      "\n",
      "        [[ 1.2590, -0.4949],\n",
      "         [ 2.4729,  0.1094],\n",
      "         [ 1.2590, -0.4949],\n",
      "         [ 1.2590, -0.4949]],\n",
      "\n",
      "        [[ 1.2590, -0.4949],\n",
      "         [ 2.4729,  0.1094],\n",
      "         [ 1.0872,  0.9255],\n",
      "         [ 1.2590, -0.4949]],\n",
      "\n",
      "        [[ 1.2590, -0.4949],\n",
      "         [ 2.4729,  0.1094],\n",
      "         [ 1.0872,  0.9255],\n",
      "         [-0.2380, -0.4213]]], grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([11,  8,  1, 12])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dimensions = 2\n",
    "em = nn.Embedding(vocab_size, embedding_dimensions)\n",
    "\n",
    "train = torch.tensor(names.data[0])\n",
    "\n",
    "context_length = 4\n",
    "y_l = 1\n",
    "batch_size = 3\n",
    "t_b = get_batch(train, context_length+y_l, batch_size)\n",
    "\n",
    "x = t_b[:, 0:context_length]\n",
    "y = t_b[:, 1:context_length+1]\n",
    "\n",
    "x = x.repeat(1, context_length)\n",
    "tr = torch.tril(torch.ones((batch_size, context_length, context_length), dtype=torch.int))\n",
    "\n",
    "multi = x.view(batch_size, context_length, context_length) * tr\n",
    "\n",
    "embedded_batch = em(multi)\n",
    "print(embedded_batch.shape, embedded_batch[0])\n",
    "y[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930f0342-7fe2-43ae-8699-54be8b92effa",
   "metadata": {},
   "source": [
    "## our added dimension\n",
    "its really just increasing the batch size, so the batch is now batch_size * context_length\n",
    "\n",
    "so our B, T, C is right now - and B = batch_size * context_length which match our Y which is also = batch_size * context_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26a60a20-946a-4936-93ca-c5efc8a175cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 4, 2])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "large_batch = embedded_batch.view(batch_size * context_length, context_length, embedding_dimensions)\n",
    "large_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d5d2c4-7f1a-4f79-9703-ab2d66f2b123",
   "metadata": {},
   "source": [
    "### and we're going to feed a sequential model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaaf371c-1553-4ca3-aa2a-eab71ffe15db",
   "metadata": {},
   "source": [
    "we want this to be available in our model, or a layer, but for now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99bfca69-87c9-49f5-9878-a66fa6a7a7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def triling(xs, batch_size, context_length):\n",
    "    tril = torch.tril(torch.ones((batch_size, context_length, context_length), dtype=torch.int))\n",
    "    x = xs.repeat(1, 1, context_length)\n",
    "    print(\"x repeat\", x.shape)\n",
    "    x = x.view(-1, context_length, context_length)\n",
    "    x = x * tril\n",
    "    print(\"x to multiple\", x.shape)\n",
    "    return x.view(-1, context_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc922ca4-f4a8-4e71-8505-7b58e02f4876",
   "metadata": {},
   "source": [
    "### lets run through what we're doing first\n",
    "\n",
    "we're going to embed our batch - but this messes up our dimensions on output\n",
    "\n",
    "we need to add a flatten layer to the model to deal with this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70a65b56-e6cd-4280-903a-4bd16e42d138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x repeat torch.Size([1, 3, 16])\n",
      "x to multiple torch.Size([3, 4, 4])\n",
      "examples from triling torch.Size([12, 4])\n",
      "torch.Size([12, 8])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 8])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 3\n",
    "context_length = 4\n",
    "embedding_dimensions = 2\n",
    "\n",
    "es = nn.Embedding(vocab_size, embedding_dimensions)\n",
    "fl = nn.Flatten(1)\n",
    "t_b = get_batch(train, context_length+1, batch_size)\n",
    "\n",
    "l1 = nn.Linear(embedding_dimensions * context_length, 50, bias=True)\n",
    "\n",
    "x = t_b[:, 0:context_length]\n",
    "x = triling(x, batch_size, context_length)\n",
    "print(\"examples from triling\", x.shape)\n",
    "\n",
    "embedded = es(x)\n",
    "## this is what we did in the sequence layer\n",
    "print(embedded.view(-1, embedding_dimensions * context_length).shape)\n",
    "# flatten will do the same in our model\n",
    "fl(embedded).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "948635f1-7d0d-4a4f-9a83-5d91f2fa2fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "nonlin = 'relu'\n",
    "\n",
    "def create_model(context_length, hidden_size, embed_size):\n",
    "\n",
    "    seq = OrderedDict([\n",
    "        ('embed', nn.Embedding(vocab_size, embed_size)),\n",
    "        ('flatten', nn.Flatten(1)),\n",
    "        ('initial', nn.Linear(embed_size * context_length, hidden_size, bias=True)),\n",
    "        ('relu1', nn.ReLU()),\n",
    "        ('final', nn.Linear(hidden_size, vocab_size, bias=True)),\n",
    "    ])\n",
    "\n",
    "    initial = seq['initial']\n",
    "    nn.init.kaiming_normal_(initial.weight, nonlinearity=nonlin)\n",
    "    seq['initial'].weight.data = initial.weight.data * 3/5\n",
    "    if initial.bias is not None:\n",
    "        nn.init.constant_(initial.bias, 0)\n",
    "\n",
    "    final = seq['final']\n",
    "    seq['final'].weight.data = final.weight.data * 0.2\n",
    "    md = nn.Sequential(seq)\n",
    "    \n",
    "    return md"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c18e25-c38c-4924-9ed0-eac5b544e34c",
   "metadata": {},
   "source": [
    "### First run through sequence\n",
    "\n",
    "just like we had it before without the extra batch stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f3181315-70c2-4f1b-9314-6e54ddf0e9c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.6684, grad_fn=<DivBackward0>)\n",
      "tensor(2.4743, grad_fn=<DivBackward0>)\n",
      "tensor(2.4072, grad_fn=<DivBackward0>)\n",
      "tensor(2.3702, grad_fn=<DivBackward0>)\n",
      "tensor(2.3481, grad_fn=<DivBackward0>)\n",
      "tensor(2.3291, grad_fn=<DivBackward0>)\n",
      "tensor(2.3186, grad_fn=<DivBackward0>)\n",
      "tensor(2.3046, grad_fn=<DivBackward0>)\n",
      "tensor(2.2977, grad_fn=<DivBackward0>)\n",
      "tensor(2.2921, grad_fn=<DivBackward0>)\n",
      "tensor(2.2856, grad_fn=<DivBackward0>)\n",
      "tensor(2.2808, grad_fn=<DivBackward0>)\n",
      "tensor(2.2759, grad_fn=<DivBackward0>)\n",
      "tensor(2.2713, grad_fn=<DivBackward0>)\n",
      "tensor(2.2717, grad_fn=<DivBackward0>)\n",
      "tensor(2.2666, grad_fn=<DivBackward0>)\n",
      "tensor(2.2641, grad_fn=<DivBackward0>)\n",
      "tensor(2.2621, grad_fn=<DivBackward0>)\n",
      "tensor(2.2613, grad_fn=<DivBackward0>)\n",
      "tensor(2.2560, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "training_runs = 1000\n",
    "batch_size = 820\n",
    "context_length = 4\n",
    "learning_rate = .1\n",
    "embedding_dimensions = 2\n",
    "\n",
    "model = create_model(4, 60, 2)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) # , weight_decay=args.weight_decay, betas=(0.9, 0.99), eps=1e-8)\n",
    "\n",
    "for ep in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    for tr in range(training_runs):\n",
    "        t_b = get_batch(train, context_length+1, batch_size)\n",
    "\n",
    "        x = t_b[:, 0:context_length]\n",
    "\n",
    "        logits = model.forward(x)\n",
    "        # cross_entropy wants to know the number of classes - in our case vocab_size\n",
    "        logits = logits.view(-1, vocab_size)\n",
    "\n",
    "        y = t_b[:, context_length:context_length+1]\n",
    "        Y = y.reshape(-1)\n",
    "\n",
    "        loss = F.cross_entropy(logits, Y) # loss function\n",
    "\n",
    "        epoch_loss += loss\n",
    "        \n",
    "        model.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(epoch_loss/training_runs)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53043ea-423c-4fe2-a93d-9a68721d7678",
   "metadata": {},
   "source": [
    "OK that looks reasonable\n",
    "\n",
    "now lets look at our tril stuff again\n",
    "\n",
    "from sequence we had\n",
    "\n",
    "get_xys(names[:2])\n",
    "([[0, 0, 0, 0, 0],\n",
    "  [0, 0, 0, 0, 1],\n",
    "  [0, 0, 0, 1, 13],\n",
    "  [0, 0, 1, 13, 25],\n",
    "  [0, 1, 13, 25, 1],\n",
    "  [1, 13, 25, 1, 8],\n",
    "  [0, 0, 0, 0, 0],\n",
    "  [0, 0, 0, 0, 26],\n",
    "  [0, 0, 0, 26, 1],\n",
    "  [0, 0, 26, 1, 13],\n",
    "  [0, 26, 1, 13, 15],\n",
    "  [26, 1, 13, 15, 18],\n",
    "  [1, 13, 15, 18, 1]],\n",
    " [1, 13, 25, 1, 8, 0, 26, 1, 13, 15, 18, 1, 0])\n",
    "\n",
    "is that what our tril does exactly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cc9c4987-ff27-4347-a610-87edfd6b3501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the batch and 'extended' labels\n",
      "tensor([[14,  7,  5, 12],\n",
      "        [25,  1,  0, 12]])\n",
      "tensor([[ 7,  5, 12,  9],\n",
      "        [ 1,  0, 12,  5]])\n",
      "x repeat torch.Size([1, 2, 16])\n",
      "x reshape torch.Size([2, 4, 4])\n",
      "tensor([[[14,  7,  5, 12],\n",
      "         [14,  7,  5, 12],\n",
      "         [14,  7,  5, 12],\n",
      "         [14,  7,  5, 12]],\n",
      "\n",
      "        [[25,  1,  0, 12],\n",
      "         [25,  1,  0, 12],\n",
      "         [25,  1,  0, 12],\n",
      "         [25,  1,  0, 12]]])\n"
     ]
    }
   ],
   "source": [
    "t_b = get_batch(train, context_length+1, 2)\n",
    "\n",
    "x = t_b[:, 0:context_length]\n",
    "y = t_b[:, 1:context_length+1]\n",
    "\n",
    "print(\"the batch and 'extended' labels\")\n",
    "print(x)\n",
    "print(y)\n",
    "\n",
    "x = x.repeat(1, 1, context_length)\n",
    "print(\"x repeat\", x.shape)\n",
    "x = x.view(-1, context_length, context_length)\n",
    "print(\"x reshape\", x.shape)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa6621d-79e1-4ece-8034-6b3d5159ce3e",
   "metadata": {},
   "source": [
    "we've got copies of both our batch examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f8fa1693-2c37-4055-a928-1e77ad513ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[14,  0,  0,  0],\n",
       "         [14,  7,  0,  0],\n",
       "         [14,  7,  5,  0],\n",
       "         [14,  7,  5, 12],\n",
       "         [25,  0,  0,  0],\n",
       "         [25,  1,  0,  0],\n",
       "         [25,  1,  0,  0],\n",
       "         [25,  1,  0, 12]]),\n",
       " tensor([[ 7,  5, 12,  9],\n",
       "         [ 1,  0, 12,  5]]))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tril = torch.tril(torch.ones((2, context_length, context_length), dtype=torch.int))\n",
    "nb = x * tril\n",
    "nb = nb.view(-1, context_length)\n",
    "print(nb.shape)\n",
    "nb, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2d53d1-f722-475c-be99-e455e845013e",
   "metadata": {},
   "source": [
    "so those are our new examples and they all have labels\n",
    "\n",
    "we'd need to shift rows right by their number of zeros to match what we had but does that matter?\n",
    "\n",
    "the characters don't talk to each other - we just present sequences and a label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6560cb2a-67ef-4dfb-8638-ed3d2ea45986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.2957, grad_fn=<DivBackward0>) [0.095]\n",
      "tensor(2.1584, grad_fn=<DivBackward0>) [0.09025]\n",
      "tensor(2.1238, grad_fn=<DivBackward0>) [0.0857375]\n",
      "tensor(2.0929, grad_fn=<DivBackward0>) [0.08145062499999998]\n",
      "tensor(2.0815, grad_fn=<DivBackward0>) [0.07737809374999999]\n",
      "tensor(2.0658, grad_fn=<DivBackward0>) [0.07350918906249998]\n",
      "tensor(2.0575, grad_fn=<DivBackward0>) [0.06983372960937498]\n",
      "tensor(2.0510, grad_fn=<DivBackward0>) [0.06634204312890622]\n",
      "tensor(2.0432, grad_fn=<DivBackward0>) [0.0630249409724609]\n",
      "tensor(2.0334, grad_fn=<DivBackward0>) [0.05987369392383786]\n",
      "tensor(2.0337, grad_fn=<DivBackward0>) [0.05688000922764597]\n",
      "tensor(2.0204, grad_fn=<DivBackward0>) [0.05403600876626367]\n",
      "tensor(2.0210, grad_fn=<DivBackward0>) [0.05133420832795048]\n",
      "tensor(2.0182, grad_fn=<DivBackward0>) [0.04876749791155295]\n",
      "tensor(2.0184, grad_fn=<DivBackward0>) [0.046329123015975304]\n",
      "tensor(2.0168, grad_fn=<DivBackward0>) [0.04401266686517654]\n",
      "tensor(2.0016, grad_fn=<DivBackward0>) [0.04181203352191771]\n",
      "tensor(2.0022, grad_fn=<DivBackward0>) [0.039721431845821824]\n",
      "tensor(2.0001, grad_fn=<DivBackward0>) [0.037735360253530734]\n",
      "tensor(1.9987, grad_fn=<DivBackward0>) [0.035848592240854196]\n",
      "tensor(1.9951, grad_fn=<DivBackward0>) [0.03405616262881148]\n",
      "tensor(1.9916, grad_fn=<DivBackward0>) [0.03235335449737091]\n",
      "tensor(1.9896, grad_fn=<DivBackward0>) [0.030735686772502362]\n",
      "tensor(1.9878, grad_fn=<DivBackward0>) [0.029198902433877242]\n",
      "tensor(1.9876, grad_fn=<DivBackward0>) [0.027738957312183378]\n",
      "tensor(1.9802, grad_fn=<DivBackward0>) [0.026352009446574207]\n",
      "tensor(1.9837, grad_fn=<DivBackward0>) [0.025034408974245494]\n",
      "tensor(1.9775, grad_fn=<DivBackward0>) [0.023782688525533217]\n",
      "tensor(1.9803, grad_fn=<DivBackward0>) [0.022593554099256556]\n",
      "tensor(1.9770, grad_fn=<DivBackward0>) [0.021463876394293726]\n",
      "tensor(1.9685, grad_fn=<DivBackward0>) [0.020390682574579037]\n",
      "tensor(1.9635, grad_fn=<DivBackward0>) [0.019371148445850084]\n",
      "tensor(1.9701, grad_fn=<DivBackward0>) [0.01840259102355758]\n",
      "tensor(1.9710, grad_fn=<DivBackward0>) [0.0174824614723797]\n",
      "tensor(1.9678, grad_fn=<DivBackward0>) [0.016608338398760712]\n",
      "tensor(1.9717, grad_fn=<DivBackward0>) [0.015777921478822676]\n",
      "tensor(1.9627, grad_fn=<DivBackward0>) [0.014989025404881541]\n",
      "tensor(1.9648, grad_fn=<DivBackward0>) [0.014239574134637464]\n",
      "tensor(1.9682, grad_fn=<DivBackward0>) [0.01352759542790559]\n",
      "tensor(1.9694, grad_fn=<DivBackward0>) [0.012851215656510309]\n",
      "tensor(1.9681, grad_fn=<DivBackward0>) [0.012208654873684792]\n",
      "tensor(1.9588, grad_fn=<DivBackward0>) [0.011598222130000552]\n",
      "tensor(1.9635, grad_fn=<DivBackward0>) [0.011018311023500524]\n",
      "tensor(1.9591, grad_fn=<DivBackward0>) [0.010467395472325497]\n",
      "tensor(1.9591, grad_fn=<DivBackward0>) [0.009944025698709221]\n",
      "tensor(1.9593, grad_fn=<DivBackward0>) [0.00944682441377376]\n",
      "tensor(1.9573, grad_fn=<DivBackward0>) [0.00897448319308507]\n",
      "tensor(1.9563, grad_fn=<DivBackward0>) [0.008525759033430816]\n",
      "tensor(1.9592, grad_fn=<DivBackward0>) [0.008099471081759275]\n",
      "tensor(1.9583, grad_fn=<DivBackward0>) [0.007694497527671311]\n",
      "tensor(1.9583, grad_fn=<DivBackward0>) [0.007309772651287745]\n",
      "tensor(1.9546, grad_fn=<DivBackward0>) [0.006944284018723357]\n",
      "tensor(1.9604, grad_fn=<DivBackward0>) [0.006597069817787189]\n",
      "tensor(1.9554, grad_fn=<DivBackward0>) [0.006267216326897829]\n",
      "tensor(1.9550, grad_fn=<DivBackward0>) [0.005953855510552938]\n",
      "tensor(1.9554, grad_fn=<DivBackward0>) [0.005656162735025291]\n",
      "tensor(1.9532, grad_fn=<DivBackward0>) [0.005373354598274026]\n",
      "tensor(1.9550, grad_fn=<DivBackward0>) [0.005104686868360324]\n",
      "tensor(1.9519, grad_fn=<DivBackward0>) [0.004849452524942308]\n",
      "tensor(1.9548, grad_fn=<DivBackward0>) [0.004606979898695193]\n",
      "tensor(1.9531, grad_fn=<DivBackward0>) [0.004376630903760433]\n",
      "tensor(1.9539, grad_fn=<DivBackward0>) [0.0041577993585724116]\n",
      "tensor(1.9573, grad_fn=<DivBackward0>) [0.0039499093906437905]\n",
      "tensor(1.9489, grad_fn=<DivBackward0>) [0.0037524139211116006]\n",
      "tensor(1.9544, grad_fn=<DivBackward0>) [0.0035647932250560204]\n",
      "tensor(1.9534, grad_fn=<DivBackward0>) [0.003386553563803219]\n",
      "tensor(1.9531, grad_fn=<DivBackward0>) [0.003217225885613058]\n",
      "tensor(1.9443, grad_fn=<DivBackward0>) [0.0030563645913324047]\n",
      "tensor(1.9536, grad_fn=<DivBackward0>) [0.0029035463617657843]\n",
      "tensor(1.9540, grad_fn=<DivBackward0>) [0.002758369043677495]\n",
      "tensor(1.9559, grad_fn=<DivBackward0>) [0.00262045059149362]\n",
      "tensor(1.9568, grad_fn=<DivBackward0>) [0.002489428061918939]\n",
      "tensor(1.9470, grad_fn=<DivBackward0>) [0.002364956658822992]\n",
      "tensor(1.9415, grad_fn=<DivBackward0>) [0.002246708825881842]\n",
      "tensor(1.9520, grad_fn=<DivBackward0>) [0.00213437338458775]\n",
      "tensor(1.9498, grad_fn=<DivBackward0>) [0.0020276547153583622]\n",
      "tensor(1.9473, grad_fn=<DivBackward0>) [0.001926271979590444]\n",
      "tensor(1.9535, grad_fn=<DivBackward0>) [0.0018299583806109217]\n",
      "tensor(1.9476, grad_fn=<DivBackward0>) [0.0017384604615803755]\n",
      "tensor(1.9489, grad_fn=<DivBackward0>) [0.0016515374385013568]\n",
      "tensor(1.9527, grad_fn=<DivBackward0>) [0.0015689605665762888]\n",
      "tensor(1.9511, grad_fn=<DivBackward0>) [0.0014905125382474742]\n",
      "tensor(1.9508, grad_fn=<DivBackward0>) [0.0014159869113351004]\n",
      "tensor(1.9468, grad_fn=<DivBackward0>) [0.0013451875657683454]\n",
      "tensor(1.9483, grad_fn=<DivBackward0>) [0.001277928187479928]\n",
      "tensor(1.9470, grad_fn=<DivBackward0>) [0.0012140317781059316]\n",
      "tensor(1.9477, grad_fn=<DivBackward0>) [0.001153330189200635]\n",
      "tensor(1.9493, grad_fn=<DivBackward0>) [0.0010956636797406032]\n",
      "tensor(1.9496, grad_fn=<DivBackward0>) [0.001040880495753573]\n",
      "tensor(1.9486, grad_fn=<DivBackward0>) [0.0009888364709658944]\n",
      "tensor(1.9520, grad_fn=<DivBackward0>) [0.0009393946474175996]\n",
      "tensor(1.9444, grad_fn=<DivBackward0>) [0.0008924249150467197]\n",
      "tensor(1.9487, grad_fn=<DivBackward0>) [0.0008478036692943836]\n",
      "tensor(1.9434, grad_fn=<DivBackward0>) [0.0008054134858296644]\n",
      "tensor(1.9476, grad_fn=<DivBackward0>) [0.0007651428115381812]\n",
      "tensor(1.9460, grad_fn=<DivBackward0>) [0.000726885670961272]\n",
      "tensor(1.9471, grad_fn=<DivBackward0>) [0.0006905413874132084]\n",
      "tensor(1.9483, grad_fn=<DivBackward0>) [0.0006560143180425479]\n",
      "tensor(1.9466, grad_fn=<DivBackward0>) [0.0006232136021404205]\n",
      "tensor(1.9470, grad_fn=<DivBackward0>) [0.0005920529220333994]\n",
      "tensor(1.9489, grad_fn=<DivBackward0>) [5.6245027593172946e-05]\n",
      "tensor(1.9516, grad_fn=<DivBackward0>) [5.3432776213514294e-05]\n",
      "tensor(1.9452, grad_fn=<DivBackward0>) [5.0761137402838575e-05]\n",
      "tensor(1.9424, grad_fn=<DivBackward0>) [4.822308053269664e-05]\n",
      "tensor(1.9445, grad_fn=<DivBackward0>) [4.581192650606181e-05]\n",
      "tensor(1.9446, grad_fn=<DivBackward0>) [4.3521330180758716e-05]\n",
      "tensor(1.9459, grad_fn=<DivBackward0>) [4.134526367172078e-05]\n",
      "tensor(1.9454, grad_fn=<DivBackward0>) [3.927800048813474e-05]\n",
      "tensor(1.9519, grad_fn=<DivBackward0>) [3.7314100463728e-05]\n",
      "tensor(1.9405, grad_fn=<DivBackward0>) [3.54483954405416e-05]\n",
      "tensor(1.9464, grad_fn=<DivBackward0>) [3.367597566851452e-05]\n",
      "tensor(1.9464, grad_fn=<DivBackward0>) [3.199217688508879e-05]\n",
      "tensor(1.9420, grad_fn=<DivBackward0>) [3.039256804083435e-05]\n",
      "tensor(1.9482, grad_fn=<DivBackward0>) [2.887293963879263e-05]\n",
      "tensor(1.9458, grad_fn=<DivBackward0>) [2.7429292656852997e-05]\n",
      "tensor(1.9481, grad_fn=<DivBackward0>) [2.6057828024010345e-05]\n",
      "tensor(1.9479, grad_fn=<DivBackward0>) [2.4754936622809825e-05]\n",
      "tensor(1.9496, grad_fn=<DivBackward0>) [2.3517189791669335e-05]\n",
      "tensor(1.9502, grad_fn=<DivBackward0>) [2.2341330302085865e-05]\n",
      "tensor(1.9500, grad_fn=<DivBackward0>) [2.1224263786981572e-05]\n",
      "tensor(1.9463, grad_fn=<DivBackward0>) [2.0163050597632492e-05]\n",
      "tensor(1.9483, grad_fn=<DivBackward0>) [1.9154898067750865e-05]\n",
      "tensor(1.9518, grad_fn=<DivBackward0>) [1.8197153164363322e-05]\n",
      "tensor(1.9528, grad_fn=<DivBackward0>) [1.7287295506145154e-05]\n",
      "tensor(1.9514, grad_fn=<DivBackward0>) [1.6422930730837896e-05]\n",
      "tensor(1.9516, grad_fn=<DivBackward0>) [1.5601784194296002e-05]\n",
      "tensor(1.9510, grad_fn=<DivBackward0>) [1.48216949845812e-05]\n",
      "tensor(1.9512, grad_fn=<DivBackward0>) [1.4080610235352139e-05]\n",
      "tensor(1.9410, grad_fn=<DivBackward0>) [1.3376579723584532e-05]\n",
      "tensor(1.9453, grad_fn=<DivBackward0>) [1.2707750737405305e-05]\n",
      "tensor(1.9462, grad_fn=<DivBackward0>) [1.2072363200535039e-05]\n",
      "tensor(1.9489, grad_fn=<DivBackward0>) [1.1468745040508286e-05]\n",
      "tensor(1.9454, grad_fn=<DivBackward0>) [1.089530778848287e-05]\n",
      "tensor(1.9494, grad_fn=<DivBackward0>) [1.0350542399058726e-05]\n",
      "tensor(1.9453, grad_fn=<DivBackward0>) [9.833015279105789e-06]\n",
      "tensor(1.9450, grad_fn=<DivBackward0>) [9.341364515150498e-06]\n",
      "tensor(1.9496, grad_fn=<DivBackward0>) [8.874296289392974e-06]\n",
      "tensor(1.9497, grad_fn=<DivBackward0>) [8.430581474923325e-06]\n",
      "tensor(1.9499, grad_fn=<DivBackward0>) [8.009052401177158e-06]\n",
      "tensor(1.9517, grad_fn=<DivBackward0>) [7.6085997811183e-06]\n",
      "tensor(1.9482, grad_fn=<DivBackward0>) [7.228169792062385e-06]\n",
      "tensor(1.9559, grad_fn=<DivBackward0>) [6.866761302459265e-06]\n",
      "tensor(1.9477, grad_fn=<DivBackward0>) [6.523423237336301e-06]\n",
      "tensor(1.9482, grad_fn=<DivBackward0>) [6.197252075469486e-06]\n",
      "tensor(1.9517, grad_fn=<DivBackward0>) [5.887389471696011e-06]\n",
      "tensor(1.9464, grad_fn=<DivBackward0>) [5.593019998111211e-06]\n",
      "tensor(1.9463, grad_fn=<DivBackward0>) [5.31336899820565e-06]\n",
      "tensor(1.9480, grad_fn=<DivBackward0>) [5.047700548295367e-06]\n",
      "tensor(1.9507, grad_fn=<DivBackward0>) [4.795315520880598e-06]\n",
      "tensor(1.9452, grad_fn=<DivBackward0>) [4.5555497448365686e-06]\n",
      "tensor(1.9508, grad_fn=<DivBackward0>) [4.32777225759474e-06]\n",
      "tensor(1.9480, grad_fn=<DivBackward0>) [4.111383644715003e-06]\n",
      "tensor(1.9463, grad_fn=<DivBackward0>) [3.905814462479253e-06]\n",
      "tensor(1.9508, grad_fn=<DivBackward0>) [3.7105237393552902e-06]\n",
      "tensor(1.9432, grad_fn=<DivBackward0>) [3.5249975523875253e-06]\n",
      "tensor(1.9446, grad_fn=<DivBackward0>) [3.348747674768149e-06]\n",
      "tensor(1.9505, grad_fn=<DivBackward0>) [3.181310291029741e-06]\n",
      "tensor(1.9470, grad_fn=<DivBackward0>) [3.022244776478254e-06]\n",
      "tensor(1.9526, grad_fn=<DivBackward0>) [2.871132537654341e-06]\n",
      "tensor(1.9443, grad_fn=<DivBackward0>) [2.7275759107716237e-06]\n",
      "tensor(1.9475, grad_fn=<DivBackward0>) [2.591197115233043e-07]\n",
      "tensor(1.9485, grad_fn=<DivBackward0>) [2.4616372594713905e-07]\n",
      "tensor(1.9430, grad_fn=<DivBackward0>) [2.3385553964978208e-07]\n",
      "tensor(1.9476, grad_fn=<DivBackward0>) [2.2216276266729297e-07]\n",
      "tensor(1.9424, grad_fn=<DivBackward0>) [2.110546245339283e-07]\n",
      "tensor(1.9438, grad_fn=<DivBackward0>) [2.0050189330723186e-07]\n",
      "tensor(1.9438, grad_fn=<DivBackward0>) [1.9047679864187027e-07]\n",
      "tensor(1.9431, grad_fn=<DivBackward0>) [1.8095295870977674e-07]\n",
      "tensor(1.9501, grad_fn=<DivBackward0>) [1.719053107742879e-07]\n",
      "tensor(1.9502, grad_fn=<DivBackward0>) [1.633100452355735e-07]\n",
      "tensor(1.9501, grad_fn=<DivBackward0>) [1.5514454297379483e-07]\n",
      "tensor(1.9491, grad_fn=<DivBackward0>) [1.473873158251051e-07]\n",
      "tensor(1.9470, grad_fn=<DivBackward0>) [1.4001795003384983e-07]\n",
      "tensor(1.9411, grad_fn=<DivBackward0>) [1.3301705253215733e-07]\n",
      "tensor(1.9516, grad_fn=<DivBackward0>) [1.2636619990554947e-07]\n",
      "tensor(1.9508, grad_fn=<DivBackward0>) [1.20047889910272e-07]\n",
      "tensor(1.9400, grad_fn=<DivBackward0>) [1.1404549541475839e-07]\n",
      "tensor(1.9478, grad_fn=<DivBackward0>) [1.0834322064402047e-07]\n",
      "tensor(1.9466, grad_fn=<DivBackward0>) [1.0292605961181944e-07]\n",
      "tensor(1.9498, grad_fn=<DivBackward0>) [9.777975663122845e-08]\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "epochs = 180\n",
    "training_runs = 800\n",
    "batch_size = 220\n",
    "context_length = 5\n",
    "learning_rate = .1\n",
    "embedding_dimensions = 7\n",
    "hidden_size = 200\n",
    "\n",
    "tril = torch.tril(torch.ones((batch_size, context_length, context_length), dtype=torch.int))\n",
    "tril_batch_size = batch_size * context_length\n",
    "\n",
    "model = create_model(context_length, hidden_size, embedding_dimensions)\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "\n",
    "lmbda = lambda epoch: 0.95\n",
    "\n",
    "m_scheduler = optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)\n",
    "\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[100, 160], gamma=0.1)\n",
    "\n",
    "for ep in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    for tr in range(training_runs):\n",
    "        t_b = get_batch(train, context_length+1, batch_size)\n",
    "\n",
    "        x = t_b[:, 0:context_length]\n",
    "        y = t_b[:, context_length:context_length+1]\n",
    "        # print(\"initial\", x.shape, y.shape)\n",
    "        \n",
    "        X = x.repeat(1, 1, context_length).view(-1, context_length, context_length)\n",
    "        ex = (X * tril).view(-1, context_length)\n",
    "        \n",
    "        y = y.reshape(-1)\n",
    "        \n",
    "        logits = model.forward(x)\n",
    "        # cross_entropy wants to know the number of classes - in our case vocab_size\n",
    "        logits = logits.view(-1, vocab_size)\n",
    "        \n",
    "        Y = y.reshape(-1)\n",
    "        # print(\"cross entropy inputs\", logits.shape, Y.shape)\n",
    "        loss = F.cross_entropy(logits, Y) # loss function\n",
    "\n",
    "        epoch_loss += loss\n",
    "        \n",
    "        model.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    m_scheduler.step()\n",
    "    scheduler.step()\n",
    "    print(epoch_loss/training_runs, m_scheduler.get_last_lr())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e7ab9caf-9237-4f9e-8b6b-291204b36d01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.9498, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(epoch_loss/training_runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f8bc1ffb-0776-48a5-8734-cf59d383439d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.2267, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(epoch_loss/training_runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "16926363-2c95-4998-a0a9-8eee16e1abf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3,  8,  1, 18, 12],\n",
      "        [ 8,  1, 18, 12, 15],\n",
      "        [ 1, 18, 12, 15, 20],\n",
      "        [18, 12, 15, 20, 20]])\n",
      "tensor([[12, 15, 20, 20,  5],\n",
      "        [15, 20, 20,  5,  0],\n",
      "        [20, 20,  5,  0, 13],\n",
      "        [20,  5,  0, 13,  9]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 7,  9,  5, 13,  1],\n",
       "        [ 1, 22,  9,  5, 18],\n",
       "        [12, 15, 14,  1,  0],\n",
       "        [ 5, 25,  0, 20,  1]])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_val_batch(data, batch_length=5, batch_size=5, i=0):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    if i == 0:\n",
    "        ix = torch.randint(len(data) - batch_length, (batch_size,))\n",
    "    else:\n",
    "        ix = torch.arange(1, 5) + 1 + i\n",
    "\n",
    "    b = torch.stack([data[i:i+batch_length] for i in ix])\n",
    "    return b\n",
    "\n",
    "print(get_val_batch(train, 5, 4, 5))\n",
    "print(get_val_batch(train, 5, 4, 9))\n",
    "\n",
    "get_batch(train, 5, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "f874381a-99eb-4bba-bde0-b10c9632e5c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_batches 22875 457\n",
      "total loss tensor(940.1379) tensor(2.0572)\n",
      "None 22875\n",
      "num_batches 16600 332\n",
      "total loss tensor(702.2509) tensor(2.1152)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "@torch.no_grad()\n",
    "def split_loss(split):\n",
    "    split_len = len(split)\n",
    "    total_loss = 0\n",
    "    batch_size = 50\n",
    "    num_batches = math.floor(split_len / batch_size)\n",
    "    print(\"num_batches\", split_len, num_batches)\n",
    "\n",
    "    for i in range(num_batches):\n",
    "\n",
    "        t_b = get_val_batch(split, context_length+1, batch_size, i*batch_size)\n",
    "        \n",
    "        x = t_b[:, 0: context_length]\n",
    "        y = t_b[:, context_length: context_length+1]\n",
    "        \n",
    "        Y = y.reshape(-1)\n",
    "        \n",
    "        logits = model(x)\n",
    "        \n",
    "        batch_loss = F.cross_entropy(logits, Y) # loss function\n",
    "        \n",
    "        if batch_loss < 100:\n",
    "            total_loss = total_loss + batch_loss\n",
    "    \n",
    "    print(\"total loss\", total_loss, total_loss / num_batches)\n",
    "\n",
    "dev = torch.tensor(names.data[1])\n",
    "print(split_loss(dev), len(dev))\n",
    "\n",
    "test = torch.tensor(names.data[2])\n",
    "split_loss(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0f3df5-2fe8-43fd-971a-cf1c225a726a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_names(num_names):\n",
    "    for i in range(num_names):\n",
    "        out = []\n",
    "        ix = [0, 0, 0, 0, 0]\n",
    "        for nl in range(10):\n",
    "            xenc = es[ix]\n",
    "\n",
    "            outputL1 = torch.relu(xenc.view(-1, dims * context_length) @ W1 + b1)\n",
    "            logits = outputL1 @ W2 + b2\n",
    "            \n",
    "            p = F.softmax(logits, dim=1)\n",
    "    \n",
    "            # ## torch.multinomial pulls out an index in p (num_samples=1) by sampling from the elements in p according to their probabilities \n",
    "            # ## (p is normalized in the softmax above)\n",
    "            prediction = torch.multinomial(p, num_samples=1).item()\n",
    "            \n",
    "            ix = [ix[1], ix[2], ix[3], ix[4], prediction]\n",
    "\n",
    "            if prediction == 0:\n",
    "                break\n",
    "            out.append(itos[prediction])\n",
    "            \n",
    "        print(\"\".join(out))\n",
    "\n",
    "generate_names(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
