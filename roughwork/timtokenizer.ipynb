{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "48ddc856-f3fc-4684-93f3-b16de08ed932",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as rex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "db5b5bed-40c0-4ba5-89e7-593960792b8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', ' ', ' of', ' the', ' Relm']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_gpt4 = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\"\n",
    "\n",
    "not_token = r\"[A-Z]{2,}|[.,:-;\\!\\?]\"\n",
    "\n",
    "all_caps = r\"[A-Z]{2,}\"\n",
    "\n",
    "pattern_general = rex.compile(split_gpt4)\n",
    "pattern_all_caps = rex.compile(all_caps)\n",
    "pattern_not_token = rex.compile(not_token)\n",
    "\n",
    "rex.findall(pattern_general, rex.sub(pattern_not_token, \"\", \"a KING of the Relm!?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "0082f318-5f14-43d3-9905-e08d203b8cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "books = [\"tiny_shakespeare.txt\", \n",
    "         \"dracula.txt\",\n",
    "         \"blake.txt\",\n",
    "         \"pickwick.txt\", \"twist.txt\", \"hard times.txt\", \"dorrit.txt\",\n",
    "         \"decline1.txt\",\n",
    "         \"vanity.txt\",\n",
    "         \"folly.txt\",\n",
    "         \"white company.txt\",\n",
    "         \"heights.txt\",\n",
    "         \"secret agent.txt\",\n",
    "         \"nonsense.txt\",\n",
    "         \"Middlemarch.txt\", \"brother jacob.txt\", \"mill on the floss.txt\", \"the lifted veil.txt\",\n",
    "         \"alice.txt\", \"hunting of the snark.txt\", \"Through the looking glass.txt\", \"a tangled.txt\", \"bruno.txt\",\n",
    "         \"jude.txt\", \"mayor of castle.txt\", \"return of the native.txt\", \"Tess of the.txt\", \"mayor of castle.txt\", \"adam bede.txt\",\n",
    "         \"Northanger Abbey.txt\", \"mansfield.txt\", \"emma.txt\", \"sense and.txt\",\n",
    "         \"treasure island.txt\", \"kidnapped.txt\"]\n",
    "\n",
    "lines = []\n",
    "token_counts = {\" \": 0}\n",
    "s_names = {\"juliet\": 0}\n",
    "\n",
    "for book in books:\n",
    "    with open(book, 'r', encoding='utf-8') as f:\n",
    "        lines += f.readlines()\n",
    "\n",
    "for line in lines:\n",
    "    for name in rex.findall(pattern_all_caps, line):\n",
    "        n = name.lower().strip()\n",
    "        exists = s_names.get(n, 0)\n",
    "        s_names[n] = exists + 1\n",
    "    lts = rex.findall(pattern_general, rex.sub(pattern_not_token, \"\", line))\n",
    "    for lt in lts:\n",
    "        is_name = s_names.get(lt.lower().strip(), False)\n",
    "        if is_name is True:\n",
    "            continue\n",
    "        exists = token_counts.get(lt, 0)\n",
    "        token_counts[lt] = exists + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "43c33d02-48fd-4fa6-9b68-8c1bbc496956",
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_names = []\n",
    "\n",
    "for (t, v) in token_counts.items():\n",
    "    tl = t.lower()\n",
    "    if t != tl:\n",
    "        add = token_counts.get(tl, 0)\n",
    "        # token_counts[t] = v + add\n",
    "        if add != 0:\n",
    "            token_counts[tl] = v + add\n",
    "        else:\n",
    "            add = token_counts.get(\" \" + tl.strip(), 0)\n",
    "            if add == 0:\n",
    "                possible_names.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "4c727609-f057-44c2-8762-53fe74262dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_counts\n",
    "    \n",
    "top_tokens = sorted([(v, k) for (k, v) in token_counts.items()], reverse=True)\n",
    "\n",
    "# for i, t in enumerate(top_tokens[:200]):\n",
    "#     print(i, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "4898e7b5-a908-418c-bfa3-77b98c8ac2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_counts\n",
    "    \n",
    "top_tokens = sorted([(v, k) for (k, v) in token_counts.items()], reverse=True)\n",
    "\n",
    "# for i, t in enumerate(top_tokens[:200]):\n",
    "#     print(i, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87baea79-0d1d-45eb-a1e9-9b2061ab6c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Casterbridge Clym Clare James Lucy Henchard Glegg Yeobright Lydgate Eustacia Dorothea Catherine Casaubon Bulstrode Rosamond Alice Ladislaw Middlemarch Farebrother Maggie Jude Tess Elizabeth Tulliver Lucetta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "9d91d0a8-04dd-421f-aee3-f5f737f1727a",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_counts = {}\n",
    "for name in possible_names:\n",
    "    count = token_counts[name]\n",
    "    n = name.strip()\n",
    "    current = name_counts.get(n, 0)\n",
    "    name_counts[n] = current + count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "0e566199-e92b-45a0-9a6f-9a159a490554",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1946, 'Mr'),\n",
       " (999, 'Lydgate'),\n",
       " (938, 'Dorothea'),\n",
       " (693, 'Mrs'),\n",
       " (659, '“I'),\n",
       " (645, 'Casaubon'),\n",
       " (568, 'Bulstrode'),\n",
       " (550, 'Fred'),\n",
       " (549, 'Rosamond'),\n",
       " (396, 'Alice')]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_names = sorted([(v, k) for (k, v) in name_counts.items()], reverse=True)\n",
    "top_names[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "aa5be9a6-97c1-4586-863f-12d7b74326da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# s_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f6d4eb7f-b067-477a-869a-589874b45566",
   "metadata": {},
   "outputs": [],
   "source": [
    "roman = ['i', 'ii', 'iii', 'iv', 'v', 'vi', 'vii', 'viii', 'ix', 'x', 'xi']\n",
    "titles = ['page', 'king', 'queen', 'prince', 'cardinal', 'bishop', 'archbishop', 'lord', 'lords', \n",
    "          'nurse', 'duke', 'duchess', 'earl', 'sir', 'lady', 'gentlemen', 'friar', 'mistress']\n",
    "randomy = ['another', 'overdone', 'elbow', 'froth', 'measure', 'for', 'sly', 'all', 'men', 'ae', 'of', 'bushy', 'serving', 'scroop', 'serving']\n",
    "\n",
    "ambiguous = ['richmond', 'paris', 'rivers', 'grey', 'gardener', 'green', 'surrey', 'york', 'oxford', 'dorset', 'hastings', 'northumberland', 'salisbury'\n",
    "             'carlisle', 'warwick', 'exeter', 'westmoreland', 'somerset', 'gloucester', 'buckingham', 'derby', 'ely']\n",
    "\n",
    "all_exceptions = roman + titles + randomy\n",
    "\n",
    "for ex in all_exceptions:\n",
    "    try:\n",
    "        s_names.pop(ex)\n",
    "    finally:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "4a8a6db6-c617-4b42-bae0-6dc2fe32995b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_pairs(ids, pairs=None):\n",
    "    if pairs is None:\n",
    "        pairs = {} \n",
    "    for pair in zip(ids, ids[1:]):\n",
    "        pairs[pair] = pairs.get(pair, 0) + 1\n",
    "    return pairs\n",
    "\n",
    "def merge(ids, pair, idx):\n",
    "    \"\"\"\n",
    "    In the list of integers (ids), replace all consecutive occurrences\n",
    "    of pair with the new integer token idx\n",
    "    Example: ids=[1, 2, 3, 1, 2], pair=(1, 2), idx=4 -> [4, 3, 4]\n",
    "    \"\"\"\n",
    "    new_ids = []\n",
    "    i = 0\n",
    "    while i < len(ids):\n",
    "        # if not at the very last position AND the pair matches, replace it\n",
    "        if ids[i] == pair[0] and i < len(ids) - 1 and ids[i+1] == pair[1]:\n",
    "            new_ids.append(idx)\n",
    "            i += 2\n",
    "        else:\n",
    "            new_ids.append(ids[i])\n",
    "            i += 1\n",
    "    return new_ids\n",
    "\n",
    "def render_token(t: bytes) -> str:\n",
    "    # pretty print a token, escaping control characters\n",
    "    s = t.decode('utf-8', errors='replace')\n",
    "    # s = replace_control_characters(s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "08f78569-fbe6-48ed-ad21-a352e7791262",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "\n",
    "    def __init__(self):\n",
    "        # default: vocab size of 256 (all bytes), no merges, no patterns\n",
    "        self.merges = {} # (int, int) -> int\n",
    "        self.pattern = \"\" # str\n",
    "        self.special_tokens = {} # str -> int, e.g. {'<|endoftext|>': 100257}\n",
    "        self.vocab = self.build_vocab() # int -> bytes\n",
    "\n",
    "    def train(self, text, vocab_size, verbose=False):\n",
    "        assert vocab_size >= 256\n",
    "        num_merges = vocab_size - 256\n",
    "\n",
    "        # input text preprocessing\n",
    "        text_bytes = text.encode(\"utf-8\") # raw bytes\n",
    "        ids = list(text_bytes) # list of integers in range 0..255\n",
    "\n",
    "        merge_start = max(ids)\n",
    "\n",
    "        print(\"vocab max\", merge_start)\n",
    "\n",
    "        # iteratively merge the most common pairs to create new tokens\n",
    "        merges = {} # (int, int) -> int\n",
    "        vocab = {idx: bytes([idx]) for idx in range(256)} # int -> bytes\n",
    "        for i in range(num_merges):\n",
    "            # count up the number of times every consecutive pair appears\n",
    "            pairs = top_pairs(ids)\n",
    "            # find the pair with the highest count\n",
    "            pair = max(pairs, key=pairs.get)\n",
    "            # mint a new token: assign it the next available id\n",
    "            idx = 256 + i\n",
    "            # replace all occurrences of pair in ids with idx\n",
    "            ids = merge(ids, pair, idx)\n",
    "            # save the merge\n",
    "            merges[pair] = idx\n",
    "            vocab[idx] = vocab[pair[0]] + vocab[pair[1]]\n",
    "            # prints\n",
    "            if verbose:\n",
    "                print(f\"merge {i+1}/{num_merges}: {pair} -> {idx} ({vocab[idx]}) had {stats[pair]} occurrences\")\n",
    "\n",
    "        # save class variables\n",
    "        self.merges = merges # used in encode()\n",
    "        self.vocab = vocab   # used in decode()\n",
    "\n",
    "    def encode(self, text):\n",
    "        text_bytes = text.encode(\"utf-8\")\n",
    "        ids = list(text_bytes)\n",
    "        while len(ids) > 1:\n",
    "            # find the pair with the lowest merge index\n",
    "            pairs = top_pairs(ids)\n",
    "            pair = min(pairs, key=lambda p: self.merges.get(p, float(\"inf\")))\n",
    "            \n",
    "            if pair not in self.merges:\n",
    "                break # nothing else can be merged anymore\n",
    "            # otherwise let's merge the best pair (lowest merge index)\n",
    "            idx = self.merges[pair]\n",
    "            ids = merge(ids, pair, idx)\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        # given ids (list of integers), return Python string\n",
    "        text_bytes = b\"\".join(self.vocab[idx] for idx in ids)\n",
    "        text = text_bytes.decode(\"utf-8\", errors=\"replace\")\n",
    "        return text\n",
    "\n",
    "    def build_vocab(self):\n",
    "        # vocab is simply and deterministically derived from merges\n",
    "        vocab = {idx: bytes([idx]) for idx in range(256)}\n",
    "        for (p0, p1), idx in self.merges.items():\n",
    "            vocab[idx] = vocab[p0] + vocab[p1]\n",
    "        for special, idx in self.special_tokens.items():\n",
    "            vocab[idx] = special.encode(\"utf-8\")\n",
    "        return vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "1d650c58-cfcf-40dd-9974-abd3c37ad75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_text = \"\"\n",
    "\n",
    "for i, t in top_tokens[:1000]:\n",
    "    simple_text += t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "25ec212d-fa6d-4fd5-8499-640e56a60cd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab max 226\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "600"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize = Tokenizer()\n",
    "\n",
    "ids = tokenize.train(simple_text, 600)\n",
    "\n",
    "len(tokenize.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "68741469-2328-4d47-b22b-c5b19c557f6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'���'"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = [224,225,226]\n",
    "\n",
    "tokenize.decode(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "80597697-e742-4b5c-83b3-5c6f830c39f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_tokens(\"v2_600\", tokenize.merges, tokenize.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "07a75c02-83bc-4b29-8447-0fda39b5877f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(model_file):\n",
    "        \"\"\"Inverse of save() but only for the model file\"\"\"\n",
    "        assert model_file.endswith(\".model\")\n",
    "        # read the model file\n",
    "        merges = {}\n",
    "        idx = 256\n",
    "        with open(model_file, 'r', encoding=\"utf-8\") as f:\n",
    "            # read the version\n",
    "            version = f.readline().strip()\n",
    "            assert version == \"minbpe v1\"\n",
    "            # # read the pattern\n",
    "            # pattern = f.readline().strip()\n",
    "            # read the special tokens\n",
    "            # # num_special = int(f.readline().strip())\n",
    "            # for _ in range(num_special):\n",
    "            #     special, special_idx = f.readline().strip().split()\n",
    "            #     special_tokens[special] = int(special_idx)\n",
    "            # read the merges\n",
    "            for line in f:\n",
    "                idx1, idx2 = map(int, line.split())\n",
    "                merges[(idx1, idx2)] = idx\n",
    "                idx += 1\n",
    "        return merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "2c2bbe2c-9928-49d2-b0a0-76f22817d7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "merges = load(\"v1.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "e5219343-25d5-42a7-ad43-6fd7f9463008",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(101, 32): 256,\n",
       " (116, 32): 257,\n",
       " (100, 32): 258,\n",
       " (114, 32): 259,\n",
       " (110, 32): 260,\n",
       " (116, 104): 261,\n",
       " (111, 117): 262,\n",
       " (105, 110): 263,\n",
       " (115, 32): 264,\n",
       " (121, 32): 265,\n",
       " (101, 259): 266,\n",
       " (101, 97): 267,\n",
       " (103, 32): 268,\n",
       " (226, 128): 269,\n",
       " (108, 108): 270,\n",
       " (101, 114): 271,\n",
       " (111, 119): 272,\n",
       " (263, 268): 273,\n",
       " (84, 104): 274,\n",
       " (111, 109): 275,\n",
       " (103, 104): 276,\n",
       " (97, 110): 277,\n",
       " (101, 260): 278,\n",
       " (101, 258): 279,\n",
       " (101, 110): 280,\n",
       " (111, 32): 281,\n",
       " (111, 114): 282,\n",
       " (107, 32): 283,\n",
       " (270, 32): 284,\n",
       " (111, 110): 285,\n",
       " (104, 97): 286,\n",
       " (101, 108): 287,\n",
       " (104, 105): 288,\n",
       " (115, 257): 289,\n",
       " (115, 256): 290,\n",
       " (111, 260): 291,\n",
       " (261, 266): 292,\n",
       " (102, 32): 293,\n",
       " (118, 256): 294,\n",
       " (108, 258): 295,\n",
       " (97, 114): 296,\n",
       " (276, 257): 297,\n",
       " (115, 116): 298,\n",
       " (111, 111): 299,\n",
       " (97, 108): 300,\n",
       " (117, 114): 301,\n",
       " (269, 156): 302,\n",
       " (269, 153): 303,\n",
       " (97, 109): 304,\n",
       " (97, 265): 305,\n",
       " (114, 101): 306,\n",
       " (97, 260): 307,\n",
       " (272, 32): 308,\n",
       " (99, 104): 309,\n",
       " (98, 101): 310,\n",
       " (66, 101): 311,\n",
       " (119, 104): 312,\n",
       " (87, 104): 313,\n",
       " (116, 116): 314,\n",
       " (105, 297): 315,\n",
       " (101, 257): 316,\n",
       " (110, 258): 317,\n",
       " (256, 83): 318,\n",
       " (110, 111): 319,\n",
       " (32, 32): 320,\n",
       " (261, 101): 321,\n",
       " (271, 256): 322,\n",
       " (108, 256): 323,\n",
       " (101, 118): 324,\n",
       " (115, 287): 325,\n",
       " (261, 273): 326,\n",
       " (101, 119): 327,\n",
       " (119, 105): 328,\n",
       " (32, 109): 329,\n",
       " (262, 295): 330,\n",
       " (105, 259): 331,\n",
       " (262, 259): 332,\n",
       " (97, 115): 333,\n",
       " (114, 267): 334,\n",
       " (97, 99): 335,\n",
       " (97, 107): 336,\n",
       " (101, 264): 337,\n",
       " (276, 32): 338,\n",
       " (100, 265): 339,\n",
       " (100, 264): 340,\n",
       " (87, 105): 341,\n",
       " (109, 32): 342,\n",
       " (275, 32): 343,\n",
       " (274, 101): 344,\n",
       " (285, 268): 345,\n",
       " (263, 258): 346,\n",
       " (267, 259): 347,\n",
       " (111, 290): 348,\n",
       " (267, 114): 349,\n",
       " (116, 301): 350,\n",
       " (10, 32): 351,\n",
       " (97, 257): 352,\n",
       " (72, 105): 353,\n",
       " (121, 262): 354,\n",
       " (261, 32): 355,\n",
       " (258, 72): 356,\n",
       " (115, 104): 357,\n",
       " (108, 101): 358,\n",
       " (115, 101): 359,\n",
       " (111, 292): 360,\n",
       " (272, 260): 361,\n",
       " (103, 97): 362,\n",
       " (325, 293): 363,\n",
       " (105, 114): 364,\n",
       " (280, 257): 365,\n",
       " (262, 338): 366,\n",
       " (314, 266): 367,\n",
       " (97, 121): 368,\n",
       " (368, 264): 369,\n",
       " (114, 105): 370,\n",
       " (105, 260): 371,\n",
       " (73, 32): 372,\n",
       " (256, 261): 373,\n",
       " (89, 262): 374,\n",
       " (79, 110): 375,\n",
       " (72, 97): 376,\n",
       " (117, 257): 377,\n",
       " (309, 32): 378,\n",
       " (105, 115): 379,\n",
       " (285, 256): 380,\n",
       " (262, 257): 381,\n",
       " (112, 32): 382,\n",
       " (108, 105): 383,\n",
       " (275, 256): 384,\n",
       " (108, 265): 385,\n",
       " (109, 97): 386,\n",
       " (105, 116): 387,\n",
       " (304, 256): 388,\n",
       " (69, 118): 389,\n",
       " (107, 279): 390,\n",
       " (105, 284): 391,\n",
       " (65, 108): 392,\n",
       " (111, 112): 393,\n",
       " (327, 32): 394,\n",
       " (117, 115): 395,\n",
       " (115, 291): 396,\n",
       " (65, 317): 397,\n",
       " (32, 65): 398,\n",
       " (111, 259): 399,\n",
       " (105, 258): 400,\n",
       " (87, 288): 401,\n",
       " (256, 77): 402,\n",
       " (65, 114): 403,\n",
       " (76, 105): 404,\n",
       " (105, 109): 405,\n",
       " (256, 84): 406,\n",
       " (117, 289): 407,\n",
       " (97, 116): 408,\n",
       " (101, 284): 409,\n",
       " (324, 266): 410,\n",
       " (102, 116): 411,\n",
       " (262, 297): 412,\n",
       " (335, 283): 413,\n",
       " (111, 289): 414,\n",
       " (335, 256): 415,\n",
       " (97, 292): 416,\n",
       " (263, 100): 417,\n",
       " (267, 258): 418,\n",
       " (299, 283): 419,\n",
       " (299, 259): 420,\n",
       " (262, 317): 421,\n",
       " (105, 100): 422,\n",
       " (370, 280): 423,\n",
       " (261, 271): 424,\n",
       " (119, 282): 425,\n",
       " (87, 282): 426,\n",
       " (121, 326): 427,\n",
       " (99, 97): 428,\n",
       " (101, 115): 429,\n",
       " (108, 267): 430,\n",
       " (269, 148): 431,\n",
       " (108, 100): 432,\n",
       " (116, 114): 433,\n",
       " (112, 112): 434,\n",
       " (300, 283): 435,\n",
       " (277, 258): 436,\n",
       " (116, 281): 437,\n",
       " (97, 264): 438,\n",
       " (104, 256): 439,\n",
       " (73, 257): 440,\n",
       " (319, 257): 441,\n",
       " (78, 111): 442,\n",
       " (83, 32): 443,\n",
       " (105, 264): 444,\n",
       " (265, 77): 445,\n",
       " (265, 119): 446,\n",
       " (309, 329): 447,\n",
       " (256, 115): 448,\n",
       " (277, 100): 449,\n",
       " (286, 257): 450,\n",
       " (271, 265): 451,\n",
       " (110, 308): 452,\n",
       " (65, 110): 453,\n",
       " (107, 256): 454,\n",
       " (256, 97): 455,\n",
       " (299, 258): 456,\n",
       " (117, 378): 457,\n",
       " (111, 295): 458,\n",
       " (102, 282): 459,\n",
       " (32, 71): 460,\n",
       " (119, 305): 461,\n",
       " (116, 119): 462,\n",
       " (276, 116): 463,\n",
       " (328, 261): 464,\n",
       " (341, 261): 465,\n",
       " (101, 121): 466,\n",
       " (109, 414): 467,\n",
       " (102, 256): 468,\n",
       " (289, 76): 469,\n",
       " (109, 277): 470,\n",
       " (101, 270): 471,\n",
       " (101, 109): 472,\n",
       " (110, 100): 473,\n",
       " (99, 256): 474,\n",
       " (103, 111): 475,\n",
       " (71, 111): 476,\n",
       " (103, 105): 477,\n",
       " (107, 273): 478,\n",
       " (110, 394): 479,\n",
       " (263, 103): 480,\n",
       " (101, 278): 481,\n",
       " (115, 301): 482,\n",
       " (350, 110): 483,\n",
       " (98, 111): 484,\n",
       " (66, 111): 485,\n",
       " (99, 262): 486,\n",
       " (117, 290): 487,\n",
       " (101, 292): 488,\n",
       " (101, 112): 489,\n",
       " (282, 257): 490,\n",
       " (298, 266): 491,\n",
       " (115, 279): 492,\n",
       " (101, 287): 493,\n",
       " (97, 270): 494,\n",
       " (97, 331): 495,\n",
       " (101, 298): 496,\n",
       " (77, 97): 497,\n",
       " (286, 114): 498,\n",
       " (82, 267): 499,\n",
       " (334, 339): 500,\n",
       " (115, 119): 501,\n",
       " (256, 274): 502,\n",
       " (293, 79): 503,\n",
       " (84, 281): 504,\n",
       " (73, 260): 505,\n",
       " (274, 352): 506,\n",
       " (353, 264): 507,\n",
       " (72, 266): 508,\n",
       " (374, 32): 509,\n",
       " (65, 264): 510,\n",
       " (66, 256): 511,\n",
       " (320, 320): 512,\n",
       " (269, 157): 513,\n",
       " (73, 264): 514,\n",
       " (115, 97): 515,\n",
       " (83, 97): 516,\n",
       " (376, 294): 517,\n",
       " (342, 98): 518,\n",
       " (66, 265): 519,\n",
       " (401, 447): 520,\n",
       " (83, 281): 521,\n",
       " (102, 114): 522,\n",
       " (70, 114): 523,\n",
       " (275, 303): 524,\n",
       " (87, 322): 525,\n",
       " (87, 330): 526,\n",
       " (78, 281): 527,\n",
       " (73, 293): 528,\n",
       " (375, 256): 529,\n",
       " (313, 278): 530,\n",
       " (87, 450): 531,\n",
       " (313, 111): 532,\n",
       " (84, 32): 533,\n",
       " (67, 330): 534,\n",
       " (87, 256): 535,\n",
       " (256, 121): 536,\n",
       " (89, 332): 537,\n",
       " (80, 32): 538,\n",
       " (296, 256): 539,\n",
       " (344, 109): 540,\n",
       " (45, 45): 541,\n",
       " (78, 308): 542,\n",
       " (269, 152): 543,\n",
       " (307, 274): 544,\n",
       " (115, 275): 545,\n",
       " (277, 265): 546,\n",
       " (116, 405): 547,\n",
       " (98, 381): 548,\n",
       " (278, 274): 549,\n",
       " (112, 291): 550,\n",
       " (77, 114): 551,\n",
       " (307, 103): 552,\n",
       " (83, 104): 553,\n",
       " (68, 32): 554,\n",
       " (87, 409): 555,\n",
       " (263, 283): 556,\n",
       " (281, 104): 557,\n",
       " (111, 102): 558,\n",
       " (97, 362): 559,\n",
       " (65, 362): 560,\n",
       " (118, 266): 561,\n",
       " (411, 266): 562,\n",
       " (116, 111): 563,\n",
       " (281, 84): 564,\n",
       " (288, 109): 565,\n",
       " (353, 109): 566,\n",
       " (286, 284): 567,\n",
       " (274, 262): 568,\n",
       " (102, 364): 569,\n",
       " (70, 364): 570,\n",
       " (413, 66): 571,\n",
       " (79, 117): 572,\n",
       " (117, 116): 573,\n",
       " (104, 277): 574,\n",
       " (110, 268): 575,\n",
       " (109, 336): 576,\n",
       " (336, 256): 577,\n",
       " (69, 121): 578,\n",
       " (389, 271): 579,\n",
       " (121, 303): 580,\n",
       " (262, 290): 581,\n",
       " (366, 274): 582,\n",
       " (108, 97): 583,\n",
       " (379, 264): 584,\n",
       " (108, 299): 585,\n",
       " (116, 336): 586,\n",
       " (83, 116): 587,\n",
       " (79, 104): 588,\n",
       " (113, 117): 589,\n",
       " (81, 117): 590,\n",
       " (119, 288): 591,\n",
       " (472, 279): 592,\n",
       " (473, 266): 593,\n",
       " (271, 363): 594,\n",
       " (119, 275): 595,\n",
       " (87, 275): 596,\n",
       " (111, 343): 597,\n",
       " (119, 369): 598,\n",
       " (111, 294): 599,\n",
       " (115, 111): 600,\n",
       " (108, 415): 601,\n",
       " (101, 393): 602,\n",
       " (602, 323): 603,\n",
       " (102, 257): 604,\n",
       " (76, 101): 605,\n",
       " (104, 349): 606,\n",
       " (119, 32): 607,\n",
       " (294, 71): 608,\n",
       " (286, 118): 609,\n",
       " (287, 257): 610,\n",
       " (275, 365): 611,\n",
       " (256, 102): 612,\n",
       " (423, 258): 613,\n",
       " (101, 326): 614,\n",
       " (105, 99): 615,\n",
       " (327, 286): 616,\n",
       " (480, 264): 617,\n",
       " (103, 280): 618,\n",
       " (116, 358): 619,\n",
       " (97, 259): 620,\n",
       " (279, 112): 621,\n",
       " (296, 257): 622,\n",
       " (282, 110): 623,\n",
       " (623, 273): 624,\n",
       " (462, 481): 625,\n",
       " (277, 257): 626,\n",
       " (272, 296): 627,\n",
       " (627, 340): 628,\n",
       " (349, 264): 629,\n",
       " (483, 279): 630,\n",
       " (84, 301): 631,\n",
       " (103, 307): 632,\n",
       " (65, 115): 633,\n",
       " (101, 279): 634,\n",
       " (258, 116): 635,\n",
       " (109, 121): 636,\n",
       " (350, 260): 637,\n",
       " (115, 112): 638,\n",
       " (267, 283): 639,\n",
       " (393, 256): 640,\n",
       " (112, 271): 641,\n",
       " (286, 112): 642,\n",
       " (642, 264): 643,\n",
       " (80, 271): 644,\n",
       " (263, 99): 645,\n",
       " (72, 272): 646,\n",
       " (324, 271): 647,\n",
       " (114, 290): 648,\n",
       " (67, 262): 649,\n",
       " (304, 105): 650,\n",
       " (650, 385): 651,\n",
       " (428, 487): 652,\n",
       " (312, 111): 653,\n",
       " (114, 360): 654,\n",
       " (285, 101): 655,\n",
       " (112, 306): 656,\n",
       " (115, 365): 657,\n",
       " (80, 306): 658,\n",
       " (395, 98): 659,\n",
       " (101, 120): 660,\n",
       " (660, 257): 661,\n",
       " (364, 108): 662,\n",
       " (282, 258): 663,\n",
       " (114, 412): 664,\n",
       " (395, 263): 665,\n",
       " (665, 429): 666,\n",
       " (666, 264): 667,\n",
       " (39, 32): 668,\n",
       " (99, 288): 669,\n",
       " (67, 288): 670,\n",
       " (267, 355): 671,\n",
       " (277, 441): 672,\n",
       " (386, 284): 673,\n",
       " (275, 273): 674,\n",
       " (271, 300): 675,\n",
       " (333, 492): 676,\n",
       " (290, 102): 677,\n",
       " (116, 256): 678,\n",
       " (270, 119): 679,\n",
       " (104, 346): 680,\n",
       " (280, 273): 681,\n",
       " (67, 494): 682,\n",
       " (287, 105): 683,\n",
       " (683, 101): 684,\n",
       " (684, 294): 685,\n",
       " (498, 100): 686,\n",
       " (117, 284): 687,\n",
       " (110, 433): 688,\n",
       " (688, 265): 689,\n",
       " (423, 340): 690,\n",
       " (296, 103): 691,\n",
       " (691, 256): 692,\n",
       " (300, 390): 693,\n",
       " (111, 337): 694,\n",
       " (115, 117): 695,\n",
       " (83, 117): 696,\n",
       " (97, 98): 697,\n",
       " (697, 323): 698,\n",
       " (304, 32): 699}"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "c8a2f9d7-0018-45c3-b3dd-6348ff833236",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_tokens(file_prefix, merges, vocab):\n",
    "        \"\"\"\n",
    "        Saves two files: file_prefix.vocab and file_prefix.model\n",
    "        This is inspired (but not equivalent to!) sentencepiece's model saving:\n",
    "        - model file is the critical one, intended for load()\n",
    "        - vocab file is just a pretty printed version for human inspection only\n",
    "        \"\"\"\n",
    "        # write the model: to be used in load() later\n",
    "        model_file = file_prefix + \".model\"\n",
    "        with open(model_file, 'w') as f:\n",
    "            # write the version, pattern and merges, that's all that's needed\n",
    "            f.write(\"minbpe v1\\n\")\n",
    "            # f.write(f\"{self.pattern}\\n\")\n",
    "            # # write the special tokens, first the number of them, then each one\n",
    "            # f.write(f\"{len(self.special_tokens)}\\n\")\n",
    "            # for special, idx in self.special_tokens.items():\n",
    "            #     f.write(f\"{special} {idx}\\n\")\n",
    "            # the merges dict\n",
    "            for idx1, idx2 in merges:\n",
    "                f.write(f\"{idx1} {idx2}\\n\")\n",
    "        # write the vocab: for the human to look at\n",
    "        vocab_file = file_prefix + \".vocab\"\n",
    "        inverted_merges = {idx: pair for pair, idx in merges.items()}\n",
    "        with open(vocab_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            for idx, token in vocab.items():\n",
    "                # note: many tokens may be partial utf-8 sequences\n",
    "                # and cannot be decoded into valid strings. Here we're using\n",
    "                # errors='replace' to replace them with the replacement char �.\n",
    "                # this also means that we couldn't possibly use .vocab in load()\n",
    "                # because decoding in this way is a lossy operation!\n",
    "                s = render_token(token)\n",
    "                # find the children of this token, if any\n",
    "                if idx in inverted_merges:\n",
    "                    # if this token has children, render it nicely as a merge\n",
    "                    idx0, idx1 = inverted_merges[idx]\n",
    "                    s0 = render_token(vocab[idx0])\n",
    "                    s1 = render_token(vocab[idx1])\n",
    "                    f.write(f\"[{s0}][{s1}] -> [{s}] {idx}\\n\")\n",
    "                else:\n",
    "                    # otherwise this is leaf token, just print it\n",
    "                    # (this should just be the first 256 tokens, the bytes)\n",
    "                    f.write(f\"[{s}] {idx}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
