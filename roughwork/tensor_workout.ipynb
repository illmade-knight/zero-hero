{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edf3b50d-7494-47c3-af94-2f3b0654726c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d541c1-b8e1-4aeb-b90b-2d0a9f8e6333",
   "metadata": {},
   "source": [
    "### Let's workout some torch tensor basics\n",
    "\n",
    "First create some values that are easy to see in multiple dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16f436ef-288c-496c-9866-02958bf42ba6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28,\n",
       "        29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mk = torch.arange(11, 40)\n",
    "\n",
    "mk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df27ab21-dc39-4fcf-8e7f-577de88fa4d0",
   "metadata": {},
   "source": [
    "create a 2D stack so we have 1's 2's and 3's (we'll skip some to make this work)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f559f318-4558-4334-bea3-c9f3c5aad9e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 9]),\n",
       " tensor([[11, 12, 13, 14, 15, 16, 17, 18, 19],\n",
       "         [21, 22, 23, 24, 25, 26, 27, 28, 29],\n",
       "         [31, 32, 33, 34, 35, 36, 37, 38, 39]]),\n",
       " tensor([11, 12, 13, 14, 15, 16, 17, 18, 19, 21, 22, 23, 24, 25, 26, 27, 28, 29,\n",
       "         31, 32, 33, 34, 35, 36, 37, 38, 39]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex = torch.vstack( (mk[0:9], mk[10:19], mk[20:29]))\n",
    "\n",
    "ex.shape, ex, ex.view(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690b35b7-bfdd-4052-bc8b-446c1a3d36c1",
   "metadata": {},
   "source": [
    "So we were able to stack splits of the original tensor and then flatten that out using view\n",
    "\n",
    "What happens if we take the Transpose?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e43437db-a8a6-4c27-9c08-0ceaa179e889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 3])\n",
      "tensor([[11, 21, 31],\n",
      "        [12, 22, 32],\n",
      "        [13, 23, 33],\n",
      "        [14, 24, 34],\n",
      "        [15, 25, 35],\n",
      "        [16, 26, 36],\n",
      "        [17, 27, 37],\n",
      "        [18, 28, 38],\n",
      "        [19, 29, 39]])\n"
     ]
    }
   ],
   "source": [
    "ex_t = ex.T\n",
    "print(ex_t.shape)\n",
    "print(ex_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8cd0952a-d2f9-4a24-85c2-c612b266a8fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[11, 21, 31],\n",
      "        [12, 22, 32],\n",
      "        [13, 23, 33]])\n",
      "tensor([[14, 24, 34],\n",
      "        [15, 25, 35],\n",
      "        [16, 26, 36]])\n",
      "torch.cat or those: tensor([[11, 21, 31],\n",
      "        [12, 22, 32],\n",
      "        [13, 23, 33],\n",
      "        [14, 24, 34],\n",
      "        [15, 25, 35],\n",
      "        [16, 26, 36]])\n",
      "cat after changing element [0][0]: tensor([[ 1, 21, 31],\n",
      "        [12, 22, 32],\n",
      "        [13, 23, 33],\n",
      "        [14, 24, 34],\n",
      "        [15, 25, 35],\n",
      "        [16, 26, 36]])\n",
      "the orginal tensors first row is unchanged tensor([11, 21, 31])\n"
     ]
    }
   ],
   "source": [
    "tbt = ex_t.view(3, 3, 3)\n",
    "print(tbt[0])\n",
    "print(tbt[1])\n",
    "\n",
    "cat = torch.cat((tbt[0], tbt[1]), 0)\n",
    "print(\"torch.cat or those:\", cat)\n",
    "cat[0][0] = 1\n",
    "print(\"cat after changing element [0][0]:\", cat)\n",
    "print(\"the orginal tensors first row is unchanged\", tbt[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3433464f-395e-4059-8371-fb033768ecf0",
   "metadata": {},
   "source": [
    "### Limits on view\n",
    "\n",
    "the tensor is created with an underlying representation of given size and stride"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "aff5e42a-5038-4073-854d-a2593ca3be9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    ex.T.view(-1)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724af9de-c659-4426-b4d6-e75a330de144",
   "metadata": {},
   "source": [
    "What happened there?\n",
    "\n",
    "It's not inherent to the shape of the tensor - if we created a [9, 3] tensor - we can view it fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "378f2357-d856-4b75-8d85-1c1c5b9d516a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([9, 3]),\n",
       " tensor([[11, 12, 13],\n",
       "         [14, 15, 16],\n",
       "         [17, 18, 19],\n",
       "         [21, 22, 23],\n",
       "         [24, 25, 26],\n",
       "         [27, 28, 29],\n",
       "         [31, 32, 33],\n",
       "         [34, 35, 36],\n",
       "         [37, 38, 39]]),\n",
       " tensor([11, 12, 13, 14, 15, 16, 17, 18, 19, 21, 22, 23, 24, 25, 26, 27, 28, 29,\n",
       "         31, 32, 33, 34, 35, 36, 37, 38, 39]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex_v = torch.vstack( (mk[0:3], mk[3:6], mk[6:9], mk[10:13], mk[13:16], mk[16:19], mk[20:23], mk[23:26], mk[26:29]))\n",
    "\n",
    "ex_v.shape, ex_v, ex_v.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7413f63e-1d23-4393-8c88-3658bcc5ef5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[11, 14, 17, 21, 24, 27, 31, 34, 37],\n",
      "        [12, 15, 18, 22, 25, 28, 32, 35, 38],\n",
      "        [13, 16, 19, 23, 26, 29, 33, 36, 39]])\n",
      "view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.\n"
     ]
    }
   ],
   "source": [
    "print(ex_v.T)\n",
    "try:\n",
    "    ex_v.T.view(-1)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fe7720-2454-44c3-a98a-1aadac3c85e6",
   "metadata": {},
   "source": [
    "again something's going on underneath that doesn't relate to the tensor directly but to some underlying representation\n",
    "\n",
    "(as an aside using the transpose operation is the same as using the .t function on the tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05b916fa-d838-45f6-9cc2-82208e7fe3fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[11, 14, 17, 21, 24, 27, 31, 34, 37],\n",
      "        [12, 15, 18, 22, 25, 28, 32, 35, 38],\n",
      "        [13, 16, 19, 23, 26, 29, 33, 36, 39]])\n",
      "tensor([[True, True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True, True]])\n"
     ]
    }
   ],
   "source": [
    "op_t = torch.transpose(ex_v, 0, 1)\n",
    "print(op_t)\n",
    "print(op_t == ex_v.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b76328d5-1d7d-4191-bfdd-ace58e6bdf6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 9]), torch.Size([3, 9]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex.shape, ex_v.T.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2100b66e-ff59-46e5-a717-ce3c5248e7cd",
   "metadata": {},
   "source": [
    "### We can reshape stuff\n",
    "\n",
    "using reshape allows us to change the underlying representation \n",
    "\n",
    "this allows us to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf6f501c-7c3c-442a-8377-e2be421b8f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 9])\n",
      "tensor([[11, 14, 17],\n",
      "        [21, 24, 27],\n",
      "        [31, 34, 37],\n",
      "        [12, 15, 18],\n",
      "        [22, 25, 28],\n",
      "        [32, 35, 38],\n",
      "        [13, 16, 19],\n",
      "        [23, 26, 29],\n",
      "        [33, 36, 39]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([11, 14, 17, 21, 24, 27, 31, 34, 37, 12, 15, 18, 22, 25, 28, 32, 35, 38,\n",
       "        13, 16, 19, 23, 26, 29, 33, 36, 39])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(op_t.shape)\n",
    "ex_v_re = op_t.reshape(9, 3)\n",
    "print(ex_v_re)\n",
    "ex_v_re.view(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069bde49-413e-410c-bd6b-6b13b59fdfb3",
   "metadata": {},
   "source": [
    "just to keep track here's what happens if we don't reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "55e52ad6-1c37-47c7-8970-2fdf85bd7ae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.\n"
     ]
    }
   ],
   "source": [
    "try: \n",
    "    op_t.view(-1)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a76150e-941b-4597-8b85-6aee1dca83fb",
   "metadata": {},
   "source": [
    "### The underlying representation is only changed if necessary\n",
    "\n",
    "reshape doesn't necessarily change the underlying representation...\n",
    "\n",
    "(f.t.d: *When possible, the returned tensor will be a view of input. Otherwise, it will be a copy*)\n",
    "\n",
    "so be careful reshape won't always create a copy and you'll need to do that manually if you don't want to change the orignal subsequently "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2ed2289-e2c3-47ca-a0bb-a1b3365c2aba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[11, 14, 17],\n",
       "          [21, 24, 27],\n",
       "          [31, 34, 37]],\n",
       " \n",
       "         [[12, 15, 18],\n",
       "          [22, 25, 28],\n",
       "          [32, 35, 38]],\n",
       " \n",
       "         [[13, 16, 19],\n",
       "          [23, 26, 29],\n",
       "          [33, 36, 39]]]),\n",
       " tensor([[[11, 14, 17],\n",
       "          [21, 24, 27],\n",
       "          [31, 34, 37]],\n",
       " \n",
       "         [[12, 15, 18],\n",
       "          [22, 25, 28],\n",
       "          [32, 35, 38]],\n",
       " \n",
       "         [[13, 16, 19],\n",
       "          [23, 26, 29],\n",
       "          [33, 36, 39]]]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "another_shape = op_t.reshape(3, 3, 3)\n",
    "another_shape, op_t.view(3, 3, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f488f3b-9d26-417b-aac3-9b7a1ec98ebf",
   "metadata": {},
   "source": [
    "### linear algebra\n",
    "\n",
    "ok enough on that for the moment\n",
    "\n",
    "onto some basic linear algebra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f9119ec-43b5-49c2-bea9-4fb2c22f9846",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 1.],\n",
       "         [1., 1.],\n",
       "         [1., 1.]], dtype=torch.float64),\n",
       " tensor([[2., 2., 2.],\n",
       "         [3., 3., 3.]]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ones = torch.ones(3, 2, dtype=float)\n",
    "\n",
    "twothree = torch.tensor([[2,2,2], [3,3,3]], dtype=torch.float)\n",
    "\n",
    "ones, twothree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e0378a77-f364-443a-8214-b9a5e8a0c395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expected m1 and m2 to have the same dtype, but got: float != double\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    twothree @ ones\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1394d253-f5f3-46a5-b6e9-d78538397a3d",
   "metadata": {},
   "source": [
    "another one of those sightly weird things to keep track of... dtype=torch.float != dtype=float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ed418a7f-1cca-4ba1-b049-34b5c753aee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float64 torch.float32\n"
     ]
    }
   ],
   "source": [
    "print(ones.dtype, twothree.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "393140e0-ba0c-4935-b09d-59b00f934c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "ones = torch.ones(3, 2, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4a376738-91ea-46f7-8fc7-7f26495ee00c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3]),\n",
       " torch.Size([3, 2]),\n",
       " tensor([[6., 6.],\n",
       "         [9., 9.]]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twothree.shape, ones.shape, twothree @ ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "396647e4-0e80-4d5d-b89f-865d94f69efb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5., 5., 5.],\n",
       "        [5., 5., 5.],\n",
       "        [5., 5., 5.]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ones @ twothree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc49bf5-7add-497c-bafb-bf695d12dea1",
   "metadata": {},
   "source": [
    "and just to review the basic operation: a dot b (or a @ b as it's written in torch here)\n",
    "\n",
    "so twothree @ ones is:\n",
    "\n",
    "take twothree's first row: twothree[0] = tensor([2., 2., 2.])\n",
    "\n",
    "and the one's first column [1, 1, 1]\n",
    "\n",
    "and 2*1 + 2*1 + 2*1 = 6\n",
    "\n",
    "the same for first row, second column (again 6)\n",
    "\n",
    "so the first row of the result is: [6, 6]\n",
    "\n",
    "to form the second row we follow the same sequence to get: [9, 9]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0bfade-9ad0-431e-8ffe-3c73ebf9f050",
   "metadata": {},
   "source": [
    "### linear layers\n",
    "\n",
    "dot product is the way we get our linear layer to work efficiently\n",
    "\n",
    "a linear layer combines the input with weights to form an output - this is the neuron idea \n",
    "\n",
    "and just to be clear for a thing (an input) with 5 values, the neuron needs 5 weights \n",
    "\n",
    "for one neuron we get one output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "885b3c46-6944-4c19-bda6-b096ca62ab98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0450,  0.6786,  1.4062,  0.3017,  0.6999])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([]), tensor(3.0414), tensor(3.0414))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.ones(5)\n",
    "w = torch.randn(5)\n",
    "out = input @ w\n",
    "\n",
    "print(w)\n",
    "out.shape, out, w.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a078e3-e51c-4282-9ea3-1c2b0688257a",
   "metadata": {},
   "source": [
    "so to create multiple neurons we increase the weights dimension\n",
    "\n",
    "if we have 3 neurons we get 3 outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "32e08b31-eea4-45b1-92b4-e25fcfb370f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5632, 4.3752, 3.5168])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = torch.randn(5, 3)\n",
    "out = input @ w\n",
    "\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b1cc87-e2b8-45be-a289-27ec2dfe5632",
   "metadata": {},
   "source": [
    "### multidimension input\n",
    "\n",
    "we can add a channel dimension to the input\n",
    "\n",
    "say we have a bunch of points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "be4c786a-e983-4c56-badb-3094a01550dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 2])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.tensor([[0, 1], [0.5, 0.866], [0.707, 0.707], [1, 0], [3, 4]], dtype=torch.float)\n",
    "input.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e899b0e4-bb72-40ee-a0df-fdc318492f8c",
   "metadata": {},
   "source": [
    "### Lets give these dimensions some names\n",
    "\n",
    "we have 5 things with 2 dimensions\n",
    "\n",
    "lets call the 5 things the F dimensoion (for Feature maybe)\n",
    "\n",
    "and the point dimensions C (for Channel maybe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52cdbeac-9af0-401a-b26a-60c206ab41d7",
   "metadata": {},
   "source": [
    "We can't just use our new input with the original weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2147b9cd-c994-4451-8564-6b9973de0472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mat1 and mat2 shapes cannot be multiplied (5x2 and 5x3)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    out = input @ w\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884ab39c-f7fc-4498-964e-991da78644d0",
   "metadata": {},
   "source": [
    "we might think we can define our weights like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9c36b337-e666-4916-b5ab-47aa3f7f43f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.2074,  0.2139,  0.3946],\n",
       "         [ 2.0351, -0.4093, -1.0297]],\n",
       "\n",
       "        [[ 0.3734,  0.5522,  1.3682],\n",
       "         [ 0.1672, -0.4822, -0.4234]],\n",
       "\n",
       "        [[-0.9352, -0.1425,  0.0659],\n",
       "         [-0.5288,  0.2957, -0.5723]],\n",
       "\n",
       "        [[-2.0087,  0.2470,  0.2548],\n",
       "         [ 1.7518,  0.2243,  0.3648]],\n",
       "\n",
       "        [[ 0.9314,  0.0761,  1.0594],\n",
       "         [ 1.2530,  0.3352,  0.3342]]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nw = torch.randn(5, 2, 3)\n",
    "nw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d2781e-e806-4721-90d6-fa6c804f0e25",
   "metadata": {},
   "source": [
    "and we get a dot product ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a9fbed8e-12df-46c8-8b0e-4c3d93d4b560",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 5, 3]),\n",
       " tensor([[ 2.0351, -0.4093, -1.0297],\n",
       "         [ 2.8661, -0.2475, -0.6944],\n",
       "         [ 2.9994, -0.1381, -0.4490],\n",
       "         [ 2.2074,  0.2139,  0.3946],\n",
       "         [14.7624, -0.9954, -2.9351]]))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = input @ nw\n",
    "out.shape, out[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51231b4-09c6-4ec8-9896-0b1e0c4834e1",
   "metadata": {},
   "source": [
    "but that's not really what we wanted is it?\n",
    "\n",
    "we have 3 neurons so we wanted three activation outputs\n",
    "\n",
    "To make the neuron thing obvious we'd prefer something like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c02a4c6b-c0f6-4b0e-b468-79761c0c97e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.9002,  0.3244],\n",
       "          [-0.4088, -1.0205],\n",
       "          [ 0.3538, -0.9612],\n",
       "          [-0.0234,  0.7453],\n",
       "          [-1.3945,  0.1509]],\n",
       " \n",
       "         [[-0.2610, -0.6733],\n",
       "          [-0.1594,  1.0097],\n",
       "          [ 0.3821, -0.2656],\n",
       "          [-1.6459, -0.2405],\n",
       "          [ 0.7149, -0.6207]],\n",
       " \n",
       "         [[-0.7647, -2.4796],\n",
       "          [ 0.3835,  0.8685],\n",
       "          [ 1.3706, -0.0041],\n",
       "          [ 0.3463,  0.6508],\n",
       "          [-0.0868,  1.8840]]]),\n",
       " torch.Size([3, 5, 2]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nw_b = torch.randn(3, 5, 2)\n",
    "nw_b, nw_b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eb726c7b-cdf9-487f-adb1-8612629e0feb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.9002,  0.3244, -0.4088, -1.0205,  0.3538],\n",
       "         [-0.9612, -0.0234,  0.7453, -1.3945,  0.1509]],\n",
       "\n",
       "        [[-0.2610, -0.6733, -0.1594,  1.0097,  0.3821],\n",
       "         [-0.2656, -1.6459, -0.2405,  0.7149, -0.6207]],\n",
       "\n",
       "        [[-0.7647, -2.4796,  0.3835,  0.8685,  1.3706],\n",
       "         [-0.0041,  0.3463,  0.6508, -0.0868,  1.8840]]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# or nw_b.view(3, 2, 5), which will allow dot product\n",
    "nw_b.view(3, 2, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "aa901f83-d279-4ae2-8d96-0be7c147891e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9612, -0.0234,  0.7453, -1.3945,  0.1509],\n",
       "        [-0.3823,  0.1420,  0.4410, -1.7179,  0.3076],\n",
       "        [-0.0431,  0.2129,  0.2378, -1.7074,  0.3568],\n",
       "        [ 0.9002,  0.3244, -0.4088, -1.0205,  0.3538],\n",
       "        [-1.1440,  0.8798,  1.7545, -8.6395,  1.6649]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "three_outputs = input @ nw_b.view(3, 2, 5)\n",
    "three_outputs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af00a69-8011-4f6f-9fb8-e016046e068c",
   "metadata": {},
   "source": [
    "### The wrong path\n",
    "\n",
    "again not what we want at all:\n",
    "\n",
    "actually what we do is simpler - looking at a torch linear layer, we're allowed to define the 1D in_features and the 1D out_features\n",
    "\n",
    "we nearly had that above 3 groups of 10 things (our 10 things in 2 dimensions though)\n",
    "\n",
    "we just make the features_in = F * C, and the number of neurons is out_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0d618d96-8992-48a2-bf1c-fc3cd8a5c5f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2478,  0.0591, -0.0774,  0.0024, -0.1309,  0.0833,  0.0797,  0.2091,\n",
       "          0.0818,  0.3126],\n",
       "        [-0.1192, -0.0415, -0.0747, -0.2269, -0.2640,  0.1607, -0.2473, -0.2439,\n",
       "          0.0712,  0.2550],\n",
       "        [-0.0193,  0.1985, -0.2678, -0.2923,  0.2221, -0.0466, -0.0418,  0.0893,\n",
       "          0.2043, -0.0953]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linl = nn.Linear(5 * 2, 3, bias=False)\n",
    "linl.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2e7b04c7-ffa8-4ec3-b24a-ba7c1e177283",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.5643, 0.6378, 0.1255], grad_fn=<SqueezeBackward4>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linl(input.view(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7767a46-eb62-461e-91b7-5ee81d87e0e7",
   "metadata": {},
   "source": [
    "### Next time:\n",
    "\n",
    "we'll start looking at specific manipulations for our gpt workout"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
