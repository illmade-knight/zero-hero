{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "965b36f1-1440-4a07-a843-80cba31c34e8",
   "metadata": {},
   "source": [
    "### Missing something again\n",
    "\n",
    "what's the difference between multi head and a bigger head?\n",
    "\n",
    "again the attention is linear unit - if we just make it wider isn't that the same as having mulitple smaller heads?\n",
    "\n",
    "Going to the paper:\n",
    "\n",
    "*Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this*\n",
    "\n",
    "so apparently the softmax function is the problem and that's why the multihead attention is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d689a91-95ca-42cd-802e-ab97496243cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec8919ad-3d6b-4421-94ce-553e3a506dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run bookreader.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5b6ef18-283c-41fd-9375-ef8d13ed5e0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "br = BookReader(False, r'[^\\d+a-zA-Z \\n?!:,]')\n",
    "br.read(\"tiny_shakespeare.txt\")\n",
    "vocab_size = br.vocab_size\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a713daf8-4a5f-430d-8f00-9c6db6743d22",
   "metadata": {},
   "source": [
    "### Get the batch with both x and y unseparated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91f9fe0b-25d9-4578-a105-4eb99138428b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(data, batch_length=5, batch_size=5):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    ix = torch.randint(len(data) - batch_length, (batch_size,))\n",
    "    b = torch.stack([data[i:i+batch_length] for i in ix])\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac12ed62-275e-43a8-b17e-9b0b6da58efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = torch.tensor(br.data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3b116f-f526-4958-bfed-07f665e89cba",
   "metadata": {},
   "source": [
    "### Create an attention head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf6cf111-bf37-49fe-8e92-de8a35f8e683",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "\n",
    "    def __init__(self, c, head_size, content_length):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(c, head_size, bias=False)\n",
    "        self.query = nn.Linear(c, head_size, bias=False)\n",
    "        self.value = nn.Linear(c, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(content_length, content_length)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   \n",
    "        q = self.query(x)\n",
    "        \n",
    "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 \n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        \n",
    "        v = self.value(x) # (B,T,hs)\n",
    "        out = wei @ v\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7138eed8-721e-4752-bf9f-8ebca24882d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, fan_in, multiplier = 4):\n",
    "        super().__init__()\n",
    "\n",
    "        layers = OrderedDict([\n",
    "            (\"l_in\", nn.Linear(fan_in, multiplier * fan_in)),\n",
    "            (\"relu\", nn.ReLU()),\n",
    "            (\"l_out\", nn.Linear(multiplier * fan_in, fan_in)),\n",
    "        ])\n",
    "        self.net = nn.Sequential(\n",
    "            layers\n",
    "        )\n",
    "\n",
    "        initial = layers['l_in']\n",
    "        nn.init.kaiming_normal_(initial.weight, nonlinearity=\"relu\")\n",
    "        layers['l_in'].weight.data = initial.weight.data * 3/5\n",
    "        if initial.bias is not None:\n",
    "            nn.init.constant_(initial.bias, 0)\n",
    "\n",
    "        final = layers['l_out']\n",
    "        layers['l_out'].weight.data = final.weight.data * .2\n",
    "        if final.bias is not None:\n",
    "            nn.init.constant_(final.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b77ec387-64c7-43b3-82a5-b24e22eb84a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "989f2ee4-1968-4bf6-8352-4e37ebaca633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MultiHead(nn.Module):\n",
    "\n",
    "#     def __init__(self, num_heads, head_size, embed_size, content_length):\n",
    "#         super().__init__()\n",
    "#         self.heads = nn.ModuleList([Head(embed_size, head_size, content_length) for _ in range(num_heads)])\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         out = torch.cat( [head(x) for head in self.heads], dim = -1 )\n",
    "#         return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c111acc3-cd35-46fe-bb16-413aeccbb4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadJoin(nn.Module):\n",
    "    \n",
    "    def __init__(self, c, num_heads, head_size, content_length):\n",
    "        super().__init__()\n",
    "        self.query = nn.Linear(c, head_size * num_heads, bias=False)\n",
    "        self.key = nn.Linear(c, head_size * num_heads, bias=False)\n",
    "        self.value = nn.Linear(c, head_size * num_heads, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(content_length, content_length)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   \n",
    "        q = self.query(x)\n",
    "        \n",
    "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 \n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        \n",
    "        v = self.value(x) # (B,T,hs)\n",
    "        out = wei @ v\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dff586a7-6f05-4e2a-95f4-9a07c8e27203",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFMultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_size, content_length, num_heads, head_size, multiplier=4):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.vocab_embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.positional_embed = nn.Embedding(content_length, embed_size)\n",
    "        self.mutli_attention = MultiHeadJoin(embed_size, num_heads, head_size, content_length)\n",
    "        self.lna = nn.LayerNorm(embed_size)\n",
    "        self.ff = FeedForward(embed_size, multiplier)\n",
    "        self.lnff = nn.LayerNorm(embed_size)\n",
    "        self.decode = nn.Linear(embed_size, vocab_size)\n",
    "        self.content_length = content_length\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        #idx B,T\n",
    "        B, T = idx.shape\n",
    "\n",
    "        idx_e = self.vocab_embed(idx)\n",
    "        # note tr is always the same - so the learning here is information passed back to the positional_embed from loss\n",
    "        tr = torch.arange(T)\n",
    "        pos_e = self.positional_embed(tr)\n",
    "\n",
    "        x = idx_e + pos_e\n",
    "        x = self.mutli_attention(x)\n",
    "        # print(\"multi ball out\", x.shape)\n",
    "        x = self.lna(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.lnff(x)\n",
    "        # print(\"feed forward out\", x.shape)\n",
    "        logits = self.decode(x)\n",
    "        # print(\"decode out\", x.shape)\n",
    "        # print(\"targets\", targets.shape)\n",
    "        # return None, None\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            targets = targets.reshape(B*T)\n",
    "            loss = F.cross_entropy(logits.view(B*T, -1), targets) #.resize(B*T)) # loss function\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -self.content_length:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16926363-2c95-4998-a0a9-8eee16e1abf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_val_batch(data, batch_length=5, batch_size=5, i=0):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    if i == 0:\n",
    "        ix = torch.randint(len(data) - batch_length, (batch_size,))\n",
    "    else:\n",
    "        ix = torch.arange(1, 5) + 1 + i\n",
    "\n",
    "    b = torch.stack([data[i:i+batch_length] for i in ix])\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f874381a-99eb-4bba-bde0-b10c9632e5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "@torch.no_grad()\n",
    "def split_loss(split):\n",
    "    split_len = len(split)\n",
    "    total_loss = 0\n",
    "    batch_size = 50\n",
    "    num_batches = math.floor(split_len / batch_size)\n",
    "    print(\"num_batches\", split_len, num_batches)\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    for i in range(num_batches):\n",
    "\n",
    "        t_b = get_val_batch(split, context_length+1, batch_size, i*batch_size)\n",
    "        \n",
    "        x = t_b[:, 0: context_length]\n",
    "        y = t_b[:, context_length: context_length+1]\n",
    "        \n",
    "        t_b = get_batch(train, context_length+1, batch_size)\n",
    "\n",
    "        x = t_b[:, 0:context_length]\n",
    "        y = t_b[:, 1:context_length+1]\n",
    "\n",
    "        logits, batch_loss = model(x, y)\n",
    "        \n",
    "        total_loss = total_loss + batch_loss\n",
    "    \n",
    "    print(\"total loss\", total_loss, total_loss / num_batches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f0d85cd-8cd9-403d-a685-6fc24aa73e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3c2d3c52-0c8a-4ab6-8121-7e8377d361d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "24091  parameters\n",
      "ep 0 tensor(2.6127, grad_fn=<DivBackward0>) [0.098]\n",
      "ep 5 tensor(1.9511, grad_fn=<DivBackward0>) [0.0885842380864]\n",
      "ep 10 tensor(1.8820, grad_fn=<DivBackward0>) [0.08007313507497958]\n",
      "ep 15 tensor(1.8487, grad_fn=<DivBackward0>) [0.07237977205924956]\n",
      "ep 20 tensor(1.8319, grad_fn=<DivBackward0>) [0.06542558123199924]\n",
      "ep 25 tensor(1.8175, grad_fn=<DivBackward0>) [0.059139543518331866]\n",
      "ep 30 tensor(1.8050, grad_fn=<DivBackward0>) [0.053457463299478813]\n",
      "ep 35 tensor(1.7948, grad_fn=<DivBackward0>) [0.048321312820571644]\n",
      "ep 40 tensor(1.7892, grad_fn=<DivBackward0>) [0.04367863958719317]\n",
      "ep 45 tensor(1.7837, grad_fn=<DivBackward0>) [0.03948203069879567]\n",
      "ep 50 tensor(1.7784, grad_fn=<DivBackward0>) [0.03568862864853744]\n",
      "ep 55 tensor(1.7723, grad_fn=<DivBackward0>) [0.03225969364468526]\n"
     ]
    }
   ],
   "source": [
    "epochs = 60\n",
    "training_runs = 800\n",
    "batch_size = 96\n",
    "context_length = 12\n",
    "learning_rate = .1\n",
    "embedding_dimensions = 32\n",
    "num_heads = 4\n",
    "head_size = embedding_dimensions // num_heads\n",
    "\n",
    "print(head_size)\n",
    "# our embedding_dimensions are still 'small' so we mutliply the size our our feed forward network to make up\n",
    "multiplier = 8\n",
    "model = FFMultiHeadAttention(embedding_dimensions, context_length, num_heads, head_size, multiplier)\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "\n",
    "print(sum(p.numel() for p in model.parameters()), ' parameters')\n",
    "\n",
    "lmbda = lambda epoch: 0.98\n",
    "\n",
    "m_scheduler = optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)\n",
    "\n",
    "for ep in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    for tr in range(training_runs):\n",
    "        t_b = get_batch(train, context_length+1, batch_size)\n",
    "\n",
    "        x = t_b[:, 0:context_length]\n",
    "        y = t_b[:, 1:context_length+1]\n",
    "\n",
    "        logits, loss = model(x, y)\n",
    "\n",
    "        epoch_loss += loss\n",
    "        \n",
    "        model.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    m_scheduler.step()\n",
    "    \n",
    "    if ep % 5 == 0:\n",
    "        print(\"ep\", ep, epoch_loss/training_runs, m_scheduler.get_last_lr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f219b8c9-2430-4b93-a0a0-4b015bd0ce1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_batches 89500 1790\n",
      "total loss tensor(3164.0281) tensor(1.7676)\n",
      "None 89500\n",
      "\n",
      "Tne, to I?\n",
      "ROMEO:\n",
      "The Go, shall will laDit shumsand part, may, this I, \n",
      "as men, Wellievenrates Xinte\n"
     ]
    }
   ],
   "source": [
    "dev = torch.tensor(br.data[1])\n",
    "print(split_loss(dev), len(dev))\n",
    "\n",
    "idx = torch.zeros((1, 1), dtype=torch.int)\n",
    "for i in range(1):\n",
    "    o = model.generate(idx, 100).data[0].tolist()\n",
    "    print(br.decode(o))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aa496f61-20fd-495e-9053-7358877c2359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LOd like so,\n",
      "But admother? but in s I shable, not so theire them lingood s outct  yover more \n",
      "Fildsir, illower a men, thy servance\n",
      "Well and in Anne toom your night unter\n",
      "Nayer\n",
      "For your ever I wear all name \n",
      "GRUMIO:\n",
      "Hest \n",
      "PRINCE:\n",
      "First the Slack theirtue your named, goo \n",
      "POLIXCALUS:\n",
      "Shall boy spenthereforet go poor:\n",
      "Dut aime\n",
      "M dange \n",
      "More for yiely be dow: I sambend were fare too trust oneed kinsmiled of fathin that propose vain  Geed by with wifer Tybalt force speak he disitter else\n",
      "ward:\n",
      "ISABELLK:\n",
      "For the noble me of you face Post theice, whis he furquest is o with the held: of  Where \n",
      "AUS:\n",
      "Even tonglying sonst grave unp as tyes bere her say shumble ward! this mine well maince, and ale in thatess I reveds\n",
      "mean:\n",
      "And his you she him then, let it all backeelse againnefirectsers fiendent of beam Malul so  Breatents a greit,\n",
      "My my Hrading your broth\n",
      "or home  colds!\n",
      "GLOUCESTER:\n",
      "Let pitessengind, genture:\n",
      "No most the they where yetroush honour ree must I hate hound the more dower gleft you s\n",
      "\n",
      "And in \n",
      "CORIOLA:\n",
      "O blowd with the was thee theison masters, all titter askings awful he withse dow hye body for think Hence, and yet not this, Bolds \n",
      "God of \n",
      "GLOUCESTER:\n",
      "Sayely me a how purrew their sir help alow to at there godessecrownince word, frum s wongugh?\n",
      "So perserous and True scarenementating yone a more \n",
      "New thereform\n",
      "Shilet\n",
      "Shepatuns, win say, minessireted on in by:\n",
      "Lords amore cheard aboute sayet ginstay purition deveest:\n",
      "Or time, by mine astaring the worn devisa son,\n",
      "Uvengue our warrong of the are king, alt:\n",
      "Would get you have well \n",
      "Ney s in d anal her  whed deney you not a velse,\n",
      "What holessengely can of your my her   CLIFF:\n",
      "3 Katesly?\n",
      "POMPEY:\n",
      "Go, betwents himpenir d the\n",
      "are nown, but the with care?\n",
      "Which  yet are are\n",
      "Is purch feed\n",
      "Do O, Citil, fives, be gentized o the be\n",
      "Lart,\n",
      "Staves\n",
      "Have \n",
      "Nay men I tizen:\n",
      "By thense this Pit o it thee, sirest compheds\n",
      "hall \n",
      "Hinst he well\n",
      "To despare the\n",
      "And for command life eakes loote live as have, and go  torcible arming \n",
      "KING RICHARD I\n"
     ]
    }
   ],
   "source": [
    "idx = torch.zeros((1, 1), dtype=torch.int)\n",
    "for i in range(2):\n",
    "    o = model.generate(idx, 1000).data[0].tolist()\n",
    "    print(br.decode(o))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "51dc4444-34c5-4400-9b4c-a181115dd502",
   "metadata": {},
   "outputs": [],
   "source": [
    "e_epochs = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "357ed651-43b4-4426-ba9a-ee8de8fd8828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 0 tensor(1.7653, grad_fn=<DivBackward0>) [0.029160207983827797]\n",
      "ep 2 tensor(1.7666, grad_fn=<DivBackward0>) [0.028005463747668217]\n",
      "ep 4 tensor(1.7636, grad_fn=<DivBackward0>) [0.026896447383260556]\n",
      "ep 6 tensor(1.7595, grad_fn=<DivBackward0>) [0.025831348066883437]\n",
      "ep 8 tensor(1.7598, grad_fn=<DivBackward0>) [0.024808426683434852]\n",
      "ep 10 tensor(1.7575, grad_fn=<DivBackward0>) [0.023826012986770832]\n",
      "ep 12 tensor(1.7574, grad_fn=<DivBackward0>) [0.022882502872494704]\n",
      "ep 14 tensor(1.7533, grad_fn=<DivBackward0>) [0.02197635575874391]\n",
      "ep 16 tensor(1.7510, grad_fn=<DivBackward0>) [0.021106092070697653]\n",
      "ep 18 tensor(1.7510, grad_fn=<DivBackward0>) [0.020270290824698025]\n",
      "ep 20 tensor(1.7536, grad_fn=<DivBackward0>) [0.019467587308039984]\n",
      "ep 22 tensor(1.7524, grad_fn=<DivBackward0>) [0.0186966708506416]\n",
      "ep 24 tensor(1.7520, grad_fn=<DivBackward0>) [0.017956282684956193]\n",
      "ep 26 tensor(1.7512, grad_fn=<DivBackward0>) [0.017245213890631928]\n",
      "ep 28 tensor(1.7484, grad_fn=<DivBackward0>) [0.016562303420562904]\n",
      "ep 30 tensor(1.7462, grad_fn=<DivBackward0>) [0.01590643620510861]\n",
      "ep 32 tensor(1.7480, grad_fn=<DivBackward0>) [0.015276541331386308]\n",
      "ep 34 tensor(1.7472, grad_fn=<DivBackward0>) [0.01467159029466341]\n",
      "ep 36 tensor(1.7434, grad_fn=<DivBackward0>) [0.014090595318994738]\n",
      "ep 38 tensor(1.7444, grad_fn=<DivBackward0>) [0.013532607744362546]\n",
      "ep 40 tensor(1.7398, grad_fn=<DivBackward0>) [0.012996716477685789]\n",
      "ep 42 tensor(1.7415, grad_fn=<DivBackward0>) [0.01248204650516943]\n",
      "ep 44 tensor(1.7377, grad_fn=<DivBackward0>) [0.011987757463564721]\n",
      "ep 46 tensor(1.7364, grad_fn=<DivBackward0>) [0.011513042268007558]\n",
      "ep 48 tensor(1.7388, grad_fn=<DivBackward0>) [0.01105712579419446]\n",
      "ep 50 tensor(1.7369, grad_fn=<DivBackward0>) [0.010619263612744357]\n",
      "ep 52 tensor(1.7369, grad_fn=<DivBackward0>) [0.01019874077367968]\n",
      "ep 54 tensor(1.7344, grad_fn=<DivBackward0>) [0.009794870639041964]\n",
      "ep 56 tensor(1.7359, grad_fn=<DivBackward0>) [0.009406993761735902]\n",
      "ep 58 tensor(1.7319, grad_fn=<DivBackward0>) [0.00903447680877116]\n"
     ]
    }
   ],
   "source": [
    "for ep in range(e_epochs):\n",
    "    epoch_loss = 0\n",
    "    for tr in range(training_runs):\n",
    "        t_b = get_batch(train, context_length+1, batch_size)\n",
    "\n",
    "        x = t_b[:, 0:context_length]\n",
    "        y = t_b[:, 1:context_length+1]\n",
    "\n",
    "        logits, loss = model(x, y)\n",
    "\n",
    "        epoch_loss += loss\n",
    "        \n",
    "        model.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    m_scheduler.step()\n",
    "    \n",
    "    if ep % 2 == 0:\n",
    "        print(\"ep\", ep, epoch_loss/training_runs, m_scheduler.get_last_lr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8c7133f0-0e6a-4f48-8789-4515173224ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"joined_head_shake_checkpoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5e1a77c5-d50c-4a7e-b5a9-0044e9e25344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_batches 89500 1790\n",
      "total loss tensor(3102.5913) tensor(1.7333)\n",
      "None 89500\n",
      "\n",
      "Bution a s from outhrefriending,\n",
      "Good incle shooding put,\n",
      "Towoman,\n",
      "To \n",
      "York, gone are with swords th\n"
     ]
    }
   ],
   "source": [
    "dev = torch.tensor(br.data[1])\n",
    "print(split_loss(dev), len(dev))\n",
    "\n",
    "idx = torch.zeros((1, 1), dtype=torch.int)\n",
    "for i in range(1):\n",
    "    o = model.generate(idx, 100).data[0].tolist()\n",
    "    print(br.decode(o))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f9c913f9-ea98-4cfa-b054-928dc89add2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hank fear hither:\n",
      "Ands quotters long be noble in that day, why lord maince, whith world\n",
      "Mestle to thee \n",
      "And \n",
      "And \n",
      "What word my mothers, appeon our my my gone,\n",
      "How dearns, as brighbornly son groon it, and seelman \n",
      "DROMEO:\n",
      "There, and Horture mementle acth everying him and are husbanishally!\n",
      "LUCENTIO:\n",
      "No flace nove s \n",
      "To him, that hame to who not disgmil and know tell was man hearts mand vaint, well buy die the Morriefi,\n",
      "Unhal?\n",
      "SPERDONE:\n",
      "Off you vower mensting thou Romeost morture we crroneather\n",
      "Show shood my \n",
      "VOLUE OF YORK:\n",
      "Mordon a a bettereour hastingman, d sidlence bread full as holdit the miscatches away hour the commannius in, Buck is them offecteosed to light your \n",
      "PAULINA:\n",
      "All pies for of me, jois, by requen?\n",
      "FATHASTINGS:\n",
      "You by dricks ther\n",
      "Show pleat \n",
      "Unhed, thanks talk remost no, we \n",
      "LADY CAPULET:\n",
      "Strants befact some you desserved by shall face?\n",
      "Sould death Lance! she \n",
      "Corians\n",
      "Your you mallanators,\n",
      "A the close, a noblands\n",
      "Tell with very not fear,\n",
      "I would suchal this boldius so h\n"
     ]
    }
   ],
   "source": [
    "idx = torch.zeros((1, 1), dtype=torch.int)\n",
    "for i in range(1):\n",
    "    o = model.generate(idx, 1000).data[0].tolist()\n",
    "    print(br.decode(o))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4aba59ed-d38a-4ab4-b12f-0aad61a2355f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 0 tensor(1.7341, grad_fn=<DivBackward0>) [0.008676711527143824]\n",
      "ep 2 tensor(1.7351, grad_fn=<DivBackward0>) [0.008333113750668928]\n",
      "ep 4 tensor(1.7354, grad_fn=<DivBackward0>) [0.008003122446142439]\n",
      "ep 6 tensor(1.7313, grad_fn=<DivBackward0>) [0.007686198797275197]\n",
      "ep 8 tensor(1.7313, grad_fn=<DivBackward0>) [0.007381825324903099]\n",
      "ep 10 tensor(1.7318, grad_fn=<DivBackward0>) [0.007089505042036937]\n",
      "ep 12 tensor(1.7306, grad_fn=<DivBackward0>) [0.006808760642372274]\n",
      "ep 14 tensor(1.7320, grad_fn=<DivBackward0>) [0.006539133720934331]\n",
      "ep 16 tensor(1.7302, grad_fn=<DivBackward0>) [0.006280184025585331]\n",
      "ep 18 tensor(1.7282, grad_fn=<DivBackward0>) [0.006031488738172152]\n",
      "ep 20 tensor(1.7297, grad_fn=<DivBackward0>) [0.005792641784140534]\n",
      "ep 22 tensor(1.7281, grad_fn=<DivBackward0>) [0.005563253169488569]\n",
      "ep 24 tensor(1.7274, grad_fn=<DivBackward0>) [0.005342948343976821]\n",
      "ep 26 tensor(1.7271, grad_fn=<DivBackward0>) [0.005131367589555339]\n",
      "ep 28 tensor(1.7274, grad_fn=<DivBackward0>) [0.004928165433008947]\n",
      "ep 30 tensor(1.7276, grad_fn=<DivBackward0>) [0.004733010081861793]\n",
      "ep 32 tensor(1.7295, grad_fn=<DivBackward0>) [0.004545582882620065]\n",
      "ep 34 tensor(1.7281, grad_fn=<DivBackward0>) [0.00436557780046831]\n",
      "ep 36 tensor(1.7217, grad_fn=<DivBackward0>) [0.004192700919569765]\n",
      "ep 38 tensor(1.7246, grad_fn=<DivBackward0>) [0.004026669963154802]\n",
      "ep 40 tensor(1.7244, grad_fn=<DivBackward0>) [0.003867213832613871]\n",
      "ep 42 tensor(1.7246, grad_fn=<DivBackward0>) [0.0037140721648423617]\n",
      "ep 44 tensor(1.7230, grad_fn=<DivBackward0>) [0.003566994907114604]\n",
      "ep 46 tensor(1.7250, grad_fn=<DivBackward0>) [0.0034257419087928656]\n",
      "ep 48 tensor(1.7221, grad_fn=<DivBackward0>) [0.003290082529204668]\n",
      "ep 50 tensor(1.7231, grad_fn=<DivBackward0>) [0.003159795261048163]\n",
      "ep 52 tensor(1.7239, grad_fn=<DivBackward0>) [0.0030346673687106558]\n",
      "ep 54 tensor(1.7196, grad_fn=<DivBackward0>) [0.0029144945409097134]\n",
      "ep 56 tensor(1.7211, grad_fn=<DivBackward0>) [0.0027990805570896884]\n",
      "ep 58 tensor(1.7233, grad_fn=<DivBackward0>) [0.0026882369670289366]\n"
     ]
    }
   ],
   "source": [
    "for ep in range(e_epochs):\n",
    "    epoch_loss = 0\n",
    "    for tr in range(training_runs):\n",
    "        t_b = get_batch(train, context_length+1, batch_size)\n",
    "\n",
    "        x = t_b[:, 0:context_length]\n",
    "        y = t_b[:, 1:context_length+1]\n",
    "\n",
    "        logits, loss = model(x, y)\n",
    "\n",
    "        epoch_loss += loss\n",
    "        \n",
    "        model.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    m_scheduler.step()\n",
    "    \n",
    "    if ep % 2 == 0:\n",
    "        print(\"ep\", ep, epoch_loss/training_runs, m_scheduler.get_last_lr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b306b520-3745-4f9b-8105-80ebcc8df1b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_batches 89500 1790\n",
      "total loss tensor(3084.2842) tensor(1.7231)\n",
      "None 89500\n"
     ]
    }
   ],
   "source": [
    "dev = torch.tensor(br.data[1])\n",
    "print(split_loss(dev), len(dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4dcb202e-a15c-425f-a21f-1907d782efb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "But be \n",
      "SOMERSET:\n",
      "Like is those no  cut dence:\n",
      "But his dest sin, and but Dence be him kings \n",
      "Percepupon \n",
      "If yet mean:\n",
      "Anfry with upplain Lord and here one justion\n",
      "In take you all free, sir forwarl so\n",
      "first:\n",
      "I must  hits with oneen Berfolzen:\n",
      "Why bore \n",
      "HORTENSIO:\n",
      "Fere off I but \n",
      "CORIOLANUS:\n",
      "Maner s, yet was in him, gentlementy be roor \n",
      "Pelosomes A\n",
      "sure unhoners naves town, yet himseless and  which hence,\n",
      "A Tucripe her to theress true with, I am my you masters evenged swelves had a heart, ange have ricking \n",
      "PROSS:\n",
      "Secoran\n",
      "Did for sit flament must of get, wife, and Pholes of an unbound set no mistic, a beceing, lasul tter puts in I cI:\n",
      "Awby than power thy yethor retters, stor the one, sir, and thummade will  \n",
      "NORTENSIO:\n",
      "You coung then I,\n",
      "Is chargour at the due mine beggainal meth: the that mousand my Romes prining?\n",
      "With child now myself us, lord,\n",
      "TRANIO:\n",
      "Persely deep to boy then his this what stanger thrights:\n",
      "It they: Rangers? should by sent offerer canne thy broyals,\n",
      "Aftus, sirectides? \n"
     ]
    }
   ],
   "source": [
    "idx = torch.zeros((1, 1), dtype=torch.int)\n",
    "for i in range(1):\n",
    "    o = model.generate(idx, 1000).data[0].tolist()\n",
    "    print(br.decode(o))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
