{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "623a40b8-15f3-474b-b2d8-a2e8a4559672",
   "metadata": {},
   "source": [
    "### ok lets play with our new ideas\n",
    "\n",
    "we could add a dimension, change the layers etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1998a4ba-4781-49c6-93e0-cde038779ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e4674a-a0f9-4d98-bc5e-3854107eeedb",
   "metadata": {},
   "source": [
    "lets add another layer - we should still be able to plot it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10e407ab-2f2f-4cfe-908f-8a49d0a50786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# our embedding space\n",
    "dims = 3\n",
    "context_length = 5\n",
    "vocab_size = 3\n",
    "es = torch.randn((vocab_size, dims))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "334f6638-8720-4bbb-ae02-fe9306bc7f9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 3]), tensor(-0.0184), tensor([ 0.3639,  0.3935, -2.1978]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show the embedding space an the 'a' co-ordinates (Transpose es just to save display space)\n",
    "es.shape, es[0,0], es[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21bc0bff-e304-42ec-b410-a925d051a7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"names.txt\", \"r\") as r:\n",
    "    names = [\".\" + f + \".\" for f in r.read().split()]\n",
    "\n",
    "import string\n",
    "letters = [l for l in string.ascii_lowercase]\n",
    "\n",
    "itos = {0: \".\"}\n",
    "stoi = {\".\": 0}\n",
    "\n",
    "for i, l in enumerate(letters):\n",
    "    offset = i+1\n",
    "    stoi[l] = offset\n",
    "    itos[offset] = l\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbbff759-28b1-4bd0-8b1e-4917c8d06ac3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 32033)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stoi['a'], len(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4c40a23-9089-409a-9d52-5b5fde7b2f5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.haze.', '.rufino.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "from random import randrange\n",
    "\n",
    "names_length = len(names)\n",
    "\n",
    "def sample_names(size=5):\n",
    "    batch_names = []\n",
    "    for i in range(size):\n",
    "        ni = randrange(names_length-1)\n",
    "        name = names[ni]\n",
    "        batch_names.append(name)\n",
    "    return batch_names\n",
    "\n",
    "sample_names(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6bbdd29d-0ba0-4ded-9f52-d040f79626ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.....', '....t', '...ti', '..tim', '.timo', 'timot', 'imoth', 'mothy']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def word_contexts(word):\n",
    "    samples = []\n",
    "    max_length = len(word)\n",
    "    fill = '.' * context_length\n",
    "    for i in range(1,max_length):\n",
    "        st = max(0, i-context_length)\n",
    "        filled = fill[i:] + word[st:i]\n",
    "        samples.append(filled[:context_length])\n",
    "    return samples\n",
    "word_contexts(\".timothy.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81c38c1d-0dba-4cdf-b5f0-08ea67a88487",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_xys(samples):\n",
    "    xs, ys = [], []\n",
    "    for s in samples:\n",
    "        for ctx in word_contexts(s):\n",
    "            x =  [stoi[c] for c in ctx]\n",
    "            xs.append(x)\n",
    "        y = [stoi[c] for c in s[1:]]\n",
    "        ys += y\n",
    "\n",
    "    return xs, ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29357dc4-4142-4a57-bafa-752041e13881",
   "metadata": {},
   "outputs": [],
   "source": [
    "## better sample with train/dev/test split\n",
    "import math\n",
    "# shuffle names\n",
    "from random import shuffle\n",
    "\n",
    "shuffle(names)\n",
    "\n",
    "x_names, y_names = get_xys(names)\n",
    "samples_length = len(y_names)\n",
    "train_length = math.floor(samples_length * .8)\n",
    "dev_offset = math.floor(samples_length * .9)\n",
    "\n",
    "dev = list(zip(x_names[train_length: dev_offset], y_names[train_length: dev_offset]))\n",
    "test = list(zip(x_names[dev_offset:], y_names[dev_offset:]))\n",
    "\n",
    "train = list(zip(x_names[:train_length], y_names[:train_length]))\n",
    "\n",
    "def sample_names(sample_size = 5, p = train):\n",
    "    samples = random.sample(p, sample_size)\n",
    "    x_samples = [s[0] for s in samples]\n",
    "    y_samples = [s[1] for s in samples]\n",
    "    return x_samples, y_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dfa164d5-5c9d-4136-8dbd-d54fe13cef14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([2, 18, 15, 14, 23], 25)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs, ys = sample_names()\n",
    "\n",
    "xs[0], ys[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79fa1191-27b6-4f65-b48a-114d1b1ce261",
   "metadata": {},
   "source": [
    "## OK lets run this and see what happens..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7cd98882-38fe-49bb-a461-033d0c4256ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layer_size = 160\n",
    "dims = 3\n",
    "\n",
    "es = torch.randn((vocab_size, dims))\n",
    "\n",
    "es = torch.randn((27, dims), requires_grad=True)\n",
    "W1 = torch.randn((dims*context_length, hidden_layer_size), requires_grad=True)\n",
    "b1 = torch.randn(hidden_layer_size, requires_grad=True)\n",
    "W2 = torch.randn((hidden_layer_size, 27), requires_grad=True)\n",
    "b2 = torch.randn(27, requires_grad=True)\n",
    "\n",
    "parameters = [es, W1, b1, W2, b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88c86409-6c7a-43b1-ab79-f74c43964fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "names_length = len(names)\n",
    "epochs = 1\n",
    "batch_size = 400\n",
    "learning_rate = .1\n",
    "sample_loops = 200 #in future we want our epoch to roughly sample everything - names_length / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9035d60c-d95d-4871-88b8-3ffdcfa061c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.5874, grad_fn=<DivBackward0>)\n",
      "0 0.095\n"
     ]
    }
   ],
   "source": [
    "for ep in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    for s in range(sample_loops):\n",
    "        x, y = sample_names(batch_size)\n",
    "        X = torch.tensor(x)\n",
    "        Y = torch.tensor(y)\n",
    "        train = es[X.view(-1)]\n",
    "        outputL1 = torch.relu(train.view(-1, dims * context_length) @ W1 + b1)\n",
    "        \n",
    "        logits = outputL1 @ W2 + b2\n",
    "\n",
    "        loss = F.cross_entropy(logits, Y)\n",
    "\n",
    "        epoch_loss += loss\n",
    "\n",
    "        for p in parameters:\n",
    "            p.grad = None\n",
    "        \n",
    "        loss.backward()\n",
    "\n",
    "        for p in parameters:\n",
    "            p.data -= learning_rate * p.grad\n",
    "\n",
    "    if ep == 61:\n",
    "        learning_rate = .01\n",
    "\n",
    "    if ep % 10 == 0:\n",
    "        print(epoch_loss/sample_loops)\n",
    "        learning_rate *= .95\n",
    "        print(ep, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6466eaa0-86ee-48fa-b805-9a3bb3fa545d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cvdikamnse\n"
     ]
    }
   ],
   "source": [
    "def generate_names(num_names):\n",
    "    for i in range(num_names):\n",
    "        out = []\n",
    "        ix = [0, 0, 0, 0, 0]\n",
    "        for nl in range(10):\n",
    "            xenc = es[ix]\n",
    "\n",
    "            outputL1 = torch.relu(xenc.view(-1, dims * context_length) @ W1 + b1)\n",
    "            logits = outputL1 @ W2 + b2\n",
    "            \n",
    "            p = F.softmax(logits, dim=1)\n",
    "    \n",
    "            # ## torch.multinomial pulls out an index in p (num_samples=1) by sampling from the elements in p according to their probabilities \n",
    "            # ## (p is normalized in the softmax above)\n",
    "            prediction = torch.multinomial(p, num_samples=1).item()\n",
    "            \n",
    "            ix = [ix[1], ix[2], ix[3], ix[4], prediction]\n",
    "\n",
    "            if prediction == 0:\n",
    "                break\n",
    "            out.append(itos[prediction])\n",
    "            \n",
    "        print(\"\".join(out))\n",
    "\n",
    "generate_names(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "964cdce5-effd-45a8-be31-2a37ad9ac7c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deelni\n",
      "iacdieod\n",
      "raytyeneah\n",
      "caibunntik\n",
      "cabnacnsan\n",
      "wbbrghnh\n",
      "naivu\n",
      "boodeaaasi\n",
      "jaras\n",
      "iahans\n",
      "wayiaidnan\n",
      "welie\n",
      "badar\n",
      "cabdnissgi\n",
      "khltijalen\n"
     ]
    }
   ],
   "source": [
    "generate_names(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "53d9d7a8-9d72-4161-a2c0-f96025b1f11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c9ee12d6-6c1b-4ac3-a2b8-83ffa27607d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "647ee7073ed84fd79ab2b9c25ff7cb49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=23.0, description='elev', max=45.0, min=1.0), FloatSlider(value=35.0, …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.interactive_plot(elev, azim, zoom)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = es[1:].T[0].tolist()\n",
    "y = es[1:].T[1].tolist()\n",
    "z = es[1:].T[2].tolist()\n",
    "n = letters\n",
    "\n",
    "def interactive_plot(elev, azim, zoom):\n",
    "    fig = plt.figure()\n",
    "    ax = plt.axes(projection='3d')\n",
    "\n",
    "    ax.view_init(elev, azim)\n",
    "    ax.set_box_aspect((1, 1, 1), zoom=zoom)\n",
    "    \n",
    "    ax.scatter(x, y, z)\n",
    "    \n",
    "    for i, txt in enumerate(n):\n",
    "        # ax.annotate(txt, (x[i], y[i], z[i]))\n",
    "        ax.text(x[i], y[i], z[i], txt, color='blue')\n",
    "\n",
    "\n",
    "interact(interactive_plot, elev=(1, 45, 0.1), azim=(1, 70, 0.1), zoom=(0, 4, 0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "2a9d3cb0-6d93-406e-8c23-4bf12428e045",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85148c01a00b41d7a15b854108f0ba38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=23.0, description='elev', max=45.0, min=1.0), FloatSlider(value=35.0, …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.interactive_plot(elev, azim, zoom)>"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = es[1:].T[0].tolist()\n",
    "y = es[1:].T[1].tolist()\n",
    "z = es[1:].T[2].tolist()\n",
    "n = letters\n",
    "\n",
    "def interactive_plot(elev, azim, zoom):\n",
    "    fig = plt.figure()\n",
    "    ax = plt.axes(projection='3d')\n",
    "\n",
    "    ax.view_init(elev, azim)\n",
    "    ax.set_box_aspect((1, 1, 1), zoom=zoom)\n",
    "    \n",
    "    ax.scatter(x, y, z)\n",
    "    \n",
    "    for i, txt in enumerate(n):\n",
    "        # ax.annotate(txt, (x[i], y[i], z[i]))\n",
    "        ax.text(x[i], y[i], z[i], txt, color='blue')\n",
    "\n",
    "\n",
    "interact(interactive_plot, elev=(1, 45, 0.1), azim=(1, 70, 0.1), zoom=(0, 4, 0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51534358-c812-4ee4-8415-5fdf388047e1",
   "metadata": {},
   "source": [
    "## Ok even more dimensions \n",
    "\n",
    "we'll lose our visualizations, but we can come back to that "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a56ee5be-d09c-49cb-a48a-dc926d43a460",
   "metadata": {},
   "outputs": [],
   "source": [
    "dimsb = 7\n",
    "context_length = 5\n",
    "nonlin='relu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "551f7947-0daf-4485-99e9-de95cbe90e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layer_size = 200\n",
    "\n",
    "esb = torch.randn((27, dimsb))\n",
    "W1b = torch.randn((dimsb*context_length, hidden_layer_size))\n",
    "\n",
    "nn.init.kaiming_normal_(W1b, nonlinearity=nonlin)\n",
    "\n",
    "b1b = torch.randn(hidden_layer_size) * 0.001\n",
    "\n",
    "# bnb = nn.BatchNorm2d(hidden_layer_size)\n",
    "\n",
    "W2b = torch.randn((hidden_layer_size, 27)) / (dimsb*context_length)**0.5\n",
    "b2b = torch.randn(27) * 0.001\n",
    "\n",
    "parametersb = [esb, W1b, b1b, W2b, b2b]\n",
    "for p in parametersb:\n",
    "    p.requires_grad=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b0c19104-8602-41cb-ba96-55df6585f80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "names_length = len(names)\n",
    "epochs = 200\n",
    "batch_size = 820\n",
    "learning_rate = .1\n",
    "sample_loops = 800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8640461e-3057-4717-b65d-e36f1139195b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([27, 7])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "esb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "83f2032f-a695-4a03-933f-7a77cb6ee33c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 5, 7])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = sample_names(32)\n",
    "batched = esb[x]\n",
    "batched.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e12f8c5e-7677-4504-bc01-7dfd884963ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.4201, grad_fn=<DivBackward0>)\n",
      "0 0.095\n",
      "tensor(2.0981, grad_fn=<DivBackward0>)\n",
      "10 0.09025\n",
      "tensor(2.0543, grad_fn=<DivBackward0>)\n",
      "20 0.0857375\n",
      "tensor(2.0300, grad_fn=<DivBackward0>)\n",
      "30 0.08145062499999998\n",
      "tensor(2.0161, grad_fn=<DivBackward0>)\n",
      "40 0.07737809374999999\n",
      "tensor(2.0064, grad_fn=<DivBackward0>)\n",
      "50 0.07350918906249998\n",
      "tensor(1.9963, grad_fn=<DivBackward0>)\n",
      "60 0.06983372960937498\n",
      "tensor(1.9919, grad_fn=<DivBackward0>)\n",
      "70 0.06634204312890622\n",
      "tensor(1.9873, grad_fn=<DivBackward0>)\n",
      "80 0.0630249409724609\n",
      "tensor(1.9849, grad_fn=<DivBackward0>)\n",
      "90 0.05987369392383786\n",
      "tensor(1.9758, grad_fn=<DivBackward0>)\n",
      "100 0.05688000922764597\n",
      "tensor(1.9773, grad_fn=<DivBackward0>)\n",
      "110 0.05403600876626367\n",
      "tensor(1.9685, grad_fn=<DivBackward0>)\n",
      "120 0.0095\n",
      "tensor(1.9681, grad_fn=<DivBackward0>)\n",
      "130 0.009025\n",
      "tensor(1.9684, grad_fn=<DivBackward0>)\n",
      "140 0.00857375\n",
      "tensor(1.9671, grad_fn=<DivBackward0>)\n",
      "150 0.0081450625\n",
      "tensor(1.9643, grad_fn=<DivBackward0>)\n",
      "160 0.007737809374999999\n",
      "tensor(1.9628, grad_fn=<DivBackward0>)\n",
      "170 0.00095\n",
      "tensor(1.9642, grad_fn=<DivBackward0>)\n",
      "180 0.0009025\n",
      "tensor(1.9639, grad_fn=<DivBackward0>)\n",
      "190 0.000857375\n"
     ]
    }
   ],
   "source": [
    "for ep in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    for s in range(sample_loops):\n",
    "        x, y = sample_names(batch_size)\n",
    "        Y = torch.tensor(y)\n",
    "        train = esb[x]\n",
    "\n",
    "        preact = train.view(-1, dimsb * context_length) @ W1b + b1b\n",
    "        # add learnable normalization?\n",
    "        # preact = (preact - preact.mean(0, keepdim=True))/preact.std(0, keepdim=True)\n",
    "        \n",
    "        if nonlin == 'relu':\n",
    "            outputL1 = torch.relu(preact)\n",
    "        else:\n",
    "            outputL1 = torch.tanh(preact)\n",
    "        \n",
    "        logits = outputL1 @ W2b + b2b\n",
    "\n",
    "        lossb = F.cross_entropy(logits, Y)\n",
    "\n",
    "        epoch_loss += lossb\n",
    "\n",
    "        for p in parametersb:\n",
    "            p.grad = None\n",
    "        \n",
    "        lossb.backward()\n",
    "\n",
    "        for p in parametersb:\n",
    "            p.data -= learning_rate * p.grad\n",
    "\n",
    "    if ep == 111:\n",
    "        learning_rate = .01\n",
    "\n",
    "    if ep == 161:\n",
    "        learning_rate = .001\n",
    "\n",
    "    if ep % 10 == 0:\n",
    "        print(epoch_loss/sample_loops)\n",
    "        learning_rate *= .95\n",
    "        print(ep, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "532d3ba5-b3c1-4a69-8d52-9c18a8a41e71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([40, 200])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(outputL1[:40]==0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0fbfed09-8a8e-4f6c-a507-78294ba7bed9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.0639, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_x = [s[0] for s in dev]\n",
    "dev_y = torch.tensor([s[1] for s in dev])\n",
    "val = esb[dev_x]\n",
    "val1 = torch.relu(val.view(-1, dimsb * context_length) @ W1b + b1b)\n",
    "\n",
    "val_logits = val1 @ W2b + b2b\n",
    "\n",
    "val_loss = F.cross_entropy(val_logits, dev_y)\n",
    "val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ea4072-2b77-4159-b34f-f092e19ee575",
   "metadata": {},
   "source": [
    "some train/val losses\n",
    "\n",
    "1.9696 / 2.0717\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "572b42da-17d3-4d64-900f-f90769b432c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "daraya\n"
     ]
    }
   ],
   "source": [
    "def generate_namesb(num_names):\n",
    "    for i in range(num_names):\n",
    "        out = []\n",
    "        ix = [0, 0, 0, 0, 0]\n",
    "        for nl in range(10):\n",
    "            xenc = esb[ix]\n",
    "\n",
    "            outputL1 = torch.relu(xenc.view(-1, dimsb * context_length) @ W1b + b1b)\n",
    "            logits = outputL1 @ W2b + b2b\n",
    "            \n",
    "            p = F.softmax(logits, dim=1)\n",
    "    \n",
    "            # ## torch.multinomial pulls out an index in p (num_samples=1) by sampling from the elements in p according to their probabilities \n",
    "            # ## (p is normalized in the softmax above)\n",
    "            prediction = torch.multinomial(p, num_samples=1).item()\n",
    "            \n",
    "            ix = [ix[1], ix[2], ix[3], ix[4], prediction]\n",
    "\n",
    "            if prediction == 0:\n",
    "                break\n",
    "            out.append(itos[prediction])\n",
    "            \n",
    "        print(\"\".join(out))\n",
    "\n",
    "generate_namesb(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "934fcef7-3601-4865-8863-40e00f865eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kolamaraes\n",
      "myliea\n",
      "skom\n",
      "rozaluna\n",
      "alfeeir\n",
      "kalmeem\n",
      "rick\n",
      "jais\n",
      "ingumadl\n",
      "demontee\n",
      "chryden\n",
      "zulah\n",
      "eaydeniel\n",
      "kilor\n",
      "rina\n"
     ]
    }
   ],
   "source": [
    "generate_namesb(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "5a78922f-9c12-49a3-a587-25436913fe79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4a1a5b77bd84d5bae4410722d91dd0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=23.0, description='elev', max=45.0, min=1.0), FloatSlider(value=35.0, …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.interactive_plot(elev, azim, zoom, dim1, dim2, dim3)>"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = letters\n",
    "\n",
    "def interactive_plot(elev, azim, zoom, dim1, dim2, dim3):\n",
    "\n",
    "    x = esb[1:].T[dim1].tolist()\n",
    "    y = esb[1:].T[dim2].tolist()\n",
    "    z = esb[1:].T[dim3].tolist()\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    ax = plt.axes(projection='3d')\n",
    "\n",
    "    ax.view_init(elev, azim)\n",
    "    ax.set_box_aspect((1, 1, 1), zoom=zoom)\n",
    "    \n",
    "    ax.scatter(x, y, z)\n",
    "    \n",
    "    for i, txt in enumerate(n):\n",
    "        # ax.annotate(txt, (x[i], y[i], z[i]))\n",
    "        ax.text(x[i], y[i], z[i], txt, color='blue')\n",
    "\n",
    "\n",
    "interact(interactive_plot, elev=(1, 45, 0.1), azim=(1, 70, 0.1), zoom=(0, 4, 0.01),  dim1=(0, 4, 1), dim2=(1, 6, 2), dim3=(1, 9, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "564e4cec-09f2-499f-9c22-beb95119ba28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "788c380cac174606a98992d9a5a27674",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=23.0, description='elev', max=45.0, min=1.0), FloatSlider(value=35.0, …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.interactive_plot(elev, azim, zoom, dim1, dim2, dim3)>"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = letters\n",
    "\n",
    "def interactive_plot(elev, azim, zoom, dim1, dim2, dim3):\n",
    "\n",
    "    x = esb[1:].T[dim1].tolist()\n",
    "    y = esb[1:].T[dim2].tolist()\n",
    "    z = esb[1:].T[dim3].tolist()\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    ax = plt.axes(projection='3d')\n",
    "\n",
    "    ax.view_init(elev, azim)\n",
    "    ax.set_box_aspect((1, 1, 1), zoom=zoom)\n",
    "    \n",
    "    ax.scatter(x, y, z)\n",
    "    \n",
    "    for i, txt in enumerate(n):\n",
    "        # ax.annotate(txt, (x[i], y[i], z[i]))\n",
    "        ax.text(x[i], y[i], z[i], txt, color='blue')\n",
    "\n",
    "\n",
    "interact(interactive_plot, elev=(1, 45, 0.1), azim=(1, 70, 0.1), zoom=(0, 4, 0.01),  dim1=(0, 4, 1), dim2=(1, 6, 2), dim3=(1, 9, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34edb3b-12ad-4aec-9ead-8eb85eca8df8",
   "metadata": {},
   "source": [
    "Just by eye-balling 10 dimensions this way it looks like some are pretty similar \n",
    "- maybe we're not getting much from them in this simple space\n",
    "- reduce and try again"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
