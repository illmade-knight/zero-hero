{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "965b36f1-1440-4a07-a843-80cba31c34e8",
   "metadata": {},
   "source": [
    "### Increase the corpus, read the paper\n",
    "\n",
    "let's start reading more books\n",
    "\n",
    "some stuff from reading the paper:\n",
    "\n",
    "Was wondering what difference multi-head attention made to just having a bigger head at attention:\n",
    "\n",
    "*Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this*\n",
    "\n",
    "so apparently the softmax function is the problem and that's why the multihead attention is used\n",
    "\n",
    "looking at optimizers - have been using straight SGD up to now - which seems good for small models, will look at using Adam\n",
    "\n",
    "*5.3 Optimizer\n",
    " Weused the Adam optimizer [20] with β1 = 0.9, β2 = 0.98 and ϵ = 10−9. We varied the learning\n",
    " rate over the course of training, according to the formula:\n",
    " lrate = d−0.5\n",
    " model · min(step_num−0.5,step_num · warmup_steps−1.5)\n",
    " (3)\n",
    " This corresponds to increasing the learning rate linearly for the first warmup_steps training steps,\n",
    " and decreasing it thereafter proportionally to the inverse square root of the step number. We used\n",
    " warmup_steps = 4000.*\n",
    "\n",
    "\n",
    " look at adding Label Smoothing as per the paper\n",
    "\n",
    " *LabelSmoothing Duringtraining,weemployedlabelsmoothingofvalueϵls=0.1[36].This\n",
    " hurtsperplexity,asthemodellearnstobemoreunsure,butimprovesaccuracyandBLEUscore.*\n",
    "\n",
    " torch softmax has optional parameter: label_smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d689a91-95ca-42cd-802e-ab97496243cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec8919ad-3d6b-4421-94ce-553e3a506dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run bookreader.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5b6ef18-283c-41fd-9375-ef8d13ed5e0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "br = BookReader(False, r'[^\\d+a-zA-Z \\n?!:,]')\n",
    "br.read(\"Middlemarch.txt\", \"The lifted veil.txt\", \"mill on the floss.txt\", \"brother jacob.txt\")\n",
    "vocab_size = br.vocab_size\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99864c15-f328-4fde-8512-3e76fc78862f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# br = BookReader(False, r'[^\\d+a-zA-Z \\n?!:,]')\n",
    "# br.read(\"Middlemarch.txt\", \"The lifted veil.txt\", \"mill on the floss.txt\", \"tiny_shakespeare.txt\", \"alice.txt\")\n",
    "# vocab_size = br.vocab_size\n",
    "# vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a713daf8-4a5f-430d-8f00-9c6db6743d22",
   "metadata": {},
   "source": [
    "### Get the batch with both x and y unseparated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91f9fe0b-25d9-4578-a105-4eb99138428b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(data, batch_length=5, batch_size=5):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    ix = torch.randint(len(data) - batch_length, (batch_size,))\n",
    "    b = torch.stack([data[i:i+batch_length] for i in ix])\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac12ed62-275e-43a8-b17e-9b0b6da58efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = torch.tensor(br.data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3b116f-f526-4958-bfed-07f665e89cba",
   "metadata": {},
   "source": [
    "### Create an attention head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf6cf111-bf37-49fe-8e92-de8a35f8e683",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "\n",
    "    def __init__(self, c, head_size, content_length):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(c, head_size, bias=False)\n",
    "        self.query = nn.Linear(c, head_size, bias=False)\n",
    "        self.value = nn.Linear(c, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(content_length, content_length)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   \n",
    "        q = self.query(x)\n",
    "        \n",
    "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 \n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        \n",
    "        v = self.value(x) # (B,T,hs)\n",
    "        out = wei @ v\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7138eed8-721e-4752-bf9f-8ebca24882d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, fan_in, multiplier = 4):\n",
    "        super().__init__()\n",
    "\n",
    "        layers = OrderedDict([\n",
    "            (\"l_in\", nn.Linear(fan_in, multiplier * fan_in)),\n",
    "            (\"relu\", nn.ReLU()),\n",
    "            (\"l_out\", nn.Linear(multiplier * fan_in, fan_in)),\n",
    "        ])\n",
    "        self.net = nn.Sequential(\n",
    "            layers\n",
    "        )\n",
    "\n",
    "        initial = layers['l_in']\n",
    "        nn.init.kaiming_normal_(initial.weight, nonlinearity=\"relu\")\n",
    "        layers['l_in'].weight.data = initial.weight.data * 3/5\n",
    "        if initial.bias is not None:\n",
    "            nn.init.constant_(initial.bias, 0)\n",
    "\n",
    "        final = layers['l_out']\n",
    "        layers['l_out'].weight.data = final.weight.data * .2\n",
    "        if final.bias is not None:\n",
    "            nn.init.constant_(final.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b77ec387-64c7-43b3-82a5-b24e22eb84a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "989f2ee4-1968-4bf6-8352-4e37ebaca633",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHead(nn.Module):\n",
    "    def __init__(self, num_heads, head_size, embed_size, content_length):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(embed_size, head_size, content_length) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(head_size * num_heads, embed_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5ccc997-e0f3-43a3-a696-594a3dec0710",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, num_heads, head_size, embed_size, content_length, ff_mul):\n",
    "        super().__init__()\n",
    "        self.multihead = MultiHead(num_heads, head_size, embed_size, content_length)\n",
    "        self.n1 = nn.LayerNorm(embed_size)\n",
    "        self.ff = FeedForward(embed_size, ff_mul)\n",
    "        self.n1 = nn.LayerNorm(embed_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_a = x.detach()\n",
    "        x = self.multihead(x)\n",
    "        x = x + x_a\n",
    "        x = self.n1(x)\n",
    "        x_b = x.detach()\n",
    "        x = self.ff(x)\n",
    "        x = x + x_b\n",
    "        x = self.n1(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dff586a7-6f05-4e2a-95f4-9a07c8e27203",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFMultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_size, content_length, num_heads, head_size, multiplier=4):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.vocab_embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.positional_embed = nn.Embedding(content_length, embed_size)\n",
    "        # self.mutli_attention = MultiHead(num_heads, head_size, embed_size, content_length)\n",
    "        # self.lna = nn.LayerNorm(embed_size)\n",
    "        # self.ff = FeedForward(embed_size, multiplier)\n",
    "        # self.lnff = nn.LayerNorm(embed_size)\n",
    "        self.atta = AttentionBlock(num_heads, head_size, embed_size, content_length, multiplier)\n",
    "        self.attb = AttentionBlock(num_heads, head_size, embed_size, content_length, multiplier)\n",
    "        self.decode = nn.Linear(embed_size, vocab_size)\n",
    "        self.content_length = content_length\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        #idx B,T\n",
    "        B, T = idx.shape\n",
    "\n",
    "        idx_e = self.vocab_embed(idx)\n",
    "        # note tr is always the same - so the learning here is information passed back to the positional_embed from loss\n",
    "        tr = torch.arange(T)\n",
    "        pos_e = self.positional_embed(tr)\n",
    "\n",
    "        x = idx_e + pos_e\n",
    "        \n",
    "        x = self.atta(x)\n",
    "        x = self.attb(x)\n",
    "        logits = self.decode(x)\n",
    "        # print(\"decode out\", x.shape)\n",
    "        # print(\"targets\", targets.shape)\n",
    "        # return None, None\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            targets = targets.reshape(B*T)\n",
    "            loss = F.cross_entropy(logits.view(B*T, -1), targets) #.resize(B*T)) # loss function\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -self.content_length:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "16926363-2c95-4998-a0a9-8eee16e1abf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_val_batch(data, batch_length=5, batch_size=5, i=0):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    if i == 0:\n",
    "        ix = torch.randint(len(data) - batch_length, (batch_size,))\n",
    "    else:\n",
    "        ix = torch.arange(1, 5) + 1 + i\n",
    "\n",
    "    b = torch.stack([data[i:i+batch_length] for i in ix])\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f874381a-99eb-4bba-bde0-b10c9632e5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "@torch.no_grad()\n",
    "def split_loss(split):\n",
    "    split_len = len(split)\n",
    "    total_loss = 0\n",
    "    batch_size = 50\n",
    "    num_batches = math.floor(split_len / batch_size)\n",
    "    print(\"num_batches\", split_len, num_batches)\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    for i in range(num_batches):\n",
    "\n",
    "        t_b = get_val_batch(split, context_length+1, batch_size, i*batch_size)\n",
    "        \n",
    "        x = t_b[:, 0: context_length]\n",
    "        y = t_b[:, context_length: context_length+1]\n",
    "        \n",
    "        t_b = get_batch(train, context_length+1, batch_size)\n",
    "\n",
    "        x = t_b[:, 0:context_length]\n",
    "        y = t_b[:, 1:context_length+1]\n",
    "\n",
    "        logits, batch_loss = model(x, y)\n",
    "        \n",
    "        total_loss = total_loss + batch_loss\n",
    "    \n",
    "    print(\"total loss\", total_loss, total_loss / num_batches)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120d7e23-4610-468c-97ef-6be91403ed96",
   "metadata": {},
   "source": [
    "Increased content_length fixed skip connnection bug in Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3c2d3c52-0c8a-4ab6-8121-7e8377d361d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "30276  parameters\n",
      "ep 0 tensor(2.5036, grad_fn=<DivBackward0>) [0.098]\n",
      "ep 2 tensor(2.0582, grad_fn=<DivBackward0>) [0.0941192]\n",
      "ep 4 tensor(1.9447, grad_fn=<DivBackward0>) [0.09039207968]\n",
      "ep 6 tensor(1.8907, grad_fn=<DivBackward0>) [0.086812553324672]\n",
      "ep 8 tensor(1.8606, grad_fn=<DivBackward0>) [0.08337477621301498]\n",
      "ep 10 tensor(1.8366, grad_fn=<DivBackward0>) [0.08007313507497958]\n",
      "ep 12 tensor(1.8189, grad_fn=<DivBackward0>) [0.07690223892601039]\n",
      "ep 14 tensor(1.8033, grad_fn=<DivBackward0>) [0.07385691026454037]\n",
      "ep 16 tensor(1.7879, grad_fn=<DivBackward0>) [0.07093217661806457]\n",
      "ep 18 tensor(1.7777, grad_fn=<DivBackward0>) [0.06812326242398921]\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "training_runs = 800\n",
    "batch_size = 96\n",
    "context_length = 24\n",
    "learning_rate = .1\n",
    "embedding_dimensions = 32\n",
    "num_heads = 4\n",
    "head_size = embedding_dimensions // num_heads\n",
    "\n",
    "print(head_size)\n",
    "# our embedding_dimensions are still 'small' so we mutliply the size our our feed forward network to make up\n",
    "multiplier = 4\n",
    "model = FFMultiHeadAttention(embedding_dimensions, context_length, num_heads, head_size, multiplier)\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "\n",
    "print(sum(p.numel() for p in model.parameters()), ' parameters')\n",
    "\n",
    "lmbda = lambda epoch: 0.98\n",
    "\n",
    "m_scheduler = optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)\n",
    "\n",
    "for ep in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    for tr in range(training_runs):\n",
    "        t_b = get_batch(train, context_length+1, batch_size)\n",
    "\n",
    "        x = t_b[:, 0:context_length]\n",
    "        y = t_b[:, 1:context_length+1]\n",
    "\n",
    "        logits, loss = model(x, y)\n",
    "\n",
    "        epoch_loss += loss\n",
    "        \n",
    "        model.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    m_scheduler.step()\n",
    "    \n",
    "    if ep % 2 == 0:\n",
    "        print(\"ep\", ep, epoch_loss/training_runs, m_scheduler.get_last_lr())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbd1592-04bb-4f87-9f23-23c859c6f38f",
   "metadata": {},
   "source": [
    "Before fix performance\n",
    "\n",
    "ep 18 tensor(1.8988, grad_fn=<DivBackward0>) [0.06812326242398921]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f397197a-b710-46a3-b974-fb9605a29ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = torch.tensor(br.data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f219b8c9-2430-4b93-a0a0-4b015bd0ce1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_batches 296550 5931\n",
      "total loss tensor(10516.9697) tensor(1.7732)\n",
      "None 296550\n",
      "\n",
      "I affer he uneliars no if at are,  on this me will s in a divo own  \n",
      " Deesal uddent of dight matings\n"
     ]
    }
   ],
   "source": [
    "print(split_loss(dev), len(dev))\n",
    "\n",
    "idx = torch.zeros((1, 1), dtype=torch.int)\n",
    "for i in range(1):\n",
    "    o = model.generate(idx, 100).data[0].tolist()\n",
    "    print(br.decode(o))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "54233b18-70ea-427d-8d88-8ecbcf40fe89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Then a esire by selvusionstirinable aggo, in kins presence of was  elsegsing of oppocience morre has boching eavide   \n",
      "\n",
      " Einr his felloth  soely me oust an any are, dulad worging an to whe to been that yge he stnase well aling of could goodicatine a und so her lught\n",
      "pronrite obsher, and go minunt and telt\n",
      "re\n",
      "gere utsudderfor other got\n",
      "ill had no uletty  seemalty?\n",
      "sa\n",
      "creave \n",
      "snd the not know he Fin well to and\n",
      "on hinds the\n",
      "favity, as pure him in eyo  a repaly playing a s from has but sheg will by vinfs if theade very aguety unhpusical at I challe of have ords at her he Comcureed thing coulded\n",
      "yoir her nor been and abor\n",
      "auid beyod with nentors of\n",
      "that lagrent chream more only go wut me in the gaved,iece dhe takigness in accort day gin be going to the\n",
      "the pay resirant ilefty band re\n",
      "oung irlss of\n",
      "coor, much with decluness when\n",
      "I he\n",
      "had Mr  F Lydgate, that Daston that shopple going nessieng be to that he would mope, to that posen a\n",
      "builk them fres ruving that eviescame as soming would:  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "idx = torch.zeros((1, 1), dtype=torch.int)\n",
    "for i in range(1):\n",
    "    o = model.generate(idx, 1000).data[0].tolist()\n",
    "    print(br.decode(o))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "de6e4b2b-94bb-47c3-b7bd-083bc9e57554",
   "metadata": {},
   "outputs": [],
   "source": [
    "e_epochs = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "92654c18-31b6-45bd-bdb6-4bb97875fa0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 0 tensor(1.7695, grad_fn=<DivBackward0>) [0.06542558123199924]\n",
      "ep 2 tensor(1.7617, grad_fn=<DivBackward0>) [0.06283472821521206]\n",
      "ep 4 tensor(1.7555, grad_fn=<DivBackward0>) [0.06034647297788966]\n",
      "ep 6 tensor(1.7487, grad_fn=<DivBackward0>) [0.05795675264796523]\n",
      "ep 8 tensor(1.7412, grad_fn=<DivBackward0>) [0.055661665243105805]\n",
      "ep 10 tensor(1.7372, grad_fn=<DivBackward0>) [0.053457463299478813]\n",
      "ep 12 tensor(1.7308, grad_fn=<DivBackward0>) [0.05134054775281945]\n",
      "ep 14 tensor(1.7243, grad_fn=<DivBackward0>) [0.0493074620618078]\n",
      "ep 16 tensor(1.7203, grad_fn=<DivBackward0>) [0.04735488656416021]\n",
      "ep 18 tensor(1.7161, grad_fn=<DivBackward0>) [0.04547963305621946]\n",
      "ep 20 tensor(1.7135, grad_fn=<DivBackward0>) [0.04367863958719317]\n",
      "ep 22 tensor(1.7067, grad_fn=<DivBackward0>) [0.04194896545954032]\n",
      "ep 24 tensor(1.7054, grad_fn=<DivBackward0>) [0.04028778642734252]\n",
      "ep 26 tensor(1.7014, grad_fn=<DivBackward0>) [0.038692390084819756]\n",
      "ep 28 tensor(1.6983, grad_fn=<DivBackward0>) [0.03716017143746089]\n",
      "ep 30 tensor(1.6969, grad_fn=<DivBackward0>) [0.03568862864853744]\n",
      "ep 32 tensor(1.6929, grad_fn=<DivBackward0>) [0.034275358954055354]\n",
      "ep 34 tensor(1.6876, grad_fn=<DivBackward0>) [0.03291805473947476]\n",
      "ep 36 tensor(1.6858, grad_fn=<DivBackward0>) [0.03161449977179156]\n",
      "ep 38 tensor(1.6821, grad_fn=<DivBackward0>) [0.030362565580828612]\n",
      "ep 40 tensor(1.6827, grad_fn=<DivBackward0>) [0.029160207983827797]\n",
      "ep 42 tensor(1.6804, grad_fn=<DivBackward0>) [0.028005463747668217]\n",
      "ep 44 tensor(1.6790, grad_fn=<DivBackward0>) [0.026896447383260556]\n",
      "ep 46 tensor(1.6754, grad_fn=<DivBackward0>) [0.025831348066883437]\n",
      "ep 48 tensor(1.6731, grad_fn=<DivBackward0>) [0.024808426683434852]\n",
      "ep 50 tensor(1.6716, grad_fn=<DivBackward0>) [0.023826012986770832]\n",
      "ep 52 tensor(1.6721, grad_fn=<DivBackward0>) [0.022882502872494704]\n",
      "ep 54 tensor(1.6683, grad_fn=<DivBackward0>) [0.02197635575874391]\n",
      "ep 56 tensor(1.6672, grad_fn=<DivBackward0>) [0.021106092070697653]\n",
      "ep 58 tensor(1.6652, grad_fn=<DivBackward0>) [0.020270290824698025]\n"
     ]
    }
   ],
   "source": [
    "for ep in range(e_epochs):\n",
    "    epoch_loss = 0\n",
    "    for tr in range(training_runs):\n",
    "        t_b = get_batch(train, context_length+1, batch_size)\n",
    "\n",
    "        x = t_b[:, 0:context_length]\n",
    "        y = t_b[:, 1:context_length+1]\n",
    "\n",
    "        logits, loss = model(x, y)\n",
    "\n",
    "        epoch_loss += loss\n",
    "        \n",
    "        model.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    m_scheduler.step()\n",
    "    \n",
    "    if ep % 2 == 0:\n",
    "        print(\"ep\", ep, epoch_loss/training_runs, m_scheduler.get_last_lr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8c4fb6e3-9d77-40d6-9c67-e40aad98981d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_batches 296550 5931\n",
      "total loss tensor(9868.3203) tensor(1.6639)\n",
      "None 296550\n",
      "\n",
      "expect\n",
      "condation  Ramould poncepty \n",
      "think once the mind he again!  said\n",
      "My couldn aloned not a voin\n",
      "in a\n",
      "sightt  Ah, add Mr  Brob had\n",
      "depoce for him at cress  I have this never the gettenous was give if Alvident, but\n",
      "in away  and me is again, and inwally spicely besistainly time of leggep  expersenc\n"
     ]
    }
   ],
   "source": [
    "print(split_loss(dev), len(dev))\n",
    "\n",
    "idx = torch.zeros((1, 1), dtype=torch.int)\n",
    "for i in range(1):\n",
    "    o = model.generate(idx, 300).data[0].tolist()\n",
    "    print(br.decode(o))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "041a859b-b35a-461f-8aec-83b478aabd0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 0 tensor(1.6629, grad_fn=<DivBackward0>) [0.019467587308039984]\n",
      "ep 2 tensor(1.6606, grad_fn=<DivBackward0>) [0.0186966708506416]\n",
      "ep 4 tensor(1.6591, grad_fn=<DivBackward0>) [0.017956282684956193]\n",
      "ep 6 tensor(1.6594, grad_fn=<DivBackward0>) [0.017245213890631928]\n",
      "ep 8 tensor(1.6570, grad_fn=<DivBackward0>) [0.016562303420562904]\n",
      "ep 10 tensor(1.6560, grad_fn=<DivBackward0>) [0.01590643620510861]\n",
      "ep 12 tensor(1.6558, grad_fn=<DivBackward0>) [0.015276541331386308]\n",
      "ep 14 tensor(1.6522, grad_fn=<DivBackward0>) [0.01467159029466341]\n",
      "ep 16 tensor(1.6525, grad_fn=<DivBackward0>) [0.014090595318994738]\n",
      "ep 18 tensor(1.6531, grad_fn=<DivBackward0>) [0.013532607744362546]\n",
      "ep 20 tensor(1.6511, grad_fn=<DivBackward0>) [0.012996716477685789]\n",
      "ep 22 tensor(1.6490, grad_fn=<DivBackward0>) [0.01248204650516943]\n",
      "ep 24 tensor(1.6510, grad_fn=<DivBackward0>) [0.011987757463564721]\n",
      "ep 26 tensor(1.6473, grad_fn=<DivBackward0>) [0.011513042268007558]\n",
      "ep 28 tensor(1.6470, grad_fn=<DivBackward0>) [0.01105712579419446]\n",
      "ep 30 tensor(1.6456, grad_fn=<DivBackward0>) [0.010619263612744357]\n",
      "ep 32 tensor(1.6460, grad_fn=<DivBackward0>) [0.01019874077367968]\n",
      "ep 34 tensor(1.6451, grad_fn=<DivBackward0>) [0.009794870639041964]\n",
      "ep 36 tensor(1.6448, grad_fn=<DivBackward0>) [0.009406993761735902]\n",
      "ep 38 tensor(1.6453, grad_fn=<DivBackward0>) [0.00903447680877116]\n",
      "ep 40 tensor(1.6423, grad_fn=<DivBackward0>) [0.008676711527143824]\n",
      "ep 42 tensor(1.6416, grad_fn=<DivBackward0>) [0.008333113750668928]\n",
      "ep 44 tensor(1.6423, grad_fn=<DivBackward0>) [0.008003122446142439]\n",
      "ep 46 tensor(1.6421, grad_fn=<DivBackward0>) [0.007686198797275197]\n",
      "ep 48 tensor(1.6400, grad_fn=<DivBackward0>) [0.007381825324903099]\n",
      "ep 50 tensor(1.6400, grad_fn=<DivBackward0>) [0.007089505042036937]\n",
      "ep 52 tensor(1.6393, grad_fn=<DivBackward0>) [0.006808760642372274]\n",
      "ep 54 tensor(1.6355, grad_fn=<DivBackward0>) [0.006539133720934331]\n",
      "ep 56 tensor(1.6374, grad_fn=<DivBackward0>) [0.006280184025585331]\n",
      "ep 58 tensor(1.6371, grad_fn=<DivBackward0>) [0.006031488738172152]\n"
     ]
    }
   ],
   "source": [
    "for ep in range(e_epochs):\n",
    "    epoch_loss = 0\n",
    "    for tr in range(training_runs):\n",
    "        t_b = get_batch(train, context_length+1, batch_size)\n",
    "\n",
    "        x = t_b[:, 0:context_length]\n",
    "        y = t_b[:, 1:context_length+1]\n",
    "\n",
    "        logits, loss = model(x, y)\n",
    "\n",
    "        epoch_loss += loss\n",
    "        \n",
    "        model.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    m_scheduler.step()\n",
    "    \n",
    "    if ep % 2 == 0:\n",
    "        print(\"ep\", ep, epoch_loss/training_runs, m_scheduler.get_last_lr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2ef57336-8eb5-42a0-878a-008d63510397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_batches 296550 5931\n",
      "total loss tensor(9643.2451) tensor(1.6259)\n",
      "None 296550\n",
      "\n",
      "line eady s hament herows  This own look mill to halve only asay same in Mrs Glegging aunting that men stagreons are would might in \n",
      "\n",
      "It s\n",
      "Mr  Philip with libless,\n",
      "thought with new\n",
      "that  Nob: experses the morride a made dide  thole such her a seeat, and  My lench, even a remate converium\n",
      "and that at\n",
      "\n",
      "Dave any\n",
      "knowinced and refuse because of perhenself Mr Tullibably annisgriven with heash, to the dorse of you\n",
      "will boid Found\n",
      "idracted ove sort feegalf entflarce a so lost at the bat seed to the Cerfaction for subjust her    Co chare caused\n",
      "other preprest you contry obscious, cases,  other man, but,\n"
     ]
    }
   ],
   "source": [
    "print(split_loss(dev), len(dev))\n",
    "\n",
    "idx = torch.zeros((1, 1), dtype=torch.int)\n",
    "for i in range(2):\n",
    "    o = model.generate(idx, 300).data[0].tolist()\n",
    "    print(br.decode(o))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "aa496f61-20fd-495e-9053-7358877c2359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_batches 339325 6786\n",
      "total loss tensor(11422.1123) tensor(1.6832)\n",
      "None 339325\n",
      "\n",
      "Mr  Brooke a wore I know, now wness sot could be before and a ade a rear was sir like, you as seefually\n",
      "soxpeactage   any undering was solition bhcs, I go  and Lydgate, wish, a suppition, Perfuiet in did if found him? \n",
      " Yes,\n",
      "it use looke,  when Maggie posted, Bne with\n",
      "hard of expose to Pippe of ext \n",
      "\n",
      "vising that she had do,  said Cadpositates, as a teat he forkness away other that solk havet,  and sportne of obly into you like their\n",
      " mall, uncorners if prepabacument at you at he saw Pianted ought, those found \n",
      "Maggie, defivyd looking held awaind main foliced a man? \n",
      "  and hard of idiofe  But an \n"
     ]
    }
   ],
   "source": [
    "print(split_loss(dev), len(dev))\n",
    "\n",
    "idx = torch.zeros((1, 1), dtype=torch.int)\n",
    "for i in range(2):\n",
    "    o = model.generate(idx, 300).data[0].tolist()\n",
    "    print(br.decode(o))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "969ce431-24fe-4dd8-9969-61f7933e4f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"elliot_checkpoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3171f5d1-306c-4db5-92ae-0497fb5fafa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 0 tensor(1.6380, grad_fn=<DivBackward0>) [0.005792641784140534]\n",
      "ep 2 tensor(1.6350, grad_fn=<DivBackward0>) [0.005563253169488569]\n",
      "ep 4 tensor(1.6360, grad_fn=<DivBackward0>) [0.005342948343976821]\n",
      "ep 6 tensor(1.6346, grad_fn=<DivBackward0>) [0.005131367589555339]\n",
      "ep 8 tensor(1.6355, grad_fn=<DivBackward0>) [0.004928165433008947]\n",
      "ep 10 tensor(1.6322, grad_fn=<DivBackward0>) [0.004733010081861793]\n",
      "ep 12 tensor(1.6329, grad_fn=<DivBackward0>) [0.004545582882620065]\n",
      "ep 14 tensor(1.6322, grad_fn=<DivBackward0>) [0.00436557780046831]\n",
      "ep 16 tensor(1.6327, grad_fn=<DivBackward0>) [0.004192700919569765]\n",
      "ep 18 tensor(1.6321, grad_fn=<DivBackward0>) [0.004026669963154802]\n",
      "ep 20 tensor(1.6311, grad_fn=<DivBackward0>) [0.003867213832613871]\n",
      "ep 22 tensor(1.6311, grad_fn=<DivBackward0>) [0.0037140721648423617]\n",
      "ep 24 tensor(1.6306, grad_fn=<DivBackward0>) [0.003566994907114604]\n",
      "ep 26 tensor(1.6293, grad_fn=<DivBackward0>) [0.0034257419087928656]\n",
      "ep 28 tensor(1.6286, grad_fn=<DivBackward0>) [0.003290082529204668]\n",
      "ep 30 tensor(1.6296, grad_fn=<DivBackward0>) [0.003159795261048163]\n",
      "ep 32 tensor(1.6303, grad_fn=<DivBackward0>) [0.0030346673687106558]\n",
      "ep 34 tensor(1.6294, grad_fn=<DivBackward0>) [0.0029144945409097134]\n",
      "ep 36 tensor(1.6286, grad_fn=<DivBackward0>) [0.0027990805570896884]\n",
      "ep 38 tensor(1.6296, grad_fn=<DivBackward0>) [0.0026882369670289366]\n",
      "ep 40 tensor(1.6311, grad_fn=<DivBackward0>) [0.0025817827831345905]\n",
      "ep 42 tensor(1.6279, grad_fn=<DivBackward0>) [0.0024795441849224603]\n",
      "ep 44 tensor(1.6313, grad_fn=<DivBackward0>) [0.0023813542351995304]\n",
      "ep 46 tensor(1.6260, grad_fn=<DivBackward0>) [0.002287052607485629]\n",
      "ep 48 tensor(1.6263, grad_fn=<DivBackward0>) [0.002196485324229198]\n",
      "ep 50 tensor(1.6277, grad_fn=<DivBackward0>) [0.0021095045053897217]\n",
      "ep 52 tensor(1.6283, grad_fn=<DivBackward0>) [0.0020259681269762884]\n",
      "ep 54 tensor(1.6282, grad_fn=<DivBackward0>) [0.0019457397891480272]\n",
      "ep 56 tensor(1.6305, grad_fn=<DivBackward0>) [0.0018686884934977653]\n",
      "ep 58 tensor(1.6255, grad_fn=<DivBackward0>) [0.0017946884291552537]\n"
     ]
    }
   ],
   "source": [
    "for ep in range(e_epochs):\n",
    "    epoch_loss = 0\n",
    "    for tr in range(training_runs):\n",
    "        t_b = get_batch(train, context_length+1, batch_size)\n",
    "\n",
    "        x = t_b[:, 0:context_length]\n",
    "        y = t_b[:, 1:context_length+1]\n",
    "\n",
    "        logits, loss = model(x, y)\n",
    "\n",
    "        epoch_loss += loss\n",
    "        \n",
    "        model.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    m_scheduler.step()\n",
    "    \n",
    "    if ep % 2 == 0:\n",
    "        print(\"ep\", ep, epoch_loss/training_runs, m_scheduler.get_last_lr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "58f9841d-2d47-4f87-9e78-c0ebeb56e23a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_batches 296550 5931\n",
      "total loss tensor(9646.5020) tensor(1.6265)\n",
      "None 296550\n",
      "\n",
      "reparent  But twe Middle, and think and\n",
      "Charth saterful, with who\n",
      "have ammemoa vincy get, and tricise  There like hurtain iddred with\n",
      " Stip  \n",
      "\n",
      "Hasy than might about in the think and with had but to lious\n",
      "obliging, difficultic and\n",
      "as knelices: a come to on the highint  get yet by decisardly  He\n",
      "thoug\n",
      "\n",
      "The sure all gaver\n",
      "poy help angitiong, an\n",
      "opeve fellow, been to tell childing suck  I m to to other laing bit no\n",
      "had triet I\n",
      "corlong taken the seemed and Maggie for hards by seady worth, retainess, busins lafe his welt\n",
      "arding\n",
      "and    said might \n",
      "shem colles strangus you seen can in that ispose sight,\n"
     ]
    }
   ],
   "source": [
    "print(split_loss(dev), len(dev))\n",
    "\n",
    "idx = torch.zeros((1, 1), dtype=torch.int)\n",
    "for i in range(2):\n",
    "    o = model.generate(idx, 300).data[0].tolist()\n",
    "    print(br.decode(o))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
