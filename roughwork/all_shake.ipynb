{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "965b36f1-1440-4a07-a843-80cba31c34e8",
   "metadata": {},
   "source": [
    "### What if we increase the corpus\n",
    "\n",
    "let's start reading more books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d689a91-95ca-42cd-802e-ab97496243cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ec8919ad-3d6b-4421-94ce-553e3a506dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run bookreader.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e5b6ef18-283c-41fd-9375-ef8d13ed5e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p [^\\d+a-zA-Z \\n?!:,]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "59"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "br = BookReader(False, r'[^\\d+a-zA-Z \\n?!:,]')\n",
    "br.read(\"tiny_shakespeare.txt\")\n",
    "vocab_size = br.vocab_size\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a713daf8-4a5f-430d-8f00-9c6db6743d22",
   "metadata": {},
   "source": [
    "### Get the batch with both x and y unseparated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91f9fe0b-25d9-4578-a105-4eb99138428b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(data, batch_length=5, batch_size=5):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    ix = torch.randint(len(data) - batch_length, (batch_size,))\n",
    "    b = torch.stack([data[i:i+batch_length] for i in ix])\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ac12ed62-275e-43a8-b17e-9b0b6da58efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = torch.tensor(br.data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3b116f-f526-4958-bfed-07f665e89cba",
   "metadata": {},
   "source": [
    "### Create an attention head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf6cf111-bf37-49fe-8e92-de8a35f8e683",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "\n",
    "    def __init__(self, c, head_size, content_length):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(c, head_size, bias=False)\n",
    "        self.query = nn.Linear(c, head_size, bias=False)\n",
    "        self.value = nn.Linear(c, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(content_length, content_length)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   \n",
    "        q = self.query(x)\n",
    "        \n",
    "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 \n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        \n",
    "        v = self.value(x) # (B,T,hs)\n",
    "        out = wei @ v\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7138eed8-721e-4752-bf9f-8ebca24882d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, fan_in, multiplier = 4):\n",
    "        super().__init__()\n",
    "\n",
    "        layers = OrderedDict([\n",
    "            (\"l_in\", nn.Linear(fan_in, multiplier * fan_in)),\n",
    "            (\"relu\", nn.ReLU()),\n",
    "            (\"l_out\", nn.Linear(multiplier * fan_in, fan_in)),\n",
    "        ])\n",
    "        self.net = nn.Sequential(\n",
    "            layers\n",
    "        )\n",
    "\n",
    "        initial = layers['l_in']\n",
    "        nn.init.kaiming_normal_(initial.weight, nonlinearity=\"relu\")\n",
    "        layers['l_in'].weight.data = initial.weight.data * 3/5\n",
    "        if initial.bias is not None:\n",
    "            nn.init.constant_(initial.bias, 0)\n",
    "\n",
    "        final = layers['l_out']\n",
    "        layers['l_out'].weight.data = final.weight.data * .2\n",
    "        if final.bias is not None:\n",
    "            nn.init.constant_(final.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c5ab893-cdb8-4702-9459-99a717f81b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_size, head_size, content_length):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.vocab_embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.positional_embed = nn.Embedding(content_length, embed_size)\n",
    "        self.attention = Head(embed_size, head_size, content_length)\n",
    "        self.ff = FeedForward(head_size)\n",
    "        self.decode = nn.Linear(head_size, vocab_size)\n",
    "        self.content_length = content_length\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        #idx B,T\n",
    "        B, T = idx.shape\n",
    "\n",
    "        idx_e = self.vocab_embed(idx)\n",
    "        # note tr is always the same - so the learning here is information passed back to the positional_embed from loss\n",
    "        tr = torch.arange(T)\n",
    "        pos_e = self.positional_embed(tr)\n",
    "\n",
    "        x = idx_e + pos_e\n",
    "        x = self.attention(x)\n",
    "        \n",
    "        x = self.ff(x)\n",
    "        \n",
    "        logits = self.decode(x)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            loss = F.cross_entropy(logits.view(B*T, -1), targets.resize(B*T)) # loss function\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -self.content_length:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b77ec387-64c7-43b3-82a5-b24e22eb84a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "989f2ee4-1968-4bf6-8352-4e37ebaca633",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHead(nn.Module):\n",
    "\n",
    "    def __init__(self, num_heads, head_size, embed_size, content_length):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(embed_size, head_size, content_length) for _ in range(num_heads)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat( [head(x) for head in self.heads], dim = -1 )\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dff586a7-6f05-4e2a-95f4-9a07c8e27203",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFMultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_size, content_length, num_heads, head_size, multiplier=4):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.vocab_embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.positional_embed = nn.Embedding(content_length, embed_size)\n",
    "        self.mutli_attention = MultiHead(num_heads, head_size, embed_size, content_length)\n",
    "        self.lna = nn.LayerNorm(embed_size)\n",
    "        self.ff = FeedForward(embed_size, multiplier)\n",
    "        self.lnff = nn.LayerNorm(embed_size)\n",
    "        self.decode = nn.Linear(embed_size, vocab_size)\n",
    "        self.content_length = content_length\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        #idx B,T\n",
    "        B, T = idx.shape\n",
    "\n",
    "        idx_e = self.vocab_embed(idx)\n",
    "        # note tr is always the same - so the learning here is information passed back to the positional_embed from loss\n",
    "        tr = torch.arange(T)\n",
    "        pos_e = self.positional_embed(tr)\n",
    "\n",
    "        x = idx_e + pos_e\n",
    "        x = self.mutli_attention(x)\n",
    "        # print(\"multi ball out\", x.shape)\n",
    "        x = self.lna(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.lnff(x)\n",
    "        # print(\"feed forward out\", x.shape)\n",
    "        logits = self.decode(x)\n",
    "        # print(\"decode out\", x.shape)\n",
    "        # print(\"targets\", targets.shape)\n",
    "        # return None, None\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            targets = targets.reshape(B*T)\n",
    "            loss = F.cross_entropy(logits.view(B*T, -1), targets) #.resize(B*T)) # loss function\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -self.content_length:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "16926363-2c95-4998-a0a9-8eee16e1abf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_val_batch(data, batch_length=5, batch_size=5, i=0):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    if i == 0:\n",
    "        ix = torch.randint(len(data) - batch_length, (batch_size,))\n",
    "    else:\n",
    "        ix = torch.arange(1, 5) + 1 + i\n",
    "\n",
    "    b = torch.stack([data[i:i+batch_length] for i in ix])\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f874381a-99eb-4bba-bde0-b10c9632e5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "@torch.no_grad()\n",
    "def split_loss(split):\n",
    "    split_len = len(split)\n",
    "    total_loss = 0\n",
    "    batch_size = 50\n",
    "    num_batches = math.floor(split_len / batch_size)\n",
    "    print(\"num_batches\", split_len, num_batches)\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    for i in range(num_batches):\n",
    "\n",
    "        t_b = get_val_batch(split, context_length+1, batch_size, i*batch_size)\n",
    "        \n",
    "        x = t_b[:, 0: context_length]\n",
    "        y = t_b[:, context_length: context_length+1]\n",
    "        \n",
    "        t_b = get_batch(train, context_length+1, batch_size)\n",
    "\n",
    "        x = t_b[:, 0:context_length]\n",
    "        y = t_b[:, 1:context_length+1]\n",
    "\n",
    "        logits, batch_loss = model(x, y)\n",
    "        \n",
    "        total_loss = total_loss + batch_loss\n",
    "    \n",
    "    print(\"total loss\", total_loss, total_loss / num_batches)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac441974-7a3d-4aff-95b1-d020feb88d0c",
   "metadata": {},
   "source": [
    "### some examples from the past\n",
    "\n",
    "of the great artist we are creating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "249df925-b0c3-434b-98f9-16c3feadfc48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " said  \n",
      "\n",
      "at othe never exane yokle as lou mout ? \n",
      " her appp aged \n",
      "\n",
      "\n",
      "trabs and at  che you of the a c\n"
     ]
    }
   ],
   "source": [
    "idx = torch.zeros((1, 1), dtype=torch.int)\n",
    "for i in range(1):\n",
    "    o = model.generate(idx, 100).data[0].tolist()\n",
    "    print(names.decode(o))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "0cd99c37-b38c-4f56-a513-f25e7d286c1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_batches 11075 221\n",
      "total loss tensor(426.6404) tensor(1.9305)\n",
      "None 11075\n",
      "\n",
      "so\n",
      "was alice bot by ran to lact the saout be he the don the knen  wen sail \n",
      "\n",
      "\n",
      " you  alice and she cr\n"
     ]
    }
   ],
   "source": [
    "dev = torch.tensor(alice.data[1])\n",
    "print(split_loss(dev), len(dev))\n",
    "\n",
    "idx = torch.zeros((1, 1), dtype=torch.int)\n",
    "for i in range(1):\n",
    "    o = model.generate(idx, 100).data[0].tolist()\n",
    "    print(names.decode(o))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ba288d0d-642c-4405-b908-9b84be59a074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_batches 11075 221\n",
      "total loss tensor(371.4741) tensor(1.6809)\n",
      "None 11075\n",
      "\n",
      "you\n",
      "nice    and there much you d i bles of eyes  the\n",
      "pig   belcome    nother look       \n",
      "set  if you\n"
     ]
    }
   ],
   "source": [
    "print(split_loss(dev), len(dev))\n",
    "\n",
    "idx = torch.zeros((1, 1), dtype=torch.int)\n",
    "for i in range(1):\n",
    "    o = model.generate(idx, 100).data[0].tolist()\n",
    "    print(names.decode(o))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3c2d3c52-0c8a-4ab6-8121-7e8377d361d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "24091  parameters\n",
      "ep 0 tensor(2.4101, grad_fn=<DivBackward0>) [0.098]\n",
      "ep 5 tensor(1.8489, grad_fn=<DivBackward0>) [0.0885842380864]\n",
      "ep 10 tensor(1.7938, grad_fn=<DivBackward0>) [0.08007313507497958]\n",
      "ep 15 tensor(1.7672, grad_fn=<DivBackward0>) [0.07237977205924956]\n",
      "ep 20 tensor(1.7509, grad_fn=<DivBackward0>) [0.06542558123199924]\n",
      "ep 25 tensor(1.7381, grad_fn=<DivBackward0>) [0.059139543518331866]\n",
      "ep 30 tensor(1.7276, grad_fn=<DivBackward0>) [0.053457463299478813]\n",
      "ep 35 tensor(1.7215, grad_fn=<DivBackward0>) [0.048321312820571644]\n",
      "ep 40 tensor(1.7164, grad_fn=<DivBackward0>) [0.04367863958719317]\n",
      "ep 45 tensor(1.7115, grad_fn=<DivBackward0>) [0.03948203069879567]\n",
      "ep 50 tensor(1.7047, grad_fn=<DivBackward0>) [0.03568862864853744]\n",
      "ep 55 tensor(1.7024, grad_fn=<DivBackward0>) [0.03225969364468526]\n",
      "ep 60 tensor(1.6998, grad_fn=<DivBackward0>) [0.029160207983827797]\n",
      "ep 65 tensor(1.6977, grad_fn=<DivBackward0>) [0.026358518435595345]\n",
      "ep 70 tensor(1.6921, grad_fn=<DivBackward0>) [0.023826012986770832]\n",
      "ep 75 tensor(1.6904, grad_fn=<DivBackward0>) [0.021536828643569032]\n",
      "ep 80 tensor(1.6890, grad_fn=<DivBackward0>) [0.019467587308039984]\n",
      "ep 85 tensor(1.6842, grad_fn=<DivBackward0>) [0.01759715703125707]\n",
      "ep 90 tensor(1.6845, grad_fn=<DivBackward0>) [0.01590643620510861]\n",
      "ep 95 tensor(1.6842, grad_fn=<DivBackward0>) [0.014378158488770141]\n",
      "ep 100 tensor(1.6799, grad_fn=<DivBackward0>) [0.012996716477685789]\n",
      "ep 105 tensor(1.6773, grad_fn=<DivBackward0>) [0.011748002314293427]\n",
      "ep 110 tensor(1.6747, grad_fn=<DivBackward0>) [0.010619263612744357]\n",
      "ep 115 tensor(1.6729, grad_fn=<DivBackward0>) [0.009598973226261125]\n"
     ]
    }
   ],
   "source": [
    "epochs = 120\n",
    "training_runs = 800\n",
    "batch_size = 96\n",
    "context_length = 12\n",
    "learning_rate = .1\n",
    "embedding_dimensions = 32\n",
    "num_heads = 4\n",
    "head_size = embedding_dimensions // num_heads\n",
    "\n",
    "print(head_size)\n",
    "# our embedding_dimensions are still 'small' so we mutliply the size our our feed forward network to make up\n",
    "multiplier = 8\n",
    "model = FFMultiHeadAttention(embedding_dimensions, context_length, num_heads, head_size, multiplier)\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "\n",
    "print(sum(p.numel() for p in model.parameters()), ' parameters')\n",
    "\n",
    "lmbda = lambda epoch: 0.98\n",
    "\n",
    "m_scheduler = optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)\n",
    "\n",
    "for ep in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    for tr in range(training_runs):\n",
    "        t_b = get_batch(train, context_length+1, batch_size)\n",
    "\n",
    "        x = t_b[:, 0:context_length]\n",
    "        y = t_b[:, 1:context_length+1]\n",
    "\n",
    "        logits, loss = model(x, y)\n",
    "\n",
    "        epoch_loss += loss\n",
    "        \n",
    "        model.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    m_scheduler.step()\n",
    "    \n",
    "    if ep % 5 == 0:\n",
    "        print(\"ep\", ep, epoch_loss/training_runs, m_scheduler.get_last_lr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f219b8c9-2430-4b93-a0a0-4b015bd0ce1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_batches 93300 1866\n",
      "total loss tensor(3133.3733) tensor(1.6792)\n",
      "None 93300\n",
      "\n",
      "madamher must and your vick  \n",
      "then ttale \n",
      "thou st  but weshome s prinute reglingbroke mard time  pro\n"
     ]
    }
   ],
   "source": [
    "dev = torch.tensor(br.data[1])\n",
    "print(split_loss(dev), len(dev))\n",
    "\n",
    "idx = torch.zeros((1, 1), dtype=torch.int)\n",
    "for i in range(1):\n",
    "    o = model.generate(idx, 100).data[0].tolist()\n",
    "    print(alice.decode(o))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "293ee733-2861-4067-a6b8-e1fa78a7df98",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"all_shake_checkpoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7d64dce9-7232-4a37-a157-a04182f8e32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "e_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a4bf08fb-bd36-451c-9a2f-90720377483a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 0 tensor(3.0056, grad_fn=<DivBackward0>) [0.008676711527143824]\n",
      "ep 2 tensor(2.3823, grad_fn=<DivBackward0>) [0.008333113750668928]\n",
      "ep 4 tensor(2.2501, grad_fn=<DivBackward0>) [0.008003122446142439]\n",
      "ep 6 tensor(2.1740, grad_fn=<DivBackward0>) [0.007686198797275197]\n",
      "ep 8 tensor(2.1291, grad_fn=<DivBackward0>) [0.007381825324903099]\n",
      "ep 10 tensor(2.0973, grad_fn=<DivBackward0>) [0.007089505042036937]\n",
      "ep 12 tensor(2.0723, grad_fn=<DivBackward0>) [0.006808760642372274]\n",
      "ep 14 tensor(2.0516, grad_fn=<DivBackward0>) [0.006539133720934331]\n",
      "ep 16 tensor(2.0372, grad_fn=<DivBackward0>) [0.006280184025585331]\n",
      "ep 18 tensor(2.0247, grad_fn=<DivBackward0>) [0.006031488738172152]\n"
     ]
    }
   ],
   "source": [
    "for ep in range(e_epochs):\n",
    "    epoch_loss = 0\n",
    "    for tr in range(training_runs):\n",
    "        t_b = get_batch(train, context_length+1, batch_size)\n",
    "\n",
    "        x = t_b[:, 0:context_length]\n",
    "        y = t_b[:, 1:context_length+1]\n",
    "\n",
    "        logits, loss = model(x, y)\n",
    "\n",
    "        epoch_loss += loss\n",
    "        \n",
    "        model.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    m_scheduler.step()\n",
    "    \n",
    "    if ep % 2 == 0:\n",
    "        print(\"ep\", ep, epoch_loss/training_runs, m_scheduler.get_last_lr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bc94d36e-ad74-41f9-bd21-4815cfd665b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mod the not it dee!\n",
      "Harck ene, pret, farmith thine or \n",
      "you athey,\n",
      "VINGS:\n",
      "To \n",
      "Rer\n",
      "VING sund my\n",
      "Helf \n",
      "Or \n",
      "SICIUS:\n",
      "The noful be thern\n",
      "Nibent wibly colave have sire wommand licrfle mere, as\n",
      "Yout s inteavet is ster wiper quest has that wine me poan a milly blon this but hapirsidin yaglion kuZans well to sold suct  my frossonarstile sive wrer s thouckee: risee:\n",
      "Thands the \n",
      "Faing it beent ess,\n",
      "Of agive dead hist wark dof goded you  of thy \n",
      "be \n",
      "Anre\n",
      "And will loved show linge hathat that by siter sow be Sitilly sas, you had seach I your beam eat gre \n",
      "Be tade ecied thy onot the let: thee doft, I thesonsurghter will\n",
      "God I him lot thouldet deatitatles on have pands a me hist a my be flancelose \n",
      "Mwordes retawed all \n",
      "And wasuedts for far lo! I at wick\n",
      "Tis all we fice \n",
      "And thee with for a now dend frok no prok and wormined jrotent,\n",
      "That will,\n",
      "RUCHIONDET:\n",
      "I t?\n",
      "BASTRELETH:\n",
      "Of whis gofus!\n",
      "ind magght he for and, me tanter an of Edrodse,\n",
      "That mace dgir succh tatter is thuntlemate, anarrds\n",
      "The pring dose kais are fathe I or thuser sitider ter severs of lellpen, my shall with sir clown, whiince,\n"
     ]
    }
   ],
   "source": [
    "idx = torch.zeros((1, 1), dtype=torch.int)\n",
    "for i in range(1):\n",
    "    o = model.generate(idx, 1090).data[0].tolist()\n",
    "    print(br.decode(o))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "aa496f61-20fd-495e-9053-7358877c2359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 0 tensor(2.0127, grad_fn=<DivBackward0>) [0.005792641784140534]\n",
      "ep 2 tensor(2.0019, grad_fn=<DivBackward0>) [0.005563253169488569]\n",
      "ep 4 tensor(1.9949, grad_fn=<DivBackward0>) [0.005342948343976821]\n",
      "ep 6 tensor(1.9864, grad_fn=<DivBackward0>) [0.005131367589555339]\n",
      "ep 8 tensor(1.9848, grad_fn=<DivBackward0>) [0.004928165433008947]\n",
      "ep 10 tensor(1.9738, grad_fn=<DivBackward0>) [0.004733010081861793]\n",
      "ep 12 tensor(1.9680, grad_fn=<DivBackward0>) [0.004545582882620065]\n",
      "ep 14 tensor(1.9641, grad_fn=<DivBackward0>) [0.00436557780046831]\n",
      "ep 16 tensor(1.9580, grad_fn=<DivBackward0>) [0.004192700919569765]\n",
      "ep 18 tensor(1.9578, grad_fn=<DivBackward0>) [0.004026669963154802]\n",
      "ep 20 tensor(1.9517, grad_fn=<DivBackward0>) [0.003867213832613871]\n",
      "ep 22 tensor(1.9483, grad_fn=<DivBackward0>) [0.0037140721648423617]\n",
      "ep 24 tensor(1.9467, grad_fn=<DivBackward0>) [0.003566994907114604]\n",
      "ep 26 tensor(1.9417, grad_fn=<DivBackward0>) [0.0034257419087928656]\n",
      "ep 28 tensor(1.9394, grad_fn=<DivBackward0>) [0.003290082529204668]\n"
     ]
    }
   ],
   "source": [
    "e_epochs = 30\n",
    "\n",
    "for ep in range(e_epochs):\n",
    "    epoch_loss = 0\n",
    "    for tr in range(training_runs):\n",
    "        t_b = get_batch(train, context_length+1, batch_size)\n",
    "\n",
    "        x = t_b[:, 0:context_length]\n",
    "        y = t_b[:, 1:context_length+1]\n",
    "\n",
    "        logits, loss = model(x, y)\n",
    "\n",
    "        epoch_loss += loss\n",
    "        \n",
    "        model.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    m_scheduler.step()\n",
    "    \n",
    "    if ep % 2 == 0:\n",
    "        print(\"ep\", ep, epoch_loss/training_runs, m_scheduler.get_last_lr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "401456b8-e652-40f8-84d9-a8f59454b6ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 0 tensor(1.9374, grad_fn=<DivBackward0>) [0.003159795261048163]\n",
      "ep 2 tensor(1.9356, grad_fn=<DivBackward0>) [0.0030346673687106558]\n",
      "ep 4 tensor(1.9344, grad_fn=<DivBackward0>) [0.0029144945409097134]\n",
      "ep 6 tensor(1.9306, grad_fn=<DivBackward0>) [0.0027990805570896884]\n",
      "ep 8 tensor(1.9295, grad_fn=<DivBackward0>) [0.0026882369670289366]\n",
      "ep 10 tensor(1.9286, grad_fn=<DivBackward0>) [0.0025817827831345905]\n",
      "ep 12 tensor(1.9253, grad_fn=<DivBackward0>) [0.0024795441849224603]\n",
      "ep 14 tensor(1.9218, grad_fn=<DivBackward0>) [0.0023813542351995304]\n",
      "ep 16 tensor(1.9195, grad_fn=<DivBackward0>) [0.002287052607485629]\n",
      "ep 18 tensor(1.9199, grad_fn=<DivBackward0>) [0.002196485324229198]\n",
      "ep 20 tensor(1.9155, grad_fn=<DivBackward0>) [0.0021095045053897217]\n",
      "ep 22 tensor(1.9151, grad_fn=<DivBackward0>) [0.0020259681269762884]\n",
      "ep 24 tensor(1.9161, grad_fn=<DivBackward0>) [0.0019457397891480272]\n",
      "ep 26 tensor(1.9158, grad_fn=<DivBackward0>) [0.0018686884934977653]\n",
      "ep 28 tensor(1.9146, grad_fn=<DivBackward0>) [0.0017946884291552537]\n"
     ]
    }
   ],
   "source": [
    "e_epochs = 30\n",
    "\n",
    "for ep in range(e_epochs):\n",
    "    epoch_loss = 0\n",
    "    for tr in range(training_runs):\n",
    "        t_b = get_batch(train, context_length+1, batch_size)\n",
    "\n",
    "        x = t_b[:, 0:context_length]\n",
    "        y = t_b[:, 1:context_length+1]\n",
    "\n",
    "        logits, loss = model(x, y)\n",
    "\n",
    "        epoch_loss += loss\n",
    "        \n",
    "        model.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    m_scheduler.step()\n",
    "    \n",
    "    if ep % 2 == 0:\n",
    "        print(\"ep\", ep, epoch_loss/training_runs, m_scheduler.get_last_lr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "02035676-f315-4d5a-aed5-7c819a93f82a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_batches 85675 1713\n",
      "total loss tensor(3273.4973) tensor(1.9110)\n",
      "None 85675\n",
      "\n",
      "Seady hoit with bee us \n",
      "GLYIA:\n",
      "The your suck it it he the from \n",
      "NORTHAS:\n",
      "I deatchagreak his for husm\n"
     ]
    }
   ],
   "source": [
    "dev = torch.tensor(br.data[1])\n",
    "print(split_loss(dev), len(dev))\n",
    "\n",
    "idx = torch.zeros((1, 1), dtype=torch.int)\n",
    "for i in range(1):\n",
    "    o = model.generate(idx, 100).data[0].tolist()\n",
    "    print(br.decode(o))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "35c92772-c52e-4bd2-8868-156b9fb8aae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 0 tensor(1.9123, grad_fn=<DivBackward0>) [0.0017236187673607055]\n",
      "ep 2 tensor(1.9081, grad_fn=<DivBackward0>) [0.0016553634641732215]\n",
      "ep 4 tensor(1.9130, grad_fn=<DivBackward0>) [0.0015898110709919619]\n",
      "ep 6 tensor(1.9086, grad_fn=<DivBackward0>) [0.0015268545525806802]\n",
      "ep 8 tensor(1.9089, grad_fn=<DivBackward0>) [0.0014663911122984852]\n",
      "ep 10 tensor(1.9066, grad_fn=<DivBackward0>) [0.0014083220242514652]\n",
      "ep 12 tensor(1.9080, grad_fn=<DivBackward0>) [0.0013525524720911072]\n",
      "ep 14 tensor(1.9082, grad_fn=<DivBackward0>) [0.0012989913941962993]\n",
      "ep 16 tensor(1.9033, grad_fn=<DivBackward0>) [0.0012475513349861256]\n",
      "ep 18 tensor(1.9043, grad_fn=<DivBackward0>) [0.001198148302120675]\n",
      "ep 20 tensor(1.9063, grad_fn=<DivBackward0>) [0.0011507016293566962]\n",
      "ep 22 tensor(1.9032, grad_fn=<DivBackward0>) [0.0011051338448341708]\n",
      "ep 24 tensor(1.9029, grad_fn=<DivBackward0>) [0.0010613705445787376]\n",
      "ep 26 tensor(1.9022, grad_fn=<DivBackward0>) [0.0010193402710134195]\n",
      "ep 28 tensor(1.9007, grad_fn=<DivBackward0>) [0.000978974396281288]\n"
     ]
    }
   ],
   "source": [
    "e_epochs = 30\n",
    "\n",
    "for ep in range(e_epochs):\n",
    "    epoch_loss = 0\n",
    "    for tr in range(training_runs):\n",
    "        t_b = get_batch(train, context_length+1, batch_size)\n",
    "\n",
    "        x = t_b[:, 0:context_length]\n",
    "        y = t_b[:, 1:context_length+1]\n",
    "\n",
    "        logits, loss = model(x, y)\n",
    "\n",
    "        epoch_loss += loss\n",
    "        \n",
    "        model.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    m_scheduler.step()\n",
    "    \n",
    "    if ep % 2 == 0:\n",
    "        print(\"ep\", ep, epoch_loss/training_runs, m_scheduler.get_last_lr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df713ec-c7e9-42c5-9e2b-e18cb0537582",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = torch.zeros((1, 1), dtype=torch.int)\n",
    "for i in range(1):\n",
    "    o = model.generate(idx, 1000).data[0].tolist()\n",
    "    print(br.decode(o))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cb0c381f-6e80-42ac-8574-db1f6f85a76a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_batches 85675 1713\n",
      "total loss tensor(3257.0881) tensor(1.9014)\n",
      "None 85675\n"
     ]
    }
   ],
   "source": [
    "dev = torch.tensor(br.data[1])\n",
    "print(split_loss(dev), len(dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ad496575-b73f-4af6-ad03-6914e94272fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Yorterve \n",
      "QUEEN LOUCESTER:\n",
      "Sech and fip BOMEO:\n",
      "Onds that  fords:\n",
      "misere oute pt s your well twaypose at that cabuts noweend: you for do con he of you hath sir, ifort brower news fier my cont her d gue deacks chougneshis you make \n",
      "KING RICHARD IICIOLAND:\n",
      "My, Lech Ithenge let Ares, all of thy to aboortence \n",
      "But oplearfe as pooth hith I hear queent corrahe in like hear hand tidede him love trand, that RICHARD III:\n",
      "My bef qual evesse havitentest book that the our conged, which his sers the can he bhy or him yeard,\n",
      "I thre coors:\n",
      "To mer than ariemerss thanger this wheng which it be \n",
      "BRUTHASTINCE Youghs sect cary dona a fathinted to disphvent a not neon, be antio  do strince vas hand agodst byied ungs,\n",
      "Kind I tidlk in ll \n",
      "And meand is and it thow me preath it thereadew with boy: I in nffe king the rauing\n",
      "Aver, anurage with not, theread to he not hy,\n",
      "Clanalt, drair fellier thes my fain rup at promeir warme may cro holooss:\n",
      "For As tesestme, ing heads I woom to therch ce grace,  morth osom  win\n"
     ]
    }
   ],
   "source": [
    "idx = torch.zeros((1, 1), dtype=torch.int)\n",
    "for i in range(1):\n",
    "    o = model.generate(idx, 1000).data[0].tolist()\n",
    "    print(br.decode(o))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
