## Bigram modelling of names

we look at a list of names from a file and see if we can create new names with similar characteristics

### run through

the first workbook is done around watching the lecture

[bigrams](nn_bigram.ipynb)

I skip through the count model here
(it's effectively recreated at the end in a few lines to compare its weights to the neural net's)

one thing that might be useful to demonstrate is that the loss is very similar

Â¬ somewhere around 2.4545

### slight return

The second workbook is done later revisiting the bigram lecture in the light of being exposed to more
ideas in the subsequent lectures

[slight return](revisit_bigrams.ipynb)

there's not much new in here: the role of the one-hot encoding is spelled out a bit more

it's more to review things for myself




