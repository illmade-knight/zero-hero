{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "374abf2e-6933-41d8-9d88-ec0514118664",
   "metadata": {},
   "source": [
    "### OK lets go back through bigram\n",
    "\n",
    "we've learned a bit about setting up tensors and batches and I just want to lock in some of the ideas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9995a9a7-e3bf-4679-9e63-e42bc09d7e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fb566a8-b711-40bb-9642-43fc51f2f43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../lib/bookreader.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7ebded0-199c-4469-bf81-f2d490f8b7b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lowercase only\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = BookReader()\n",
    "names.read(\"../resources/names.txt\")\n",
    "vocab_size = names.vocab_size\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129f6fac-1e92-4016-8444-9cc95524bc64",
   "metadata": {},
   "source": [
    "jsut spell out the mapping from characters to their simple 1D embedding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0efdde63-f0c5-4631-bfa5-7631f37d4744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of symbols 27 lengths of our train, dev, test splits 183300 21600 23025\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'0:\\n, 1:a, 2:b, 3:c, 4:d, 5:e, 6:f, 7:g, 8:h, 9:i, 10:j, 11:k, 12:l, 13:m, 14:n, 15:o, 16:p, 17:q, 18:r, 19:s, 20:t, 21:u, 22:v, 23:w, 24:x, 25:y, 26:z, '"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"number of symbols\", names.vocab_size, \"lengths of our train, dev, test splits\", len(names.data[0]), len(names.data[1]), len(names.data[2]))\n",
    "\n",
    "symbols = \"\"\n",
    "for i, c in enumerate(names.chars):\n",
    "    symbols += f\"{i}:{c}, \"\n",
    "\n",
    "symbols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb9953d-a3ef-45d3-8390-4930146a0c07",
   "metadata": {},
   "source": [
    "## batch stuff again "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c37a823a-4856-4eae-a4fb-49873802ad46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labeled_batch(data, context_length=1, label_length=1, batch_size=5):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    ix = torch.randint(len(data) - context_length, (batch_size,))\n",
    "    b = torch.stack([data[i:i+context_length+label_length] for i in ix])\n",
    "    \n",
    "    x = b[:, 0:context_length]\n",
    "    y = b[:, context_length:context_length+label_length]\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "135fbd7a-5262-4a9b-ab2d-c93f7616ec0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[12],\n",
       "         [14],\n",
       "         [ 3],\n",
       "         [ 1],\n",
       "         [12]]),\n",
       " tensor([[12],\n",
       "         [15],\n",
       "         [ 8],\n",
       "         [19],\n",
       "         [25]]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = torch.tensor(names.data[0])\n",
    "t_b = get_labeled_batch(train)\n",
    "t_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ffef699-ea03-4741-9b31-9f22f27667b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([27, 27])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_weights = torch.randn(vocab_size, vocab_size)\n",
    "bigram_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f5c7ee8-bea2-4676-b703-11e33df19e49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[12],\n",
       "         [14],\n",
       "         [ 3],\n",
       "         [ 1],\n",
       "         [12]]),\n",
       " tensor([[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0]],\n",
       " \n",
       "         [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0]],\n",
       " \n",
       "         [[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0]],\n",
       " \n",
       "         [[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0]],\n",
       " \n",
       "         [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0]]]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_encoded = F.one_hot(t_b[0], num_classes=27)\n",
    "t_b[0], bigram_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bf93af-0b2f-4f39-bb24-bba28f111c21",
   "metadata": {},
   "source": [
    "## just go over that one_hot stuff\n",
    "\n",
    "so thinking about the interactions of the one_hot and the bigram weight again\n",
    "\n",
    "each example in the batch picks out a single row in the bigram weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17e4113e-ce40-45b3-a3bd-dd365577ae8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.3541, -0.3201, -0.3401,  0.2690,  0.3055, -0.3056,  1.0199, -1.2648,\n",
       "          1.7387, -0.6993, -0.4494, -0.3260,  0.8690,  0.2729, -0.3548, -0.2096,\n",
       "         -0.8791,  0.3454, -0.1465,  0.8456,  0.9477,  0.6807, -0.7641,  0.1982,\n",
       "         -0.1490, -0.8013, -0.7173]),\n",
       " tensor([[ 0.0000,  0.0000,  0.0000, -0.0000, -0.0000,  0.0000,  0.0000, -0.0000,\n",
       "          -0.0000,  0.0000, -0.0000, -0.0000, -1.0666,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000, -0.0000,  0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000,\n",
       "           0.0000, -0.0000,  0.0000],\n",
       "         [-0.0000, -0.0000, -0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000,\n",
       "          -0.0000, -0.0000,  0.0000, -0.0000, -0.3318, -0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000, -0.0000, -0.0000, -0.0000, -0.0000,  0.0000,  0.0000,\n",
       "          -0.0000,  0.0000, -0.0000],\n",
       "         [-0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
       "          -0.0000,  0.0000,  0.0000,  0.0000, -0.7716, -0.0000,  0.0000, -0.0000,\n",
       "          -0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000, -0.0000,  0.0000,\n",
       "          -0.0000, -0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000, -0.0000, -0.0000,  0.0000,  0.0000, -0.0000,\n",
       "           0.0000,  0.0000,  0.0000, -0.0000,  1.1924,  0.0000,  0.0000,  0.0000,\n",
       "          -0.0000, -0.0000,  0.0000, -0.0000,  0.0000, -0.0000, -0.0000,  0.0000,\n",
       "           0.0000,  0.0000, -0.0000]]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_row = F.one_hot(t_b[0][0], num_classes=vocab_size)\n",
    "bigram_weights[:, 0], one_row * bigram_weights[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e478bbd0-766c-4932-8bf1-095bdd64fd7d",
   "metadata": {},
   "source": [
    "so the prediction will only function across those weights, they're the only things that go into forming the prediction\n",
    "\n",
    "lets set it up so we imagine all the names are 'alice' so 'a' is always followed by 'l'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c981c3e-12d9-4523-a962-033589d24aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([27, 27]) torch.Size([1, 27]) torch.Size([27, 27])\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0]]) torch.Size([1, 27])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[ 2.4991e-03,  3.4836e-04,  6.5716e-04,  1.9820e-03,  4.4591e-04,\n",
       "           1.9402e-03,  9.4219e-04,  1.4633e-03,  6.3191e-03,  1.8428e-02,\n",
       "           9.1579e-05,  1.6926e-03, -3.4510e-02,  1.8812e-03,  3.2318e-03,\n",
       "           1.6203e-03,  3.1105e-04,  8.8817e-04,  5.7903e-03,  4.1110e-03,\n",
       "           6.1033e-04,  5.5342e-03,  1.5140e-03,  1.2462e-03,  3.2354e-04,\n",
       "           1.4253e-03,  6.1292e-04]]),\n",
       " tensor([[-0.0000, 0.0025, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [-0.0000, 0.0003, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [-0.0000, 0.0007, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [-0.0000, 0.0020, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_weights.requires_grad = True\n",
    "\n",
    "the_a_row = F.one_hot(torch.tensor(names.encode('a')), num_classes=27)\n",
    "\n",
    "logits = bigram_weights * the_a_row\n",
    "\n",
    "print(bigram_weights.shape, the_a_row.shape, logits.shape)\n",
    "\n",
    "the_expected_answer = F.one_hot(torch.tensor(names.encode('l')), num_classes=27)\n",
    "print(the_expected_answer, the_expected_answer.shape)\n",
    "\n",
    "loss = F.cross_entropy(logits, the_expected_answer.view(-1))\n",
    "\n",
    "bigram_weights.grad = None\n",
    "loss.backward()\n",
    "bigram_weights.grad[:, names.encode('a')].T, bigram_weights.grad[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beaaf2e8-cb75-4575-81a5-e4f3ecfdebec",
   "metadata": {},
   "source": [
    "we can see the only gradient is on the 'a' weights in the system\n",
    "\n",
    "## now lets train some batches\n",
    "\n",
    "by batching we can speed up our tensor calculations (well if we used a gpu or metal or whatever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1fc80e19-e52d-4524-bf73-d4226c75ad73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1]) torch.Size([3, 1])\n",
      "xs tensor([[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "          0, 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0]]])\n",
      "ys tensor([[[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "          0, 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
      "          0, 0, 0, 0]]])\n"
     ]
    }
   ],
   "source": [
    "xs, ys = get_labeled_batch(train, 1, 1, 3)\n",
    "print(xs.shape, ys.shape)\n",
    "o_h = F.one_hot(xs, num_classes=vocab_size)\n",
    "labels = F.one_hot(ys, num_classes=vocab_size)\n",
    "print(\"xs\", o_h)\n",
    "print(\"ys\", labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d11956f5-4ad8-468c-a59d-e426b4b7dc32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch loss tensor(3.1933, grad_fn=<DivBackward0>)\n",
      "epoch loss tensor(2.8015, grad_fn=<DivBackward0>)\n",
      "epoch loss tensor(2.6802, grad_fn=<DivBackward0>)\n",
      "epoch loss tensor(2.6144, grad_fn=<DivBackward0>)\n",
      "epoch loss tensor(2.5844, grad_fn=<DivBackward0>)\n",
      "epoch loss tensor(2.5656, grad_fn=<DivBackward0>)\n",
      "epoch loss tensor(2.5393, grad_fn=<DivBackward0>)\n",
      "epoch loss tensor(2.5248, grad_fn=<DivBackward0>)\n",
      "epoch loss tensor(2.5264, grad_fn=<DivBackward0>)\n",
      "epoch loss tensor(2.5156, grad_fn=<DivBackward0>)\n",
      "epoch loss tensor(2.5043, grad_fn=<DivBackward0>)\n",
      "epoch loss tensor(2.4924, grad_fn=<DivBackward0>)\n",
      "epoch loss tensor(2.4977, grad_fn=<DivBackward0>)\n",
      "epoch loss tensor(2.4945, grad_fn=<DivBackward0>)\n",
      "epoch loss tensor(2.4851, grad_fn=<DivBackward0>)\n",
      "epoch loss tensor(2.4900, grad_fn=<DivBackward0>)\n",
      "epoch loss tensor(2.4837, grad_fn=<DivBackward0>)\n",
      "epoch loss tensor(2.4835, grad_fn=<DivBackward0>)\n",
      "epoch loss tensor(2.4816, grad_fn=<DivBackward0>)\n",
      "epoch loss tensor(2.4860, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "training_runs = 400\n",
    "batch_size = 80\n",
    "learning_rate = .5\n",
    "bigram_weights = torch.randn((vocab_size, vocab_size), requires_grad=True)\n",
    "for ep in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    for tr in range(training_runs):\n",
    "        xs, ys = get_labeled_batch(train, 1, 1, batch_size)\n",
    "        o_h = F.one_hot(xs, num_classes=vocab_size).float()\n",
    "        logits = o_h @ bigram_weights\n",
    "        labels = F.one_hot(ys, num_classes=vocab_size)\n",
    "    \n",
    "        # reshape because of batches\n",
    "        r_logits = logits.view(-1, vocab_size) \n",
    "        r_labels = labels.view(-1, vocab_size)\n",
    "    \n",
    "        probs = F.softmax(r_logits, 1) \n",
    "        loss = -(r_labels * probs).sum(1).log().mean()\n",
    "\n",
    "        epoch_loss += loss\n",
    "    \n",
    "        bigram_weights.grad = None\n",
    "        loss.backward()\n",
    "        bigram_weights.data -= learning_rate * bigram_weights.grad\n",
    "\n",
    "    print(\"epoch loss\", epoch_loss/training_runs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd7d244-7bd5-4512-9aca-3a4818666e8f",
   "metadata": {},
   "source": [
    "### look at train vs dev\n",
    "\n",
    "we want to compare our training loss to the loss on a set of names we haven't yet looked at \n",
    "\n",
    "- we either skipped this first time through or it's covered in a later lecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f8c56d3a-be30-422a-9c15-e4f805433e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def split_loss(set):\n",
    "    eval_loss = 0\n",
    "    set_len = len(set)\n",
    "    for i in range(set_len-1):\n",
    "        o_h = F.one_hot(set[i], num_classes=vocab_size).float()\n",
    "        logits = o_h @ bigram_weights\n",
    "        labels = F.one_hot(set[i+1], num_classes=vocab_size)\n",
    "        probs = F.softmax(r_logits, 1) \n",
    "        eval_loss += -(r_labels * probs).sum(1).log().mean()\n",
    "\n",
    "    return eval_loss/set_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b6afb9-b8df-4046-b6ff-0d44aeb901f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev split tensor(2.5858)\n"
     ]
    }
   ],
   "source": [
    "dev = torch.tensor(names.data[1])\n",
    "\n",
    "dev_loss = split_loss(dev)\n",
    "print(\"dev split\", dev_loss)\n",
    "train_loss = split_loss(train)\n",
    "print(\"train split\", train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "56611375-c2e6-454d-b63c-2c30f1932d3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1c2f01dd690>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "wplot = bigram_weights.exp().data\n",
    "plt.imshow(wplot)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
