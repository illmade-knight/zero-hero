## Initialization

when we trained our previous models there could be large variations in how quickly we learned

next we start to look at how [initialization](initialization.ipynb) can affect the layers

by reducing the variance of our weights at initialization (by a principled approach) we can get
more consistent results and this will help us comparing models going forward

we add more weight adjustments and increase the [model size](initialization_slight_return.ipynb)

### Next

we start to use torch's nn layers for our model and track performance as the model scales